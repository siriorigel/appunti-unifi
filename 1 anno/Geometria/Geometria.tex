\documentclass[a4paper, oneside]{book}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[a4paper,
            centering,
            %bindingoffset=0.2in,
            left=1in,
            right=1in,
            top=1in,
            bottom=1in,
            footskip=.25in]{geometry} 
\usepackage[italian]{babel}
\usepackage{pgfplots}
\usepackage{tabularx}
\usepackage{wrapfig}
\definecolor{page}{rgb}{0.129,0.157,0.212}
\pagecolor{page}
\color{white}
\graphicspath{ {./images/} }
\usetikzlibrary{datavisualization}
\usetikzlibrary{datavisualization.formats.functions}
\usetikzlibrary{patterns}
\pgfplotsset{width=10cm,compat=1.9}

\title{Geometria e algebra lineare}
\author{Tommaso Miliani}
\date{2024/2025}

\begin{document}

\newtheoremstyle{theoremEnv}
                {}          % Space above
                {}          % Space below
                {\slshape}  % Body font
                {}          % Indent amount
                {\bfseries} % Head font
                {.}         % Punctuation after head
                {\newline}         % Space after theorem head
                {}          % Theorem head spec
\theoremstyle{theoremEnv}

\newtheorem{definition}{Definizione}[chapter]
\newtheorem{theorem}{Teorema}[chapter]
\newtheorem{lemma}{Proposizione}[chapter]
\newtheorem{observation}{Osservazione}[chapter]
\newtheorem{corollary}{Corollario}[theorem]
\newtheorem{example}{Esempio}[chapter]

\maketitle

\tableofcontents

\chapter{Gruppi, Campi e relazioni}

\section{Gruppi}
\begin{definition}[Unicità dell'elemento unico in un insieme]
  Sia $G$ un'insieme non vuoto con un'operazione del tipo
  \begin{align}
    \cdot : G \times G \rightarrow G
  \end{align}
  Se esiste un elemento $e \in G$ tale che $a \cdot e = e \cdot a = a\  \forall a \in G$,
  allora tale elemento è unico.
\end{definition}
\begin{proof}
  Siano $e, e^{\prime} \in G$ tali che:
\[
    a \cdot e = e \cdot a = a, \qquad a \cdot e^{\prime} = e^{\prime} \cdot a = a
\]
per qualsiasi $a \in  G$, allora in particolare
\[
    e^{\prime} = e \cdot e^{\prime} = e,
\]
quindi $e = e^{\prime}$.
\end{proof}

\begin{theorem}[Definizione di gruppo e proprietà]
    Un insieme non vuoto G con un'operazione:
    \[
        \cdot : G \times G \rightarrow G
    \]
    si dice gruppo se valgono le seguenti proprietà:
    \begin{enumerate}
        \item $(a \cdot b) \cdot c = a \cdot (b \cdot c), \quad \forall a, b, c \in G$ (proprietà associativa);
        \item $\exists e \in G$ tale che $a \cdot e = e \cdot a, \quad \forall a \in G$ (esistenza dell'elemento neutro);
        \item $\forall a \in G, \exists \overline{a}$ tale che $a \cdot \overline{a} = \overline{a} \cdot a = e$ (esistenza degli inversi).
    \end{enumerate}
\end{theorem}

\begin{theorem}[Unicità dell'inverso]
    Sia G un gruppo, ogni elemento di G ha un solo inverso.
\end{theorem}
\begin{proof}
      Sia $g \in G$ e siano $h$ e $h^{\prime}$ tali che:
    \[
        g \cdot h = h \cdot g = e, \qquad g \cdot h^{\prime} = h^{\prime} \cdot g = e.
    \]
    Moltiplicando entrambi i membri dell'uguaglianza $g \cdot h = e$ per $h^{\prime}$ si ottiene:
    \begin{eqnarray*}
        h^{\prime} \cdot (g \cdot h) = h^{\prime} \cdot e,\\
        h^{\prime} \cdot (g \cdot h) = h^{\prime},\\
        (h^{\prime} \cdot g) \cdot h = h^{\prime};
    \end{eqnarray*}
    Essendo che $h^{\prime} \cdot g = e$, otteniamo: 
    \[
        e \cdot h = h^{\prime},
    \]
    da cui si ricava che $h = h^{\prime}$.
\end{proof}

\begin{definition}[Gruppo commutativo]
    Un gruppo G si dice \textbf{commutativo} o \textbf{abeliano} se
    $a \cdot b = b \cdot a, \quad \forall a, b \in G$.
\end{definition}

Alcuni gruppi come $\mathbb{Z}, \mathbb{Q}, \mathbb{R}, \mathbb{C}$ sono gruppi abeliani nella somma. \\
Tutti questi possono essere abeliani anche nella moltiplicazione, semplicemente
rimuovendo lo zero, tranne $\mathbb{Z}$, poiché solo -1 e 1 hanno l'inverso.
\begin{lemma}
    Sia $G$ un gruppo, $\forall a, b \in G$:
    \begin{align}
        (a \cdot b)^{-1} = b^{-1} \cdot a^{-1}
    \end{align}
    e 
    \begin{align}
        (a^{-1})^{-1} = a
    \end{align}
\end{lemma}
\begin{proof}
  La prima si dimostra utilizzando la proprietà commutativa del prodotto
  in un gruppo e attraverso la proprietà degli elementi inversi:
  \begin{gather*}
      (a \cdot  b)^{-1}  = b^{-1} \cdot  a^{-1} 
  \end{gather*}
  Diventa
  \begin{gather*}
      (a \cdot  b) (a \cdot  b)^{-1}  = (b^{-1} \cdot  a^{-1} ) \cdot  (a \cdot  b) \\
      e = (b^{-1}  \cdot  b) \cdot  (a^{-1}  \cdot  a) \\
      e = e \cdot  e
  \end{gather*}
  E allora è dimostrata. Per la seconda invece si ha
  \begin{gather*}
    (a^{-1} )^{-1}  \cdot  a^{-1}  = a \cdot  a^{-1}  \\
    e = e
  \end{gather*}
\end{proof}

\begin{example}
  Sia X un'insieme: le bigezioni che vanno da X in X con l'operazione della composizione
di funzioni è un gruppo denotato con $S(X)$, e detto gruppo simmetrico.
Se X ha almeno tre elementi $S(X$) non è abeliano: sia f la bigezione che manda
a in b, b in a, e tutti gli altri elementi in  sé stessi e sia g la bigezione che
manda b in c, c in b e gli altri in sé stessi. \\
Osserviamo che $(f \circ g)(a) = b$ mentre $(g \circ f)(a) = c$ quindi
$f \circ g \ne g \circ f$ e quindi $S(X)$ non è abeliano.
\end{example}

\section{Campi}
\begin{definition}[Campo]
  Sia $\mathbb{K}$ un insieme non vuoto su cui sono definite le seguenti operazioni:
  \begin{align}
    + : \mathbb{\mathbb{K}} \times \mathbb{K} \rightarrow \mathbb{K}
  \end{align}
  \begin{align}
    \cdot : \mathbb{K} \times \mathbb{K} \rightarrow \mathbb{K}
  \end{align}
  L'insieme $\mathbb{K}$ con tali operazioni si dice un campo se valgono le seguenti proprietà:
  \begin{enumerate}
    \item $\forall a, b, c \in \mathbb{K}$, 
    \[
      (a + b) + c = a + (b + c)
    \]
    (proprietà associativa della somma);
    \item esiste l'elemento neutro per l'operazione +, cioè esiste l'elemento,
    in genere denotato con $0_K$ o semplicemente 0, tale che:
    \[
      0_K + a = a + 0_K = a, \quad \forall a \in \mathbb{K};
    \]
    \item $\forall a \in \mathbb{K}$ esiste l'opposto di $a$ cioè l'elemento $a^{\prime}$
    di $\mathbb{K}$ tale per cui:
    \[
      a + a^{\prime} = a^{\prime} + a = 0_K
    \]
    (esistenza dell'opposto);
    \item $\forall a, b \in \mathbb{K}$
    \[
      a + b = b + a
    \]
    (proprietà commutativa della somma);
    \item $\forall a, b, c \in \mathbb{K}$
    \[
      a \cdot (b \cdot c) = (a \cdot b) \cdot c
    \]
    (proprietà associativa del prodotto);
    \item $\mathbb{K} - \{0\}$ è non vuoto ed esiste in $\mathbb{K} - \{0\}$ un'elemento neutro per l'operazione
    $\cdot$, cioè esiste un elemento in $\mathbb{K} - \{0\}$ denotato in genere con $1_K$ o 
    semplicemente con 1, tale che
    \[
      1_K \cdot a = a \cdot 1_K = a \qquad \forall a \in \mathbb{K} - \{0\}
    \]
    (esistenza dell'elemento neutro per il prodotto);
    \item $\forall a \in \mathbb{K} - \{0_K\}$, esiste un'inverso, cioè esiste un'elemento
    $\tilde{a}$ tale che:
    \[
      a \cdot \tilde{a} = \tilde{a} \cdot a = 1_K
    \]
    (esistenza dell'inverso);
    \item
    \[
      a \cdot b = b \cdot a \qquad \forall a, b \in \mathbb{K}
    \]
    (proprietà commutativa del prodotto);
    \item
    \[
      a \cdot (b + c) = a \cdot b + a \cdot c \qquad \forall a, b, c \in \mathbb{K}
    \]
    e
    \[
      (b + c) \cdot a = b \cdot a + c \cdot a \qquad \forall a, b, c \in \mathbb{K}
    \]
    (proprietà distributiva del prodotto rispetto alla somma e della somma rispetto al prodotto).
  \end{enumerate}

\end{definition}
  \begin{definition}[Ogni elemento moltiplicato per zero è zero]
    In ogni campo $\mathbb{K}$ si ha che
    \[
      0a = 0, \quad \forall a \in \mathbb{K}.
    \]
  \end{definition}

\begin{proof}
    Avendo:
  \[
    0a = (0 + 0 )a = 0a + 0a
  \]
  Se si ponesse $b$ come l'opposto di $a0$ e lo si sommasse membro a membro, si otterrebbe
  \begin{gather*}
    0a + b = (0a + 0a) + b\\
    0 = 0a + (0a + b)\\
    0 = 0a + 0\\
    0 = 0a
  \end{gather*}
  La relazione è dunque dimostrata.
\end{proof}

\begin{definition}[Definizione dell'elemento opposto]
  Sia $\mathbb{K}$ un campo, si denota con -1 l'opposto di 1. Allora $\forall a \in \mathbb{K}$, 
  l'elemento $(-1) \cdot a$ è un'opposto di $a$.
\end{definition}
\begin{proof}
    \[
    (-1)a + a = (-1)a + 1a = (-1+1)a = 0a = 0
  \]
\end{proof}

\section{Relazioni di equivalenza}
\begin{definition}[Relazione binaria]
  Una relazione binaria $\sim$ su un insieme A è un sottoinsieme $\Re$ di $A \times A$.
  Siano $a, a^{\prime} \in A$. Si dice che $a$ è in relazione con $a^{\prime}$ e
  si scrive $a \sim a^{\prime}$ se $(a, a^{\prime} \in \Re)$.
\end{definition}

\begin{definition}[Relazione di equivalenza]
  Una relazione binaria su un insieme A è una \textbf{relazione di equivalenza} se è:
  \begin{enumerate}
    \item Riflessiva: cioè $\forall a \in A, \ \ a \sim a$;
    \item Simmetrica: se $\forall a, b \in A, \ \ a \sim b \wedge b \sim a$;
    \item Transitiva: cioè se $\forall a, b, c \in A, \ \ a \sim b, \wedge b \sim c \Rightarrow a \sim c$. 
  \end{enumerate}
\end{definition}
Il sottoinsieme A che contiene tutte e soli gli elementi equivalenti a un qualche
elemento $a \in A$ prende il nome di \textbf{classe di equivalenza} di $a$ indicata con $[a]$.
In una classe di equivalenza tutti gli elementi in essa sono contenuti tra loro equivalenti.
Le classi di equivalenza sono a due a due disgiunte.
\begin{example}
  Preso $A = \mathbb{Z}$, due elementi di A si dicono equivalenti se la loro differenza è
divisibile per 3. La relazione così definita è una relazione di equivalenza. 
Le classi di equivalenza sono tre: [0], [1], [2].
\end{example}
\begin{definition}[Insieme quoziente]
  L'insieme delle classi di equivalenza si chiama \textbf{insieme quoziente di A per la relazione}
$\sim$ e viene indicato con l'espressione $A/\sim$.
\end{definition}




\chapter{Campi elevati alla $n$ e matrici}

\section{$\mathbb{K}^{n}$ e matrici}
\begin{definition} [Campo]
  Sia $\mathbb{K}$ un campo e sia $n \in \mathbb{N}$. Si definisce $\mathbb{K}^n$ come
  \begin{align}
      \mathbb{K}^{n} = \left\{
      \left( 
      \begin{array}{l}
        x_1 \\
        \cdot \\
        \cdot \\
        \cdot \\
        x_n
      \end{array} 
      \right)
    | x_1, \dots, x_n \in \mathbb{K}
    \right\},
  \end{align}
  ossia l'insieme delle $n$-uple di elementi di $\mathbb{K}$.
\end{definition}

\begin{definition}[Vettore]
  Sia $\mathbb{K}$ un campo e sia $n \in \mathbb{N} - \{0\}$.
  Una $n-$upla di elementi in $\mathbb{K}$ si dice \textbf{vettore} del campo $\mathbb{K}^{n}$ e si
  indica con
  \begin{align}
      x = \left(\begin{array}{l}
    x_1 \\
    \cdot \\
    \cdot \\
    \cdot \\
    x_n
  \end{array}\right) \in \mathbb{K}^{n}.
  \end{align}
  L'elemento $x_i$, che compare all'$i$-esimo posto, si definisce $i$-esima componente di $x$.
\end{definition}

\begin{definition} [Somma e prodotto in un campo]
  Sia $\mathbb{K}$ un campo e sia $n \in \mathbb{N} - \{0\}$. \\
  La somma di  due elementi di $\mathbb{K}^{n}$ è definita nel seguente modo:
  \begin{align}
      \begin{pmatrix} x_1 \\
      \vdots \\
      x_n \end{pmatrix} +  
      \begin{pmatrix} y_1 \\
      \vdots \\
      y_n \end{pmatrix} = 
      \begin{pmatrix} x_1 + y_n\\
      \vdots \\
      x_n + y_n \end{pmatrix}
  \end{align}
  Se $x, y \in \mathbb{K}^{n}$ si definisce $x + y$ come l'elemento di $\mathbb{K}^{n}$ tale che
  $(x + y)_i=x_i+y_i$ per qualsiasi $i = 1, \dots, n$.
  Il prodotto tra un elemento di $\mathbb{K}^{n}$ e un elemento di $\mathbb{K}$ è:
  \begin{align}
          \lambda \cdot \left( 
      \begin{array}{c}
        x_1 \\
        \vdots \\
        x_n
      \end{array} 
      \right) = \begin{pmatrix} \lambda x_1 \\
      \vdots \\
      \lambda x_n \end{pmatrix} 
  \end{align}
\end{definition}

\clearpage
\section{Matrici}
\begin{definition}[Matrice]
  Siano $m,n \in \mathbb{N} - \{0\}$. Una matrice $m \times n$ è una tabella con $m$ righe
  e $n$ colonne. Si dice a coefficienti in un campo $\mathbb{K}$ se tutti gli elementi della
  tabella sono elementi di $\mathbb{K}$. \\
  L'elemento $a$ in una matrice $A$ all'$i$-esima riga e alla $j$-esima colonna si denota con
  $a_{i,j}$. \\
  La riga $i$-esima si denota come $A_{(i)}$. \\
  La colonna $j$-esima si denota come $A^{(j)}$. \\
  La sottomatrice dalle colonne $j_a, \dots j_b$, con $a < b$, e dalle righe
  $i_s, \dots i_t$, con $s < t$, si denota con $A_{i_s, \dots, i_t}^{j_a, \dots, j_b}$.
\end{definition}

\begin{definition}[Matrice come campo]
  Siano $m, n \in \mathbb{N} - \{0\}$ e sia $\mathbb{K}$ un campo. Si definisce l'insieme delle matrici $M$ a coefficienti
  in un campo $\mathbb{K}$ con $m$ righe e $n$ colonne il seguente insieme: 
  \[
    M(m \times n, \mathbb{K}) = \{A | A \ matrice \ m \times n \ \text{a coefficienti in } \ \mathbb{K}\}.
  \]
\end{definition}

\begin{lemma}[Proprietà delle matrici]
  Le regole di somma tra matrici e di prodotto scalare si applicano nello stesso 
modo che nei campi, tranne che per la somma, che, per essere effettuata tra matrici,
richiede che le matrici abbiano lo stesso formato. 
Siano $n,m \in \mathbb{N} - \{0\}$ e sia $\mathbb{K}$ un campo. Allora valgono le seguenti proprietà:
\begin{enumerate}
  \item $\forall A, B, C \in M(m \times n, \mathbb{K})$:
  \[
    (A +  B) + C = A + (B + C)
  \]
  (proprietà associativa della somma),
  \item $\forall A, B \in M(m \times n, \mathbb{K})$:
  \[
    A + B = B + A
  \]
  (proprietà commutativa della somma),
  \item la matrice $m \times n$ con tutti coefficienti nulli ($0_{m \times n}$) è
  l'elemento neutro per la somma in $M(m \times n, \mathbb{K})$ cioè
  \[
    0_{m \times n} + A = A + 0_{m \times n} = A \qquad \forall A \in M(m \times n, \mathbb{K})
  \]
  (esistenza dell'elemento neutro per la somma);
  \item $\forall A \in M(m \times n, \mathbb{K})$, l'elemento $(-1)A$ è opposto di $A$ per la somma:
  in $M(m \times n, \mathbb{K})$ cioè
  \[
    A + (-1)A = (-1)A + A = 0_{m \times n},
  \]
  (esistenza dell'opposto);
  \item $\forall \lambda, \mu \in \mathbb{K} \ \wedge \ \forall A \in M(m \times n, \mathbb{K})$
  \[
    (\lambda + \mu)A = \lambda A + \mu A,
  \]
  (proprietà distributiva rispetto alla somma tra scalari in $\mathbb{K}$);
  \item $\forall \lambda \in \mathbb{K} \ \wedge \ \forall A, B \in M(m \times n, \mathbb{K})$
  \[
    \lambda(A + B) = \lambda A + \lambda B
  \]
  (proprietà distributiva rispetto alla somma tra matrici in $M(m \times n, \mathbb{K})$),
  \item $\forall \lambda, \mu \in \mathbb{K} \ \wedge \ \forall A \in M(m \times n, \mathbb{K})$
  \[  
    (\lambda \mu)A = \lambda(\mu A),
  \] 
  (proprietà associativa degli scalari);
  \item $\forall A \in M(m \times n, \mathbb{K})$
  \[
    1_{k}A = A.
  \]
  (esistenza dell'elemento neutro per il prodotto).
\end{enumerate}

\end{lemma}

\section{Vari tipi di matrici e definizioni generali}

\begin{definition}[Diagonale principale]
  La diagonale (principale) di una matrice A è l'$n$-upla $(a_{1,1}, \dots, a_{n,n})$ di una
  matrice $A \in M(n \times n, \mathbb{K})$
\end{definition}

\begin{definition}[Matrice diagonale]
  Una matrice è diagonale se gli elementi $a_{i, j} = 0$ di una matrice $A \in M(n \times m, \mathbb{K})$
  $\forall i, j \in \{1, \dots, n\}\  : \ i \ne j$.
\end{definition}

\begin{definition}[Matrice identità]
  La matrice \textbf{identità} è quella matrice a coefficienti $n \times n$ che ha tutti i coefficienti della
  diagonale principale 1 e gli altri 0.
\end{definition}

\begin{definition}[Matrice nulla con un solo elemento non nullo]
  Si indica con $E_{i, j}$ la matrice nulla con un solo elemento uguale ad $1$
  alla posizione $(i, j)$.
\end{definition}

\begin{definition}[Triangolare inferiore e superiore]
  Una matrice a coefficienti $A \in M(n \times n, \mathbb{K})$ è \textbf{triangolare superiore} se
  $a_{i, j} = 0 \ \forall i, j \in \{1, \dots, n\} \ : \ i > j$. \\
  Una matrice è invece \textbf{triangolare inferiore} se
  $a_{i, j} = 0 \ \forall i, j \in \{1, \dots, n\} \ : \ i < j$.
\end{definition}

\begin{definition}[Matrice simmetrica e antisimmetrica]
  Una matrice $A \in M(n \times n, \mathbb{K})$ è \textbf{simmetrica} se
  $a_{i, j} = a_{j, i} \ \forall i,j \in \{1, \dots, n\}$. \\
  Una matrice è \textbf{antisimmetrica} se 
  $a_{i, j} = -a_{j, i} \ \forall i,j \in \{1, \dots, n\}$. \\
  Se il campo $\mathbb{K}$ è tale che $1_k + 1_k \ne 0$, gli elementi della diagonale
principale di una matrice antisimmetrica sono nulli, infatti, per qualsiasi $i \in \{1, \dots, n\}$ si ha $a_{i, i} = -a_{i, i}$, 
quindi $a_{i, i} = 0$.
\end{definition}

\begin{definition}[Matrice trasposta]
  Siano $m, n \in \mathbb{N} -\{0\}$ e sia $\mathbb{K}$ un campo. Sia $A \in M(m \times n, \mathbb{K})$. Si definisce
  \textbf{trasposta} di A una matrice $n \times m$ così definita:
  \begin{equation}
    ^{t}A_{i, j} = A_{j, i} \quad \forall i \in {1, \dots, n} \ \wedge \ \forall j \in {1, \dots, m}.
  \end{equation} 
   Una trasposta ha le seguenti proprietà:
\begin{enumerate}
  \item $^{t}(^{t}A) = A \ \forall A \in M(m \times n, \mathbb{K})$.
  \item $^{t}(A + B) = \ ^{t}A + \ ^{t}B \ \forall A, B \in M(m \times n, \mathbb{K})$.
  \item $^{t}(\lambda A) = \lambda ^{t}A \ \forall \lambda \in \mathbb{K} \ \wedge \ \forall A \in M(m \times n, \mathbb{K})$.
\end{enumerate}
\end{definition}
\begin{proof}
  \begin{enumerate}
  \item $\forall i \in \{1, \dots, m\} \ \wedge \ \forall j \in \{1, \dots, n\}$ si ha:
  \[
    (^{t}(^{t}A))_{i, j} = (^{t}A)_{j, i} = A_{i, j}.
  \]
  \item $\forall i \in \{1, \dots, n\} \ \wedge \ \forall j \in \{1, \dots, m\}$ si ha:
  \begin{eqnarray*}
    (^{t}(A + B))_{i,j} = (A + B)_{j, i} = A_{j, i} + B_{j, i} \\
    (^{t}A + ^{t}B)_{i,j} = ^{t}A_{i,j} + ^{t}B_{i,j} = A_{j, i} + B_{j, i}
  \end{eqnarray*}
  \item $\forall i \in \{1, \dots, n\} \ \wedge \ \forall j \in \{1, \dots, m\}$ si ha:
  \[
    (^{t}(\lambda A))_{i, j} = (\lambda A)_{j, i} = \lambda A_{j, i} = \lambda (^{t}A)_{i, j} = (\lambda ^{t}A)_{i, j}.
  \]
\end{enumerate}
\end{proof}


\begin{definition}[Matrice simmetrica]
  Una matrice $A \in M(n \times n, \mathbb{K})$ si dice simmetrica se
  \begin{align}
      A = \ ^{t}A
  \end{align}
\end{definition}

\begin{definition}[Matrice antisimmetrica]
  Una matrice $A \in M(n \times n, \mathbb{K})$ si dice antisimmetrica se
  \begin{align}
      A = -\ ^{t}A
  \end{align}
\end{definition}

\begin{definition}[Traccia di una matrice]
  Si definisce \textbf{traccia} di una matrice quadrata A la somma degli elementi
  della sua diagonale denotata con $tr(A)$. 
\end{definition}

\begin{definition}[Matrice a blocchi]
  Una matrice $(n_1 + \dots + n_k) \times (n_1 + \dots + n_k)$ a coefficienti in un campo
  $\mathbb{K}$ si dice \textbf{diagonale a blocchi} $(n_1 \times n_1), \dots, (n_k \times n_k)$
  se $\exists A_1 \in M(n_1 \times n_1, \mathbb{K}), \dots, A_k \in M(n_k \times n_k, \mathbb{K})$ tali che:
  \begin{align}
      A = \left(\begin{array}{c c c c c c c}
      A_1 & 0 & \cdot & \cdot & \cdot & 0 & 0 \\
      0 & A_2 & \cdot & \cdot & \cdot & 0 & 0 \\
      \cdot & \cdot & \cdot & \cdot & \cdot & \cdot & \cdot \\
      \cdot & \cdot & \cdot & \cdot & \cdot & \cdot & \cdot \\ 
      \cdot & \cdot & \cdot & \cdot & \cdot & \cdot & \cdot\\ 
      0 & 0 & \cdot & \cdot & \cdot & A_{k-1} & 0 \\
      0 & 0 & \cdot & \cdot & \cdot & 0 & A_k \\
    \end{array}\right).
  \end{align}
\end{definition}

\section{Prodotto scalare standard e norma}
\begin{definition}[Prodotto scalare tra due vettori]
  Sia $n \in \mathbb{N} - \{0\}$ e sia $\mathbb{K}$ un campo. Si definisce \textbf{prodotto scalare standard} 
  fra due elementi $v$ e $w$ di $\mathbb{K}^{n}$ l'elemento di $\mathbb{K}$, che si denota con $(v, w)$,
  \begin{equation}
    \sum_{i=1}^{n}v_i w_i.
  \end{equation}
\end{definition}

\begin{lemma}[Proprietà del prodotto scalare standrd]
  Il prodotto scalare standard gode delle seguenti proprietà:
  \begin{enumerate}
  \item Simmetria: $(v, w) = (w, v) \quad \forall  v, w \in \mathbb{K}^{n}$,
  \item Bilinearità, ossia linearità del primo argomento:
  \begin{eqnarray*}
    (\lambda v, w) = \lambda(v, w) \quad \forall \lambda \in \mathbb{K} \ \wedge \ \forall v, w \in \mathbb{K}^{n} \\  
    (v + w, u) = (v, u) + (w, u) \quad \forall v, w, u \in \mathbb{K}^{n}, 
  \end{eqnarray*}
  e linearità nel secondo argomento:
  \begin{gather*}
    (v,\lambda w) = \lambda(v, w) \quad \forall \lambda \in \mathbb{K} \ \wedge \ \forall v, w \in \mathbb{K}^{n} \\  
    (v, w + u) = (v, w) + (v, u) \quad \forall v, w, u \in \mathbb{K}^{n}, 
  \end{gather*}
  \item Definita positività: se $\mathbb{K} = \mathbb{R}$ allora $(v, v) \geq 0 \ \forall v \in \mathbb{R}^{n}$;
  inoltre $(v, v) = 0 \Longleftrightarrow v = 0$.
\end{enumerate} 
\end{lemma}
\begin{proof}
  Si dimostrano attraverso la definizione di prodotto scalare:
  \begin{enumerate}
  \item $(v, w) = \sum_{i = 1}^{n} v_i w_i = \sum_{i = 1}^{n} w_i v_i = (w, v)$;
  \item $\lambda(v, w) = \lambda \sum_{i = 1}^{n}v_i w_i$,
  \begin{eqnarray*}
    (\lambda v, w) = \sum_{i = 1}^{n} (\lambda v_i)w_i = \sum_{i = 1}^{n}\lambda v_i w_i =
    \lambda \sum_{i = 1}^{n}v_i w_i, \\
    (v, \lambda w) = \sum_{i = 1}^{n} v_i(\lambda w)_i = \sum_{i = 1}^{n} v_i \lambda w_i =
    \lambda \sum_{i = 1}^{n} v_i w_i, \\
    (v + w, u) = \sum_{i = 1}^{n}(v + w)_i u_i = \sum_{i = 1}^{n}(v_i + w_i)u_i =
    \sum_{i = 1}^{n}(v_i u_i + w_i u_i)  = \\ \sum_{i = 1}^{n}v_i u_i + \sum_{i = 1}^{n} w_i u_i =
    (v, u) + (w, u).
  \end{eqnarray*}
  La linearità nel secondo argomento si dimostra in modo analogo oppure la possiamo derivare
  dalla linearità nel secondo argomento e dalla simmetria.
  \item $(v, v) = \sum_{i = 1}^{n} v_i v_i = \sum_{i = 1}^{n}v_i^{2} \geq 0$;
  inoltre $(v, v) = 0$ se e solo se $\sum_{i = 1}^{n}(v_i)^{2} = 0$ e questo è vero
  se e solo se $(v_i)^{2} = 0 \ \forall i \in \{1, \dots, n\}$ cioè se e solo se 
  $v_i = 0 \ \forall i \in \{1, \dots, n\}$.
\end{enumerate}
\end{proof}

\begin{definition}[Norma, modulo o lunghezza]
  Sia $v \in \mathbb{R}^{n}$. Si definisce \textbf{norma, modulo o lunghezza} di $v$, e si denota con
  $|v|$, il numero reale:
  \[
    \sqrt{(v, v)},
  \]
  cioè la radice quadrata del prodotto scalare di $v$ con sé stesso:
  \begin{equation}
    |v| = \sqrt{\sum_{i = 1}^{n} v_i^{2}}.
  \end{equation}
\end{definition}

\begin{lemma}[Proprietà della norma]
  La norma gode delle seguenti proprietà:
  \begin{enumerate}
  \item $|v| \geq 0 \ \forall v \in \mathbb{R}^{n}$; inoltre $|v| = 0 \Longleftrightarrow v = 0$,
  \item $|\lambda v| = |\lambda||v| \ \forall \lambda \in \mathbb{R} \ \wedge \ \forall v \in \mathbb{R}^{n}$. 
\end{enumerate}
\end{lemma}
\begin{proof}
  Si dimostrano a partire dalla definizione di norma:
  \begin{enumerate}
  \item Essendo la norma una radice quadrata, questa è maggiore o uguale di zero.
  Ovviamente $|0| = 0$ e se $v \in \mathbb{R}^{n}$ e $|v| = 0$, allora:
  \[
    \sqrt{\sum_{i = 1}^{n} v_i^{2}} = 0 \ \Longrightarrow \ \sum_{i = 1}^{n} v_i^{2} = 0.
  \]
  Pertanto $v_i^{2} = 0 \ \forall i \in \{1, \dots, n\} \ \wedge \ v_i = 0\  \forall i \in
  \{1, \dots, n\}$ cioè $v = 0$.  
  \item \begin{eqnarray*}
    |\lambda v| = \sqrt{(\lambda v, \lambda v)} = \sqrt{\sum_{i = 1}^{n}(\lambda v)_i^{2}}
    = \sqrt{\sum_{i = 1}^{n} (\lambda v_i)^{2}} = \\
    \sqrt{\sum_{i = 1}^{n}\lambda^{2} v_i^{2}} = \sqrt{\lambda^{2} \sum_{i = 1}^{n} v_i^{2}}
    = |\lambda|\sqrt{\sum_{i = 1}^{n}v_i^{2}} = |\lambda||v|.
  \end{eqnarray*}
\end{enumerate}  
\end{proof}


\begin{definition}[Coseno dell'angolo convesso tra due vettori]
  Dati due elementi $v, w \in \mathbb{R}^{n}$, si definisce l'angolo convesso da essi formato come
  \begin{equation}
    \theta = \arccos\frac{(v, w)}{|v||w|}.
  \end{equation}
  La dimostrazione si vedrà al capitolo 9.
\end{definition}

\clearpage
\section{Prodotto di matrici}
\begin{definition}[Prodotto tra matrici]
  Siano $m, n, l \in \mathbb{N} -\{0\}$ e sia $\mathbb{K}$ un campo. Siano $A \in M(m \times n, \mathbb{K})$ e
  $B \in M(n \times l, \mathbb{K})$. Si definisce $A \cdot B$ la matrice $m \times l$ tale che:
  \begin{equation}
    (A \cdot B)_{i, j} = \sum_{k = 1}^{n} A_{i, k}B_{k, j}
  \end{equation}
  $\forall i \in \{1, \dots, m\} \ \wedge \ \forall j \in \{1, \dots, l\}$ oppure:
  \begin{equation}
    (A \cdot B)_{i, j} = (^{t}A_{(i)}, B^{(j)})
  \end{equation}
  $\forall i \in \{1, \dots, m\} \ \wedge \ \forall j \in \{1, \dots, l\}$.
  \\Si noti che il prodotto tra matrici non gode della proprietà commutativa a causa
  della sua definizione e che il numero di colonne della prima matrice
  deve essere uguale al numero di righe della seconda.
\end{definition}

\begin{lemma}[Proprietà del prodotto tra matrici]
  Si possono allora elencare le proprietà del prodotto di matrici. 
Sia $\mathbb{K}$ un campo e siano $m, n, p, q \in \mathbb{N} - \{0\}$.
\begin{enumerate}
  \item $(AB)C = A(BC) \quad \forall A \in M(m \times n, \mathbb{K}), B \in M(n \times p, \mathbb{K}), C \in M(p \times q, \mathbb{K})$;
  \item $A(B+C) = AB + AC \quad \forall A \in M(m \times n, \mathbb{K}), B \in M(n \times p, \mathbb{K}), C \in M(n \times p, \mathbb{K})$;
  \item $(A + B)C = AC + BC \quad \forall A \in M(m \times n, \mathbb{K}), B \in M(m \times n, \mathbb{K}), C \in M(n \times p, \mathbb{K})$;
  \item $\lambda(AB) = (\lambda A)B = A(\lambda B) \quad \forall A \in M(m \times n, \mathbb{K}), B \in M(n \times p, \mathbb{K}), \lambda \in \mathbb{K}$;
  \item $AI_n = A \ \wedge \ I_mA = A \quad \forall A \in M(m \times n, \mathbb{K})$.
\end{enumerate}
\end{lemma}
\begin{proof}
  Per dimostrare l'uguaglianza fra due matrici, basta dimostrare che hanno lo stesso formato
e, che per qualsiasi $i, j$, il coefficiente $i, j$ della matrice che costituisce il primo membro
dell'uguaglianza sia uguale al coefficiente $i, j$ della matrice che costituisce il secondo membro.
\begin{enumerate}
  \item \begin{eqnarray*}
    ((AB)C)_{i, j} = \sum_{k = 1}^{p}(AB)_{i, k}C_{k, j} = \sum_{k=1}^{p} (\sum_{r=1}^{n}
    A_{i, r}B_{r, k})C_{k,j} = \\ = \sum_{k = 1}^{p}(\sum_{r = 1}^{n}A_{i, r}B_{r,k}C_{k, j}) =
    \sum_{k, r = 1}^{p, n}A_{i, r}B_{r, k}C_{k, j}; \\
    (A(BC))_{i, j} = \sum_{r = 1}^{n}A_{i, r}(BC)_{r, j} = \sum_{r = 1}^{n}A_{i, r}
    \sum_{k = 1}^{p}B_{r, k}C_{k, j} = \\ = \sum_{r = 1}^{n}(\sum_{k = 1}^{p}A_{i, r}B_{r, k}C_{k, j}) =
    \sum_{r, k = 1}^{n, p}A_{i, r}B_{r, k}C_{k, j}
  \end{eqnarray*}
  \item \begin{eqnarray*}
    (A(B + C))_{i, j} = \sum_{k = 1}^{n}A_{i, k}(B + C)_{k, j} = \sum_{k = 1}^{n}
    A_{i, k}(B_{k, j} + C_{k, j}) = \\ = \sum_{k = 1}^{n}(A_{i, k}B_{k, j} + A_{i, k}C_{k, j}) =
    \sum_{k = 1}^{n}A_{i, k}B_{k, j} + \sum_{k = 1}^{n}A_{i, k}C_{k, j} = \\ =
    (AB)_{i, j} + (AC)_{i, j} = (AB + AC)_{i, j}.
  \end{eqnarray*}
  \item Analoga a 2.
  \item \begin{eqnarray*}
    (\lambda(AB))_{i, j} = \lambda(AB)_{i, j} = \lambda \sum_{k = 1}^{n}A_{i, k}B_{k, j} = \\ =
    \sum_{k = 1}^{n} \lambda(A_{i, k}B_{k, j}) = \sum_{k = 1}^{n}(\lambda A_{i, k})B_{k, j} = \\ =
    \sum_{k = 1}^{n}(\lambda A)_{i, k}B_{k, j} = ((\lambda A)B)_{i, j}.
  \end{eqnarray*}
  Analogamente la seconda uguaglianza.
  \item \begin{eqnarray*}
    (AI_n)_{i, j} = \sum_{k = 1}^{n}A_{i, k}(I_n)_{k, j} = \sum_{k = 1}^{n} A_{i,k}\delta_{k, j} =
    A_{i, j}1 = A_{i, j}.
  \end{eqnarray*}
  Analogamente l'altra uguaglianza
\end{enumerate}
\end{proof}


\begin{definition}[Simbolo di Kronecker]
  Il simbolo di Kronecker, $\delta_{i, j}$, vale $1$
  se $i = j$ nelle matrici identità, altrimenti vale $0$.
\end{definition}

\begin{lemma}[La trasposta del prodotto è il prodotto delle trasposte scambiate]
  Sia $\mathbb{K}$ un campo e siano $m, k, n \in \mathbb{N} - \{0\}$. Si ha:
  \begin{equation}
    ^{t}(AB) = \ ^{t}B \ ^{t}A \quad \forall A \in M(m \times k, \mathbb{K}), B \in M(k \times n, \mathbb{K}).
  \end{equation}
\end{lemma}
\begin{proof}
E' facile vedere che $^{t}(AB)$ e $^{t}B \ ^{t}A$ hanno lo stesso formato, (si ha dalla definizione
di prodotto tra matrici e dalla definizione di matrice trasposta). Siano gli elementi
alla posizione $i, j$ del prodotto, $\forall i \in \{1, \dots, n\} \ \wedge \ \forall j \in \{1, \dots, m\}$, uguali a
\begin{gather*}
  (^{t}(AB))_{i, j} = (AB)_{j, i} = \sum_{t = 1}^{k} A_{j, t}B_{t, i}, \\
  (^{t}B \ ^{t}A)_{i, j} = \sum_{t = 1}^{k}(^{t}B)_{i, t}(^{t}A)_{t, j} = 
  \sum_{t = 1}^{k}B_{t, i}A_{j, t} = \sum_{t = 1}^{k}A_{j, t}B_{t, i}.
\end{gather*}
\end{proof}

\begin{lemma}[La traccia del prodotto di matrici commuta]
  Sia $n \in \mathbb{N} - \{0\}$ e $\mathbb{K}$ un campo. Siano $A, B \in M(n \times n, \mathbb{K})$, si ha:
  \begin{equation}
    tr(AB) = tr(BA).
  \end{equation}
\end{lemma}
\begin{proof}\begin{eqnarray*}
  tr(AB) = \sum_{i = 1}^{n} (AB)_{i, i} = \sum_{i = 1}^{n}\left(\sum_{k = 1}^{n}
  A_{i, k} B_{k, i}\right)  = \sum_{i = 1}^{n}\left(\sum_{k = 1}^{n} B_{k, i}A_{i, k}\right) = \\ =
  \sum_{k = 1}^{n}\left(\sum_{i = 1}^{n}B_{k, i}A_{i, k}\right) = \sum_{k = 1}^{n}(BA)_{k, k} = tr(BA).
\end{eqnarray*} 
\end{proof}


\begin{definition} [Potenza di matrici]
  Data una matrice quadrata $A$ ed un numero naturale positivo $s$, si denota con $A^{s}$ 
  il prodotto di $A$ ripetuto $s$ volte:
  \begin{equation}
    A \cdot . . .  \cdot A
  \end{equation}
\end{definition}

\section{Matrici invertibili}
\begin{definition}[Matrici invertibili]
  Sia $\mathbb{K}$ un campo e sia $n \in \mathbb{N} - \{0\}$. Sia $A \in M(n \times n, \mathbb{K})$. Si dice che
  $A$ è \textbf{invertibile} se $\exists B \in M(n \times n, \mathbb{K})$ tale che:
  \begin{equation}
    AB = BA = I_n.
  \end{equation}
  Con $GL(n, \mathbb{K})$ si denotano tutte le matrici $n \times n$ con coefficienti invertibili
  in $\mathbb{K}$.
\end{definition}

\begin{observation}[Unicità della matrice inversa]
  Dal momento che esiste un solo elemento neutro in un gruppo, se $A, B, C \in M(n \times n, \mathbb{K})$ e
$AB = BA = I_N = AC = CA$, allora $B =C$, in altre parole esiste una ed una sola matrice
$B \ | \ AB = BA = I_n$. Tale matrice si dice l'inversa di $A$ e si denota come $A^{-1}$. Si applicano
le proprietà delle potenze.
\end{observation}

\begin{example}[Formula per l'inversa di una matrice $2 \times 2$]
  Data una matrice $A$:
  \begin{gather*}
      A = \begin{pmatrix}
          a & b \\
          c & d
      \end{pmatrix} \qquad A^{-1} = \frac{1}{ad - bc} \begin{pmatrix}
          d & -b \\
          -c & a
      \end{pmatrix}
  \end{gather*}
\end{example}

\section{Matrici ortogonali}
\begin{definition} [Matrici ortogonali]
  Sia $n \in \mathbb{N} - \{0\}$. Sia $A \in M(n \times n, \mathbb{R})$. Si dice che A è \textbf{ortogonale} se
  \begin{equation}
    ^{t}AA = A \ ^{t}A = I_n.
  \end{equation}
  L'insieme delle matrici ortogonali $n \times n$ si denota come $O(n)$.
\end{definition}

\begin{observation}
  La condizione $^{t}AA = I_n$ equivale a dire che le colonne di A sono di norma 1 e
  ortogonali tra loro per il prodotto scalare standard, infatti:
  \begin{eqnarray*}
    ^{t}AA = I_n \Longleftrightarrow \\
    (^{t}AA)_{i, j} = (I_n)_{i, j} \ \forall \ i, j \in \{1, \dots, n\} \Longleftrightarrow \\
    ((^{t}A)_{(i)}, A^{(j)}) = \delta_{i, j} \ \forall \ i, j \in \{1, \dots, n\} \Longleftrightarrow \\
    (A^{(i)}, A^{(j)}) = \delta_{i, j} \ \forall \ i, j \in \{1, \dots, n\} \Longleftrightarrow \\
  \end{eqnarray*} 
  Analogamente la condizione $A\ ^{t}A = I_n$ equivale a dire che le righe di A sono
  di norma 1 e ortogonali tra loro per il prodotto scalare standard.
\end{observation}

\begin{observation}[Invertibilità delle matrici ortogonali]
  Le matrici ortogonali sono invertibili.
\end{observation}
\begin{proof}
  Dato che una matrice invertibile è tale se e solo se esiste una matrice
  $B \in M(n \times n, \mathbb{K})$, le matrici ortogonali soddisfano
  questa condizione in quanto per definizione $AB = I$. 
  In questo caso $B$ è proprio l'inversa, e dunque la trasposta, di $A$.
\end{proof}

\begin{observation}[Ortogonalità del prodotto tra matrici ortogonali]
  Il prodotto di matrici ortogonali è una matrice ortogonale.
\end{observation}
\begin{proof}
  Siano A e B due matrici ortogonali con lo stesso formato. Allora si ha:
\begin{eqnarray*}
  ^{t}(AB)(AB) = (^{t}B \ ^{t}A)(AB) = \ ^{t}B(^{t}A(AB)) = \\
  = ^{t}B((^{t}AA)B) = ^{t}B(IB) = ^{t}BB = I
\end{eqnarray*}
dove la prima uguaglianza vale per le proprietà della trasposta, mentre 
la seconda per la proprietà associativa delle matrici.
Analogamente si dimostra per $(AB) \ ^{t}(AB)$
\end{proof}

\begin{corollary}[Le matrici ortogonali costituiscono un gruppo]
  Sia $n \in \mathbb{N} - \{0\}$. L'insieme $O(n)$ con l'operazione di prodotto tra matrici
  è un gruppo.
\end{corollary}

\begin{observation}[Una matrice ortogonale conserva il prodotto scalare]
  Sia $A$ una matrice ortogonale $n \times n$. Allora: 
  \[
    (v, w) = (Av, Aw) \ \forall \ v, w \in \mathbb{R}^{n}
  \]
  Ossia la moltiplicazione per una matrice ortogonale conserva il prodotto scalare,
  conservando sia il modulo che l'angolo.
\end{observation}

\begin{proof}
Siano $v, w \in \mathbb{R}^{n}$:
\begin{gather*}
  (Av, Aw) = \ ^{t}(Av)(Aw) = (^{t}v \ ^{t}A)(Aw) = \\
  = \ ^{t}v(^{t}A(Aw)) = \ ^{t}v((^{t}AA)w) = \ ^{t}v(Iw) = \ ^{t}vw = (v, w).
\end{gather*}
\end{proof}




\chapter{Sistemi lineari}
\section{Forma matriciale dei sistemi lineari}
\begin{definition}[Sistema lineare]
  Un sistema lineare è un sistema di equazioni lineari, ossia un sistema di equazioni
  i cui membri sono polinomi di grado $\leq 1$ nelle incognite.
\end{definition}

\begin{observation}[Scrittura di un sistema lineare]
  Sia $\mathbb{K}$ un campo. Un sistema lineare di $m$ equazioni a coefficienti in $\mathbb{K}$ nelle $n$
  incognite $x_1, \dots, x_n$ si può scrivere nella forma:
  \begin{align}
      Ax = b 
  \end{align}
  per qualche matrice $A \in M(m \times n, \mathbb{K})$ e qualche $b \in \mathbb{K}^m$ dove:
  \[
    b = \left(\begin{array}{c}
      x_1, \\ \vdots \\ x_m
    \end{array}\right).
  \]
\end{observation}

\begin{definition}[Definizione di matrice incompleta e completa del sistema]
  La matrice $A$ si dice \textbf{incompleta} del sistema. La matrice $m \times (n + 1)$
  ottenuta affiancando ad A l'$m-upla \ b$ si chiama matrice \textbf{completa} del sistema
  indicata come $A|b$.
\end{definition}

\begin{example}
  \[
    \left\{ \begin{array}{l}
      2x_1  + x_3 - 6x_4 = 0 \\
      x_1 - x_4 - 6x_5 = 5 \\
      x_2 - x_3 - x_4 = 3
    \end{array}\right.
  \]
  La matrice incompleta e la colonna b sono:
  \[
    A = \left( \begin{array}{c c c c c}
      2 & 0 & 1 & -6 & 0 \\
      1 & 0 & 0 & -1 & -6 \\
      0 & 1 & -1 & -1 & 0
    \end{array}\right) \qquad b =
    \left(\begin{array}{l}
      0 \\ 5 \\ 3
    \end{array}\right)
  \]
  La matrice completa è dunque:
  \[
    A|b = \left( \begin{array}{c c c c c | c}
      2 & 0 & 1 & -6 & 0 & 0\\
      1 & 0 & 0 & -1 & -6 & 5 \\
      0 & 1 & -1 & -1 & 0 & 3
    \end{array}\right)
  \]
\end{example}

\begin{definition}[Sistema omogeneo]
  Un sistema lineare del tipo $Ax = b$ si dice \textbf{omogeneo} se $b = 0$.
\end{definition}

\section{Operazioni elementari e matrici a scalini}
\begin{definition}[Operazioni elementari di riga]
  Si definiscono \textbf{operazioni elementari} sulle righe di una matrice le seguenti
  operazioni:
  \begin{enumerate}
    \item Scambiare due righe;
    \item Moltiplicare una riga per uno scalare non nullo;
    \item Sommare ad una riga un multiplo di un'altra riga.
  \end{enumerate}
\end{definition}

\begin{definition}[Matrice a scalini]
  Sia $A$ una matrice $m \times n$; si dice che $A$ è a \textbf{scalini} se soddisfa
  la seguente proprietà: $\forall i \in \{1, \dots, m-1\} \ \wedge \ \forall k \in
  \{0, \dots, n\}$ se la riga i-esima ha i primi k elementi nulli, allora la riga $(i + 1)-esima$
  o è tutta nulla o ha almeno i primi $k + 1$ elementi nulli. \\
  Il primo elemento non nullo di ogni riga non nulla di una matrice a scalini si dice \textbf{pivot}
\end{definition}

\begin{example}
  Per esempio questa è una matrice a scalini i cui pivot sono $a_{1,1}, a_{2,2}, 
a_{3,4}$ e $a_{4,5}$:
\[
  A = \left( \begin{array}{c c c c c}
    3 & 1 & 0 & 9 & 1 \\ 
    0 & 1 & 0 & 6 & -1 \\
    0 & 0 & 0 & -3 & 4 \\
    0 & 0 & 0 & 0 & 2 \\
    0 & 0 & 0 & 0 & 0
  \end{array}
  \right)
\]
\end{example}

\begin{lemma}
  Data una qualsiasi matrice, si può ottenere da essa, con operazioni elementari di riga
  di tipo $I$ e $III$, una matrice a scalini.
\end{lemma}
\begin{proof}
  Sia $\overline{j}$ l'indice della prima colonna non nulla. Con uno scambio opportuno di righe
possiamo fare in modo che il primo coefficiente della $\overline{j}$-esima colonna sia diverso
da zero. Con operazioni di tipo tre possiamo fare in modo che tutti i coefficienti dal
secondo in poi della $\overline{j}$-esima colonna siano nulli. Ripetiamo il procedimento
togliendo la prima riga. 
\end{proof}

\begin{example}
  \[
    A = \left(\begin{array}{c c c c c}
      0 & 1 & 0 & 6 & -1 \\
      0 & 1 & 2 & 6 & -1 \\
      0 & 2 & 0 & -3 & 4 \\
      0 & 0 & 0 & 10 & 12 
    \end{array}\right)
  \]
  Con operazioni di tipo 3 annulliamo tutti i coefficienti sotto il primo elemento della
  seconda colonna. In questo caso le operazioni da fare sono: seconda riga - prima riga,
  $3^ar - 2 \cdot 1^ar$ ed infine $4^ar + \frac{2}{3}3^ar$., ottenendo:
  \[
    A = \left(\begin{array}{c c c c c}
      0 & 1 & 0 & 6 & -1 \\
      0 & 0 & 2 & 0 & 0 \\
      0 & 0 & 0 & -15 & 6 \\
      0 & 0 & 0 & 0 & 6 
    \end{array}\right)
  \]
\end{example}

\clearpage
\section{Metodo per risolvere un sistema lineare quando la sua matrice completa è a scalini}
Le incognite che stanno sopra ad un pivot si chiamano \textbf{incognite corrispondenti ad un pivot}.
Per trovare le soluzioni di un sistema lineare si seguono i seguenti passaggi (
posto che non ci siano pivot nell'ultima colonna poiché altrimenti
non sarebbe solubile:  nel sistema si avrebbe $0x_1 + \dots + 0x_n = p$.):
\begin{enumerate}
  \item Si scrive la sua matrice completa e le incognite sopra di essa;
  \item Si scrivono le incognite corrispondenti ai pivot in funzioni di quelle non corrispondenti;
  \item Si risolve e si scrive in una forma più compatta.
\end{enumerate}

\begin{example}[Sistema lineare non solubile]
Non tutti i sistemi sono solubili:
\[
  \left\{ \begin{array}{l}
    x = 1 \\ x = 2
  \end{array}\right.
\]
Non ha ovviamente soluzione.
\end{example}

\begin{example}[Risoluzione di un sistema lineare]
  \[
    \begin{array}{l}
      \quad x \quad \ y \quad z \quad w \ \ \quad u \\
      \left(\begin{array}{c c c c c | c}
        1 & 2 & 1 & -2 & -1 & 0 \\
        0 & 0 & 1 & -1 & -3 & 1 \\
        0 & 0 & 0 & 1 & -1 & 2
      \end{array}\right)
    \end{array}
  \]
  si ottiene risolvendo come detto sopra l'insieme delle soluzioni:
  \[
    \left\{\left(\begin{array}{c}
      -2y -3u + 1 \\
      y \\
      3 + 2u \\
      -u + 2 \\
      u
    \end{array}\right) | y, u \in \mathbb{R}\right\}
  \]
  oppure in forma vettoriale:
  \[
    \left\{ y \left(\begin{array}{c}
      -2 \\ 1 \\ 0 \\ 0 \\ 0
    \end{array}\right) + u \left(\begin{array}{c}
      -3 \\ 0 \\ 2 \\ -1 \\ 1
    \end{array}\right) + \left(\begin{array}{c}
      1 \\ 0 \\ 3 \\ 2 \\ 0
    \end{array} \right)| y, u \in \mathbb{R}\right\}
  \]
\end{example}

\section{Metodo per risolvere un generico sistema lineare}
\begin{lemma}[Inalterabilità dell'insieme di soluzioni]
  Facendo operazioni elementari di riga sulla matrice completa di un sistema
  lineare non si altera l'insieme delle soluzioni.
\end{lemma}
\begin{proof}
  Fare un'operazione elementare di tipo 1 sulla matrice vuol dire scambiare due equazioni
e questo non altera nulla. Fare un'operazione di tipo 2 significa moltiplicare 
entrambi i membri di un equazione per uno scalare, il che non altera assolutamente 
l'insieme delle soluzioni. Fare infine un'operazione di tipo 3 su un sistema equivale a sommare
ad un'equazione un multiplo di un'altra e questo non altera minimamente l'insieme delle soluzioni.
\end{proof}
\begin{observation}[Passi per risolvere un qualsiasi sistema lineare]
    Risolvere un sistema lineare qualsiasi:
\begin{enumerate}
  \item Scrivere la matrice completa;
  \item Ridurre la matrice completa a scalini;
  \item Si risolve il sistema associato alla matrice;
  \item L'insieme delle soluzioni è uguale all'insieme delle soluzioni iniziali.
\end{enumerate}
\end{observation}

\begin{observation}[Fare operazioni di colonna altera le variabili]
  Ovviamente fare operazioni di colonna sulla matrice incompleta di un sistema
  lineare corrisponde a considerare nuove variabili; ovvero bisogna ricordarsi di
  applicare le stesse procedure all'indietro per poter ottenere nuovamente le
  soluzioni del sistema iniziale.
\end{observation}

\begin{observation}[Ogni sistema lineare omogeneo ha almeno una soluzione]
  Un sistema lineare omogeneo ha sempre almeno una soluzione: quella nulla, Infatti
  $A0 = 0$.
\end{observation}

\begin{lemma}
  Un sistema lineare omogeneo con più incognite che equazioni ha almeno una soluzione non nulla.
\end{lemma}
\begin{proof}
  Infatti quando si riconduce la matrice a scalini, si ottiene che il numero dei
pivot è minore, o uguale,  al numero delle righe. Questo vuol dire che il numero di pivot è minore del numero
delle incognite, pertanto è possibile assegnare dei valori diversi da zero
alle incognite non assegnate ai pivot, ottenendo dunque una soluzione non nulla.
\end{proof}

\begin{lemma}[Condizione di esistenza delle soluzioni]
  Un sistema lineare ha soluzione se e solo se, riducendo la matrici completa a scalini
  con operazioni elementari di riga, non ci sono pivot nell'ultima colonna.
\end{lemma}
\begin{proof}
  Basta dimostrare che un sistema lineare la cui matrice completa è a scalini ha soluzioni
se e solo se non ci sono pivot nell'ultima colonna. Questo è ovvio poiché se ci fosse
il pivot allora l'elemento corrispondente sarebbe $\ne 0$, il che non ha senso poiché
stiamo eguagliando 0 a quell'elemento.
\end{proof}

\begin{theorem}[Teorema di struttura]
  Siano $m, n \in \mathbb{N} - \{0\}$ e sia $\mathbb{K}$ un campo. Sia $A \in M(m \times n, \mathbb{K})$ e sia $b \in \mathbb{K}^m$. Siano:
  \begin{gather*}
      S := \{x \in \mathbb{K}^n | Ax = b\} \\
    S_0 := \{x \in \mathbb{K}^n | Ax = 0\}
  \end{gather*}
  Supponendo che $S$ sia non vuoto, allora presa $\overline{} \overline{x}$ una delle soluzioni
  di $Ax = b$, si ottiene 
  \[
    S = \overline{x} + S_0,
  \]
  cioè 
  \begin{align}
          S = \{\overline{x} + y | y \in \mathbb{K}^n \ \wedge \ Ay = 0\}.
  \end{align}
\end{theorem}

\begin{proof}
  Dimostriamo che $\overline{x} + S_0 \subset S$.
Sia quindi $y \in \mathbb{K}^n$ tale per cui $Ay = 0$, allora vogliamo dimostrare che $\overline{x} + y \in S$,
ossia $A(\overline{x} + y) = b$. Per la proprietà distributiva si ottiene:
\[
  A(\overline{x} + y) = A\overline{x} + Ay = b + 0 = b.
\]
Dimostriamo che $S \subset \overline{x} + S_0$.
Sia $x \in S$, allora bisogna dimostrare che $\exists y \in \mathbb{K}^n | Ay = 0 \ \wedge \
x = \overline{x} + y$. Definendo $y = x - \overline{x}$, ovviamente $Ay = 0$, infatti:
\[
  Ay = A(x - \overline{x}) = Ax - A\overline{x} = b -b = 0.
\]
\end{proof}

\begin{lemma}[Una matrice invertibile ha solo una soluzione]
  Siano $\mathbb{K}$ un campo e $n \in \mathbb{N} - \{0\}$. Se $A \in GL(n, \mathbb{K})$ e
  $b \in \mathbb{K}^n$, il sistema lineare $Ax = b$ ha solamente una soluzione:
  \begin{align}
      A^{-1}b.
  \end{align}
\end{lemma}
\begin{proof}
  La dimostrazione segue dalla definizione di matrice invertibile:
  \begin{gather*}
      A(A^{-1} b) = b 
  \end{gather*}
  Viceversa se $x$ è tale che $Ax = b$ allora
  \begin{gather*}
      A^{-1} Ax = A^{-1} b \ \Longrightarrow \  x = A^{-1} b.
  \end{gather*}
\end{proof}



\chapter{Spazi vettoriali}
\section{Definizione e proprietà}
\begin{definition}[Spazio vettoriale]
  Sia $\mathbb{K}$ un campo. Sia $V$ un insieme dotato di due operazioni: 
  \begin{align}
    + : V \times V \to V \qquad\cdot: \mathbb{K} \times V \to V
  \end{align}
  Si dice dunque che $V$ è uno \textbf{spazio vettoriale} su $ \mathbb{K}$ i cui elementi sono vettori, mentre
  gli elementi del campo sono chiamati scalari. Valgono le seguenti proprietà:
  \begin{enumerate}
    \item \[(v + w) + u = v + (w + u) \quad \forall v,w,u \in V\]
    (proprietà associativa della somma);
    \item \[\exists ! z \in V \ | \ z + v = v + z = v \quad \forall v \in V, z = 0_V\]
    (esistenza dell'elemento neutro per la somma);
    \item \[\forall v \in V \ \exists! v' \in V \ | \ v + v' = v' + v = 0_V\]
    (esistenza ed unicità dell'inverso);
    \item \[v + w = w + v \quad \forall v, w \in V\]
    (proprietà commutativa della somma);
    \item \[( \lambda + \mu) \cdot v = \lambda v + \mu v, \quad \forall \lambda, \mu
    \in \mathbb{K}, v \in V\]
    (proprietà distributiva degli scalari rispetto alla somma);
    \item \[\lambda \cdot (v + w) = \lambda \cdot v + \lambda w \quad \forall 
    \lambda \in \mathbb{K}, v, w \in V\]
    (proprietà distributiva degli scalari rispetto al prodotto);
    \item \[(\lambda \mu) \cdot v = \lambda \cdot (\mu \cdot v) \quad \forall 
    \lambda, \mu \in \mathbb{K}, v \in V\]
    (proprietà associativa degli scalari);
    \item \[1 \cdot v = v \quad \forall v \in V\]
    (esistenza dell'elemento neutro per il prodotto).
  \end{enumerate}
  Le proprietà dalla 1 alla 4 si possono omettere semplicemente affermando che
  $V$ è un gruppo abeliano con la somma. La moltiplicazione, o divisione,
  tra vettori non esiste in uno spazio vettoriale.
\end{definition}

\begin{lemma}[Lo zero del campo coincide con lo zero dello spazio]
  Sia $V$ uno spazio vettoriale su un campo $\mathbb{K}$. Si ha:
  \begin{gather*}
    \begin{split}
      0_Kv = 0_V \quad \forall v \in V.
    \end{split}
    \\
    \begin{split}
      \lambda0_V = 0_V \quad \forall \lambda \in V.
    \end{split}
  \end{gather*}
\end{lemma}
\begin{proof}
  Si ha che:
\[
  0_K v = (0_K + 0_K)v = 0_Kv + 0_Kv
\]
Posto $w$ come l'opposto di $0_Kv$, sommando membro a membro si ottiene:
\[
  0_Kv + w = (0_Kv + 0_Kv) + w 
\]
Dunque
\[
  0_V = 0_Kv + (0_Kv + w) \ \Longrightarrow \ 0_V = 0_Kv + 0_V
\]
Infine è dimostrata:
\[
  0_V = 0_Kv  
\]

\end{proof}

\begin{lemma}[L'opposto di un vettore è il vettore per meno uno]
  Sia $V$ uno spazio vettoriale  su un campo $\mathbb{K}$. $\forall v \in V$:
  \begin{align}
      (-1_K)v = -v
  \end{align}
\end{lemma}
\begin{proof}
  è immediata poiché:
\[
  (-1_K)v + v = (-1_K)v + 1_Kv = (-1_K + 1_K)v = 0_Kv = 0_V
\]
\end{proof}

\begin{lemma}[Unicità dell'elemento opposto]
  Sia $V$ uno spazio vettoriale su un campo $\mathbb{K}$.
  \begin{align}
      \forall v \in V \ \exists! -v \ | \ v + (-v) = 0_V
  \end{align}
\end{lemma}
\begin{proof}
  Dato che un campo è anche un gruppo, allora esisterà un unico elemento
  opposto di $v$ tale che la tesi sia verificata. Se così non fosse
  allora si avrebbe che esisterebbero più elementi opposti, 
  il che vorrebbe dire andare contro la proprietà 3 dei gruppi.
\end{proof}

\begin{definition}[Combinazione lineare]
  Sia $V$ uno spazio vettoriale su un campo $\mathbb{K}$. Siano $v_1, \dots,v_k \in V$ e
  $\lambda_1, \dots, \lambda_k \in \mathbb{K}$. Si definisce \textbf{combinazione lineare dei $v$ con
  coefficienti $\lambda$}, il vettore ottenuto sommando tutti i prodotti $v_i\lambda_i$
  \begin{align}
      \sum_{i = 1}^{k} \lambda_iv_i.
  \end{align}
\end{definition}


\clearpage
\section{Sottospazi vettoriali}
\begin{definition}[Sottospazio vettoriale]
  Sia $V$ uno spazio vettoriale su un campo $\mathbb{K}$. Un sottospazio di $V$ è un 
  sottoinsieme non vuoto $S$ di $V$ tale che:
  \begin{enumerate}
    \item $v + w \in S \quad \forall v, w \in S$ (chiusura per la somma);
    \item $\lambda v \in S \quad \forall \lambda \in \mathbb{K} \ \wedge \ \forall v \in S$
    (chiusura per la moltiplicazione tra scalari).
  \end{enumerate}
  Un sottospazio vettoriale è uno spazio vettoriale anch'esso.
\end{definition}

\begin{example}[Sottospazio vettoriale]
  Il seguente insieme di vettori
  \[
    S = \left\{t \left(\begin{array}{l}
      1 \\ 2
    \end{array}\right) \ | \ t \in \mathbb{R}\right\}
  \]
  è un sottospazio vettoriale poiché sommando qualsiasi multiplo di $\left(\begin{array}{l}
    1 \\ 2
  \end{array}\right)$ o moltiplicando per qualsiasi scalare, si ottiene sempre un
  multiplo di $\left(\begin{array}{l}
    1 \\ 2
  \end{array}\right)$.
\end{example}

\begin{example}[Controesempio]
    Sia
  \[
    P = \left\{\left(\begin{array}{l}
      x \\ y
    \end{array}\right) \ | \ y = x^2\right\}
  \]
  non è un sottospazio vettoriale poiché:
  \begin{gather*}
    \begin{split}
      \left(\begin{array}{l}
        1 \\ 1
      \end{array}\right)
       \in P,
      \left(
        \begin{array}{l}
          -1 \\ 1
        \end{array}
      \right) 
      \in P
    \end{split}
    \\
    \begin{split}
      \left(\begin{array}{l}
        1 \\ 1
      \end{array}\right)
      +
      \left(
        \begin{array}{l}
          -1 \\ 1
        \end{array}
      \right) =
      \left(\begin{array}{l}
        0 \\ 2
      \end{array}\right)
      \notin P.
    \end{split}
  \end{gather*}
  Poiché non soddisfa la condizione $y = x^2$. 
\end{example}

\begin{example}[Verifica di sottospazio vettoriale con parametri]
  Il seguente sottospazio vettoriale 
  \begin{gather*}
      S_{a, b, c} = \left\{x \in \mathbb{R}^{3} : ax_1 + abx_2^{2} + (b - 1)x_3 = 0  \right\}
  \end{gather*}
  Per quali $a, b, c$ è sottospazio vettoriale? \\
  Per essere sottospazio vettoriale si deve verificare inizialmente che $0 \in S$, questa condizione si
  ha per $c = 0$. Si devono inoltre dimostrare la chiusura per la somma e per il prodotto
  per scalare, oltre che verificare la condizione del sottospazio dato. Se $a = 0 \ \vee  \ b = 0$, 
  $S_{a, b, c}$ soddisfa queste condizioni e dunque è sottospazio vettoriale; se invece $a \neq 0, b \neq 0$
  non è sottospazio vettoriale poiché i vettori
  \begin{gather*}
      \begin{pmatrix} -b \\
      1 \\
      0 \end{pmatrix}, \begin{pmatrix} -b \\
      -1 \\
      0 \end{pmatrix} \in S_{a, b, c} \qquad   \begin{pmatrix} -b \\
      1 \\
      0 \end{pmatrix} + \begin{pmatrix} -b \\
      -1 \\
      0 \end{pmatrix} = \begin{pmatrix} -2b \\
      0\\
      0 \end{pmatrix} \notin S_{a, b, c} 
  \end{gather*} 
  Allora è sottospazio vettoriale se e solo se $c = 0$ e
  almeno uno fra $a$ e $b$ è zero.
\end{example}

\begin{example}[Un esempio complesso]
  \begin{gather*}
      X_{a, b, c} = \left\{\begin{pmatrix} x \\
      y \end{pmatrix} \in \mathbb{R}^{2} : a(x^{2} + y^{2} ) + b(x + y) + c = 0\right\}
  \end{gather*}
  $X_{a, b. c}$ è un sottospazio vettoriale se
  contiene l'origine, ossia per $c = 0$ e
  quindi rimane solo da verificare che
  \begin{gather*}
      a(x^{2} + y^{2}  ) + b(x + y) = 0
  \end{gather*}
  Se $a = 0$ e $b = 0$ è tutto il piano $\mathbb{R}^{2}$, infatti tutti i
  vettori $\in \mathbb{R}^2$ soddisfano le condizioni $0 = 0$. \\
  Se $a = 0$, $b \neq 0$ è una retta per l'origine e  quindi un sottospazio vettoriale: 
  la condizione diventa
  \begin{gather*}
      x = -y
  \end{gather*}
  Se invece fosse $a \neq 0$ e $b \neq 0$  
  \begin{gather*}
      x^{2} + y^{2} + \frac{b}{a}(x + y) = 0 \\
      \left(x + \frac{b}{2a}\right)^{2} + \left(y + \frac{b}{2a}\right)^{2} - \frac{b^{2} }{4a^{2} } = 0    
  \end{gather*}
  Ossia il la circonferenza centrata in $\left(-\frac{b}{2a};\frac{b}{2a}\right)$
  e di raggio $\left|\frac{b}{2a}\right|$ e quindi non è un sottospazio vettoriale.
  E' allora un sottospazio vettoriale se e solo se $b = c = 0$ oppure se $a = c = 0$.
\end{example}

\begin{example}[Un esempio da esame]
  Sia $S_t = \{x \in \mathbb{R}^{3} : t(2x_1 - x_3) = (t - 4)x_2^{2} \}$.\\
  i) Trovare tutti i valori di $t \in \mathbb{R}$ tali che $S_t$ è un sottospazio vettoriale di $\mathbb{R}^{3}$ 
  e, per quei valori di $t$ trovati, trovare una base di $S_t$. \\
  Se $t = 0$ l'equazione diventa $-4x_2^{2} = 0$ e quindi equivale a dire che $x_2 = 0$. Allora $S_0$ è un sottospazio vettoriale e quindi una sua base è semplicemente $\left< e_1, e_3 \right>$. Se invece
  $t = 4$ allora l'equazione diventerebbe
  \begin{gather*}
      4(2x_1 - x_3) = 0 \ \Rightarrow \ x_3 = 2x_1
  \end{gather*}  
  Allora il sottospazio risultante sarà
  \begin{gather*}
      S_4 = \{x \in \mathbb{R}^{3} : x_3 = 2x_1\} = \left\{\begin{pmatrix} x_1 \\
      x_2\\
      2x_1 \end{pmatrix} : x_1, x_2 \in \mathbb{R} \right\} = \left< \begin{pmatrix} 1\\
      0\\
      2 \end{pmatrix}, \begin{pmatrix} 0\\
      1\\
      0 \end{pmatrix}  \right> 
  \end{gather*}
  Se invece si avesse che $t \neq 0, 4$, allora $S_t$ non è un sottospazio vettoriale. La controprova è la seguente: si pone
  $x_2 = \sqrt{\frac{t(2x_1 - x_3)}{t - 4}}$ e quindi si ottiene che i due vettori
  \begin{gather*}
      \begin{pmatrix} 0\\
      1\\
      -\frac{t - 4}{t} \end{pmatrix}, \begin{pmatrix} 0\\
      -1\\
      -\frac{t - 4}{t}\end{pmatrix}  \in S_t
  \end{gather*} 
  Ma la loro somma non appartiene a $S_t$. 
\end{example}


\begin{observation}[Ogni sottospazio contiene lo zero del campo]
  Sia $V$ uno spazio vettoriale su un campo $\mathbb{K}$, allora un qualsiasi suo sottospazio contiene $0_V$.
\end{observation}
\begin{proof}
  Sia $S$ un sottospazio vettoriale di $V$. Dal momento che è non vuoto, per la
proprietà della chiusura della moltiplicazione contiene anche $0_Kv$, per la 
proposizione sull'equivalenza tra lo zero del campo e lo zero
dello spazio vettoriale si ha che $0_Kv = 0_V$ e quindi $0_V \in S$.
\end{proof}

\begin{observation}[I sottospazi vettoriali sono spazi vettoriali]
  Sia $V$ uno spazio vettoriale su un campo $\mathbb{K}$. Un sottospazio di $V$ con le 
  operazioni ereditate da $V$ è lui stesso uno spazio vettoriale.
\end{observation}

\begin{lemma}[Span e sottospazi derivati da altri sottospazi]
  Sia $V$ uno spazio vettoriale su un campo $\mathbb{K}$; sia $k \in \mathbb{N}$ e siano $v_1, \dots, v_k \in V$.
  L'insieme delle combinazioni lineari sarà
  \begin{align}
      \left\{\lambda_1v_1, + \dots + \lambda_k v_k \ | \ \lambda_1, \dots, \lambda_k \in \mathbb{K}\right\}
  \end{align}
  che è esso stesso un sottospazio vettoriale di $V$. Tale sottospazio è chiamato
  \textbf{span} di $v_1, \dots, v_k$ o sottospazio generato da $v_1, \dots, v_k$
  e si denota con $\langle v_1, \dots, v_k\rangle$.
\end{lemma}
\begin{proof}
  Sia $S = \left\{\lambda_1v_1, + \dots + \lambda_k v_k \ | \ \lambda_1, \dots, \lambda_k \in \mathbb{K}\right\}$.
$S$ è non vuoto e chiuso per la somma e sommando qualsiasi $\lambda, \mu$ a $v$ si
ottiene un nuovo spazio vettoriale per le proprietà precedenti e quindi sono tutti
elementi di $S$. \\
$S$ è anche chiuso per la moltiplicazione per cui moltiplicando qualsiasi 
$\lambda_kv_k$ per un nuovo scalare $\gamma$ si ottiene un'altro spazio vettoriale
che è anch'esso elemento di $S$.
\end{proof}

\clearpage
\begin{lemma}[Inalterabilità del sottospazio generato dai soliti vettori]
  Sia $V$ uno spazio vettoriale su un campo $\mathbb{K}$. Sia $k \in \mathbb{N} - \{0\}$ e siano
  $v_1, \dots, v_k \in V$.
  \begin{enumerate}
    \item Il sottospazio generato da $v_1, \dots, v_k$ non cambia cambiando l'ordine
    dei vettori.
    \item \[\langle v_1, \dots, v_i, \dots, v_k\rangle = \langle
    v_1, \dots, \lambda v_i, \dots, v_k\rangle\] ossia $\forall \lambda \in \mathbb{K} -\{0\}$
    il sottospazio generato dai vettori non cambia se uno di loro è moltiplicato per
    uno scalare non nullo.
    \item $\forall \lambda \in \mathbb{K}$ il sottospazio generato da $v_1, \dots v_k$ non
    cambia se ad uno dei vettori è sommato il multiplo di un'altro.
  \end{enumerate}
\end{lemma}
\begin{proof}
  \begin{enumerate}
      \item Cambiando l'ordine dei generatori non cambia il sottospazio poiché
      per qualsiasi permutazione dei generatori si ha che ogni vettore in quel
      determinato spazio vettoriale è esprimibile come
      \begin{gather*}
          \sum \lambda_i v_i
      \end{gather*}
      Per cui cambiando l'ordine degli addendi non cambia il vettore risultante.
      \item Ogni combinazione lineare può essere trasformata in una combinazione
      lineare con vettori
      \begin{gather*}
          v_1, \dots, \mu v_i, \dots, v_k 
      \end{gather*}
      Ponendo allora
      \begin{gather*}
          \lambda_i v_i = \lambda_i \cdot  \mu^{-1}  \cdot  \mu \cdot  v_i
      \end{gather*}
      Si può riscrivere la combinazione ottenuta utilizzando $\mu v_i$ 
      dato che $v_i = \mu ^{-1}  \cdot  (\mu v_i)$. Poiché 
      anche $\mu v_i$ appartiene al sottospazio generato dai vettori di partenza in quanto
      si ottiene, ovviamente, moltiplicando per $\mu$ il vettore $v_i$ e moltiplicando tutti
      gli altri vettori per lo scalare nullo.
      \item Ogni combinazione lineare (dati i generatori $\left<v_1, \dots, v_k \right>$)
      dei vettori a cui si somma un multiplo di un altro generatore 
      $\left<v_1, \dots, v_i + \lambda v_j, \dots, v_k\right>$, il vettore somma può essere
      riscritto come
      \begin{gather*}
          v_i = (v_i + \lambda v_j) - \lambda v_j 
      \end{gather*}
      Ed è quindi scritta come combinazione lineare di due vettori appartenenti ai
      generatori del sottospazio, dunque appartiene anch'esso al sottospazio e allora
      non cambia se si somma un multiplo di un altro vettore di un generatore.
  \end{enumerate}
\end{proof}

\begin{theorem}[Le soluzioni di un sistema lineare sono un sottospazio vettoriale $\Longleftrightarrow$ sono omogenee]
  Sia $\mathbb{K}$ un campo. L'insieme delle soluzioni in $\mathbb{K}^n$ di un sistema lineare a coefficienti
  in $\mathbb{K}$ in $n$ incognite è un sottospazio vettoriale di $\mathbb{K}^n$ se e solo se il sistema
  lineare è omogeneo.
\end{theorem}
\begin{proof}
  Si scrive il sistema lineare nella forma matriciale $Ax = b$ e si vuole dimostrare che
che:
\[
  S := \left\{x \in \mathbb{K}^n \ | \ Ax = b\right\}
\]
è un sottospazio vettoriale se e solo se $b = 0_{\mathbb{K}^m}$. \\
$\ \Longrightarrow \ $Se $S$ è un sottospazio vettoriale allora deve contenere $0_{\mathbb{K}^n}$ quindi
$A0_{\mathbb{K}^n} = b$ e quindi $b = 0_{\mathbb{K}^m}$. 
$\ \Longleftarrow \ $Viceversa se $b = 0_{\mathbb{K}^m}$ si ha:
\[
  S = \left\{x \in \mathbb{K}^n \ | \ Ax = 0_{\mathbb{K}^m}\right\}
\]
Ed esso stesso è un sottospazio vettoriale poiché è non vuoto, è chiuso per
la somma e per la moltiplicazione tra scalari.
\end{proof}

\section{Generatori, insieme di vettori linearmente indipendenti, basi, dimensioni}
\begin{definition}[Vettori linearmente dipendenti e indipendenti]
  Sia $V$ uno spazio vettoriale su un campo $\mathbb{K}$. Siano quindi $v_1, \dots, v_k \in
  V, k \in \mathbb{N} - \{0\}$. $\{v_1, \dots, v_k\}$ è un insieme di \textbf{vettori
  linearmente dipendenti} se esistono $\lambda_1, \dots, \lambda_k \in \mathbb{K}$ non nulli
  tali per cui:
  \begin{gather}
    \lambda_1v_1 + \dots + \lambda_kv_k = 0.
  \end{gather}
  Si dice, invece, che $\{v_1, \dots, v_k\}$ è un insieme di \textbf{vettori linearmente
  indipendenti} se non è un insieme di vettori dipendenti.
\end{definition}

\begin{lemma}[Un insieme di vettori è dipendente se e solo se è combinazione di una certa base]
  Un sottoinsieme di vettori linearmente indipendenti è ancora un insieme di vettori
  linearmente indipendenti.
\end{lemma}
\begin{proof}
  Se avessi una combinazione lineare nulla con coefficienti non tutti nulli del sottoinsieme
di vettori, allora esisterebbe una combinazione nulla con coefficienti non tutti nulli
dell'insieme di vettori.
\end{proof}

\begin{lemma}[Condizioni di dipendenza lineare]
  Sia $V$ uno spazio vettoriale su un campo $\mathbb{K}$. Siano $v_1, \dots, v_k \in V$ con
  $k \in \mathbb{N} -\{0, 1\}$. L'insieme $\{v_1, \dots, v_k\}$ è un insieme di vettori dipendenti
  di $V$ se e solo se $\exists i \in \{1, \dots, k\} \ | \ v_i$ è una combinazione
  lineare di $v_1, \dots, v_{i-1}, v_{i+1}, \dots, v_k$.
\end{lemma}
\begin{proof}
$\Longrightarrow$ Supponendo che $v_1, \dots, v_k$ siano un insieme di vettori linearmente dipendenti,
allora $\exists \lambda_1, \dots, \lambda_k \in \mathbb{K} \ne 0 $ :
\begin{gather*}
  \lambda_1v_1 + \dots + \lambda_kv_k = 0
\end{gather*}
Sia quindi $i \ | \ \lambda_i \ne 0$. Pertanto si ha:
\begin{gather*}
  v_i = -\frac{\lambda_1}{\lambda_i}v_1 - \dots - \frac{\lambda_{i-1}}{\lambda_i}v_{i-1}
  -\frac{\lambda_{i+1}}{\lambda_i}v_{i+1} - \dots - \frac{\lambda_k}{\lambda_i}v_k.
\end{gather*}
$\Longleftarrow$
Supponendo che $\exists i \in \{1, \dots, k\} \ | \ v_i$ sia una combinazione lineare di
$v_1, \dots, v_{i-1}, v_{i+1}, \dots, v_k$; quindi $\exists \mu_1, \dots, \mu_{i-1},
\mu_{i+1}, \dots, \mu_k \in \mathbb{K} \ |$:
\begin{gather*}
  v_i = \mu_1v_1 + \dots + \mu_{i-1}v_{i-1} + \mu_{i+1}v_{i+1} + \dots + \mu_kv_k
\end{gather*} 
Pertanto
\begin{gather*}
  -\mu_1v_1 - \dots - \mu_{i-1}v_{i-1} + v_i -\mu_{i+1}v_{i+1} -\dots - \mu_kv_k = 0
\end{gather*}
Quindi esiste una combinazione nulla lineare di $v_1, \dots, v_k$ con coefficienti
non tutti nulli e quindi $\{v_1, \dots, v_k\}$ è un insieme di vettori dipendenti
(infatti il coefficienti di $v_i$ è 1).
\end{proof}

\begin{definition}[Vettori generatori]
  Sia $V$ uno spazio vettoriale su un campo $\mathbb{K}$. Siano $v_1, \dots, v_k \in V$ (con
  $k \in \mathbb{N} - \{0\}$). L'insieme $\{v_1, \dots, v_k\}$ si dice un \textbf{insieme
  di generatori} per $V$ se ogni elemento di $V$ è combinazione lineare di questi vettori.
\end{definition}

\begin{definition}[Base di uno spazio]
  Sia $V$ uno spazio vettoriale su un campo $\mathbb{K}$. Siano $v_1, \dots, v_k \in V, k \in
  \mathbb{N} - \{0\}$. L'insieme $\{v_1, \dots, v_k\}$ si dice \textbf{base} di $V$ se è un
  insieme di vettori linearmente indipendenti che genera $V$.
\end{definition}

\begin{definition}[Base canonica]
  La base canonica di un certo spazio $V \subset \mathbb{K }^{n} $ è definita come
  i vettori che hanno alla componente $i$-esima $1$ e zero a tutte le altre.
\end{definition}

\begin{example}[Base canonica]
  La base canonica di un certo sottospazio di dimensione 3 è:
  \begin{gather*}
      V \subset \mathbb{K }^{3}  \Rightarrow \left< \begin{pmatrix} 1\\
      0\\
      0 \end{pmatrix}, \begin{pmatrix} 0\\
      1\\
      0 \end{pmatrix}, \begin{pmatrix} 0\\
      0\\
      1 \end{pmatrix}    \right> 
  \end{gather*}
  E' base canonica di $V$.
\end{example}

\begin{example}[Altro esempio sulla base canonica]
  Sia $n \in \mathbb{N} - \{0\}$ e sia $\mathbb{K}$ un campo. $\forall i \in \{1, \dots, n\}, e_i \in \mathbb{K}^n
\ | \ e_i = 1$ e gli altri nulli. L'insieme $\{e_1, \dots, e_n\}$ è una base di $\mathbb{K}^n$
detta \textbf{base canonica} poiché si ha necessariamente che se
\begin{gather*}
  \lambda_1e_1 + \dots + \lambda_ne_n = 0
\end{gather*}
allora $\lambda_1 = \dots = \lambda_n = 0$ e quindi $\{e_1, \dots, e_n\}$ è un insieme
di vettori indipendenti . Inoltre $\forall x \in \mathbb{K}^n$  è combinazione di $e_1, \dots, e_n$
\begin{gather*}
  x = x_1e_1 + \dots + x_ne_n
\end{gather*}
e quindi $\{e_1, \dots, e_n\}$ è anche insieme di generatori.
\end{example}

\begin{example}[Vettori generatori]
  Siano $v_1, v_2, v_3, v_4$ i seguenti vettori in $\mathbb{R}^3$:
\begin{gather*}
  v_1 = \left(\begin{array}{l}
    1 \\ 2 \\ 0
  \end{array}\right) \quad
  v_2 = \left(\begin{array}{l}
    1 \\ 0 \\ 1
  \end{array}\right) \quad
  v_3 = \left(\begin{array}{l}
    0 \\ 2 \\ 3
  \end{array}\right) \quad
  v_4 = \left(\begin{array}{l}
    2 \\ 2 \\ 1
  \end{array}\right).
\end{gather*}
Perché questi siano generatori bisogna che:
\begin{gather*}
  \lambda_1\left(\begin{array}{l}
    1 \\ 2 \\ 0
  \end{array}\right) + 
  \lambda_2\left(\begin{array}{l}
    1 \\ 0 \\ 1
  \end{array}\right) +
  \lambda_3\left(\begin{array}{l}
    0 \\ 2 \\ 3
  \end{array}\right) + 
  \lambda_4 \left(\begin{array}{l}
    2 \\ 2 \\ 1
  \end{array}\right) = 
  \left(\begin{array}{l}
    x_1 \\ x_2 \\ x_3
  \end{array}\right)
\end{gather*}
Creando quindi la matrice completa di questo sistema lineare si osserva che è solubile
e quindi attraverso la risoluzione del sistema si trovano i $\lambda$ tali che
soddisfano l'ipotesi, quindi l'insieme di questi vettori è un insieme di generatori di $\mathbb{R}^3$,
anche se questi vettori non sono linearmente indipendenti poiché uno è combinazione lineare
degli altri:
\begin{gather*}
  v_1 + v_2 + 0v_3 = v_4
\end{gather*}
\end{example}

\begin{theorem}[I vettori linearmente indipendenti sono $\leq$ dei generatori di uno spazio]
  Sia $V$ uno spazio vettoriale su un campo $\mathbb{K}$. Siano $k, s \in \mathbb{N} -\{0\}$. \\
  Sia $\{v_1, \dots, v_k\}$ un insieme di vettori indipendenti di $V$. \\
  Sia $\{w_1, \dots, w_s\}$ un insieme di generatori di $V$. \\
  Allora $k \leq s$.
\end{theorem}
\begin{proof}
  Dato che $\{w_1, \dots, w_s\}$ è un insieme di generatori, allora $\exists \lambda_{i, j}
\in \mathbb{K}$ : 
\begin{gather*}
  v_i = \sum_{j = 1}^{s} \lambda_{i, j} w_{j} \qquad \text{per} \ i = \{1, \dots, k\}.
\end{gather*}
Siano dunque $x_1, \dots, x_k \in \mathbb{K}$. Considerando la combinazione lineare di 
$v_1, \dots, v_k$ con coefficienti $x_1, \dots, x_k$ si ha che: 
\begin{gather*}
  \begin{split}
    \sum_{i = 1}^{k} x_i v_i = \sum_{i = 1}^{k}x_i \cdot\left(\sum_{j = 1}^{s}
    \lambda_{i, j}w_{j}\right) = \sum_{i = 1}^{k} \sum_{j = 1}^{s} x_i \lambda_{i, j}w_j =
  \end{split}
  \\
  \begin{split}
    = \sum_{j = 1}^{s} \sum_{i = 1}^{k} x_i \lambda_{i, j}w_{j} = \sum_{j = 1}^{s}
    \left(\sum_{i = 1}^{k} x_i \lambda_{i, j}\right)w_j.
  \end{split}
\end{gather*} 
Se, per assurdo, si avesse che $k > s$, il sistema lineare omogeneo nelle incognite
$x_i$ dato dalle $s$ equazioni 
\begin{gather*}
  \sum_{i = 1}^{k} x_i\lambda_{i, j} = 0
\end{gather*}
per $j = 1, \dots, s$, secondo la proposizione sull'esistenza di soluzioni non nulle,
si avrebbe almeno una soluzione non nulla in quanto si avrebbero più incognite che equazioni  $(\overline{x_1}, \dots, \overline{x_k})$.
Quindi si avrebbe
\begin{gather*}
  \sum_{i = 1}^{k} \overline{x_i}v_i = 0
\end{gather*}
contraddicendo l'ipotesi secondo la quale $\{v_1, \dots, v_k\}$ è un insieme di vettori
indipendenti di $V$. Allora deve necessariamente risultare che $k \leq s$.
\end{proof}

\begin{corollary}[Due basi di uno spazio vettoriale hanno lo stesso numero di vettori]
  Sia $V$ uno spazio vettoriale su un campo $\mathbb{K}$. Siano $\{v_1, \dots, v_k\}$ e 
  $\{w_1, \dots, w_s\}$ due basi di $V$ (con $k, s \in \mathbb{N} - \{0\})$. Allora 
  $k = s$. 
\end{corollary}
\begin{proof}
  Deriva direttamente dal teorema precedente in quanto una base è un insieme
  di vettori indipendenti.
\end{proof}

\begin{example}[Esempio da esame]
  Sia $V$ uno spazio vettoriale su un campo $\mathbb{K}$. Siano $A$ e $B$ due sottoinsiemi finiti di $V$ tali che $A$ è un insieme
  di vettori linearmente indipendenti e $B$ è un insieme di generatori di $V$ e $A \cap  B \neq  \empty$. Allora se $A \cap B$ è una base,
  allora $A$ è una base. Questo è vero perché $B$ è un insieme di generatori, dunque contiene
  sia vettori linearmente indipendenti che vettori linearmente dipendenti. Inoltre
  $B$, generando lo spazio $V$, per il teorema di prima vuol dire che presenterà un numero di
  vettori linearmente indipendenti minore o uguale al totale. Allora, se
  l'intersezione tra $A$ e $B$ è una base, necessariamente anche $A$ è una base
  in quanto è un insieme di vettori linearmente indipendenti. 
\end{example}

\begin{definition}[Dimensione di uno spazio vettoriale]
  Sia $V$ uno spazio vettoriale su un campo $\mathbb{K}$. Se $\{v_1, \dots, v_k\}$ è una base
  di $V$ (con $k \in \mathbb{N} - \{0\}$), allora $k$ si dice \textbf{dimensione} di $V$.
  Se $V = \emptyset$ allora si dice che $V$ ha dimensione zero. La dimensione di $V$
  si denota come $\dim_{\mathbb{K}} (V)$ o semplicemente $\dim (V)$ quando il campo è chiaro dal contesto.
\end{definition}

\begin{definition}[Coordinate di uno spazio vettoriale]
  Sia $V$ uno spazio vettoriale su un campo $\mathbb{K}$ e sia $\{v_1, \dots, v_k\}$ una
  base di $V$ (con $k \in \mathbb{N} -\{0\}$). Ogni elemento $v \in V$ si scrive in modo \textbf{unico} come
  combinazione lineare di elementi di $\{v_1, \dots, v_k\}$. I coefficienti di tale 
  combinazione si dicono \textbf{coordinate} di $v$ rispetto alla base.
\end{definition}
\begin{proof}
  Ovviamente ogni vettore $v \in V$ si scrive come combinazione lineare di $V$,
  dato che $\{v_1, \dots, v_k\}$ è una base di $V$, e quindi un insieme di generatori,
  si suppone che esistano due set di coordinate per lo stesso vettore $v$:
  \begin{align*}
    v = \lambda_1 v_1 + \dots + \lambda_k v_k \\
    v = \mu_1 v_1 + \dots + \mu_k v_k
  \end{align*}
  con $\lambda_1, \dots, \lambda_k$, $\mu_1, \dots, \mu_k$ $\in \mathbb{K}$. Allora, per definizione di base
  \begin{gather*}
    \lambda_1 v_1 + \dots + \lambda_k v_k = \mu_1 v_1 + \dots + \mu_k v_k \\
    (\lambda_1 - \mu_1)v_1 + \dots + (\lambda_k -\mu_k)v_k = 0 \\
    \lambda_1 - \mu_1 = \dots = \lambda_k - \mu_k = 0
  \end{gather*} 
  In quanto $\{v_1, \dots, v_k\}$ è un insieme di vettori linearmente indipendenti.
  Pertanto si ha che:
  \[
    \lambda_1 = \mu_1 , \dots , \lambda_k = \mu_k.
  \]

\end{proof}

\begin{observation}
  Se $\{v_1, \dots, v_k\}$ non è una base di $V$ allora $\left<v_1, \dots, v_k\right> \neq V$. Quindi 
  scegliendo qualsiasi vettore $v_{k+1}$ si ha ancora un insieme di vettori linearmente
  indipendenti. Ossia siano $\lambda_1, \dots, \lambda_{k+1} \in \mathbb{K}$:
  \begin{gather}
      \sum_{i = 1}^{k +1} \lambda_i v_i = 0
  \end{gather}
  Dal momento che $\lambda_{k + 1} = 0$, altrimenti sarebbe combinazione lineare,
  si ha che anche gli altri $\lambda_i$ sono uguali a zero poiché sono vettori indipendenti.
\end{observation}

\begin{theorem}[Completamento di un insieme di vettori indipendenti ad una base]
  Siano $k, n \in \mathbb{N} - \{0\}$. Dato un insieme di vettori linearmente indipendenti $\{v_1, \dots, v_k\}$ 
  in uno spazio vettoriale $V$ di dimensione $n$ allora si ha che $k \leq n$:
  \begin{enumerate}
    \item Se $k = n$ allora $\{v_1, \dots, v_k\}$ è una base di $V$;
    \item Se invece $k < n$ è sempre possibile trovare $v_{k + 1}, \dots, v_n \in V :  \{v_1, \dots, v_n\}$ è una base di $V$.
  \end{enumerate}
\end{theorem}

\begin{proof}
  Il fatto che $k \leq n$ segue dal teorema sull'inferiorità dei vettori
  linearmente indipendenti rispetto a quelli della base quindi, per l'osservazione di prima, si ha che:
  se $k = n, \{v_1, \dots, v_k\}$ è una base di $V$, altrimenti si avrebbe un insieme
  di $k + 1$ vettori indipendenti e si avrebbe $k  +  1 \leq n$ il che è assurdo poiché
  $k = n$. Se $k < n$ allora si aggiungono vettori fino ad $n$.
\end{proof}

\clearpage
\begin{theorem}[Estrazione di una base da un insieme di generatori]
  Sia $V$ uno spazio vettoriale e siano $k, n \in \mathbb{N} - \{0\}$: 
  \begin{enumerate}
      \item se $\{v_1, \dots, v_k\}$ è un insieme di generatori di V, si può trovare un sottoinsieme di essi che è una base di $V$, in altre parole si estrae una base di $V$;
  \item se $\dim(V) = n$, e $\{v_1, \dots, v_n\}$ è un insieme di generatori di $V$, allora
  $\{v_1, \dots, v_n\}$ è una base di $V$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  1.) Se $\{v_1, \dots, v_k\}$ non è già una base, allora $\exists v$ combinazione
  lineare dei rimanenti, dunque essi formano ancora un insieme di generatori di $V$
  per cui si può considerare il caso in cui solamente l'ultimo elemento sia linearmente
  dipendente rispetto agli altri:
  \begin{gather*}
      v_k = \sum_{i = 1}^{k -1} c_i v_i
  \end{gather*}
  per certi $c_i \in \mathbb{K}$; sia $v \in V$; dato che i vettori $v_1, \dots, v_k$ generano $V$
  allora $\exists \lambda_1, \dots, \lambda_k$ tali che:
  \begin{gather*}
      v = \sum_{i = 1}^{k} \lambda_i v_i \\
      v = \sum_{i = 1}^{k - 1} \lambda_i v_i + \lambda_k \left(\sum_{i = 1}^{k - 1}c_i v_i \right)  = \sum_{i = 1}^{k - 1} (\lambda_i + \lambda_k c_i)v_i 
  \end{gather*}
  Questo vale perché l'ultimo elemento della sommatoria fino a $k$
  è esattamente $\lambda_k v_k$ che si può sostituire con l'espressione 
  $\lambda_k \sum c_i \lambda_i$. 
  Quindi $v$ è combinazione lineare e pertanto sono generatori (fino a $v_{k - 1}$)  e alla fine
  si arriva ad estrarre una base da $\{v_1, \dots, v_k\}$. \\
  2.) Per la prima parte di dimostrazione si ha che anche la seconda parte del teorema è dimostrata
  in quanto da $\{v_1, \dots, v_n\}$ si estrae una base costituita da $n$ elementi e coincide con $\{v_1, \dots, v_n\}$.
\end{proof}

\begin{theorem}[Un sottospazio vettoriale ha dimensione minore o uguale allo spazio di partenza]
  Siano $k, n \in \mathbb{N}$. Se S è un sottospazio vettoriale di dimensione $k$ di uno spazio
  vettoriale $V$ di dimensione $n$ allora si ha che $k \leq n$. Se $k = n$ allora $S = V$.
\end{theorem}
\begin{proof}
  Se $S = \{0_V\}$ l'affermazione è ovvia. Supponendo allora che $\dim(S) > 0$, sia 
  $\{v_1, \dots, v_k\}$ una base di $S$. Essendo che $\{v_1, \dots, v_k\}$ sono un insieme di vettori
  linearmente indipendenti in $S$, è anche un insieme di vettori linearmente indipendenti in $V$ e quindi $k \leq n$.
  Supponendo adesso che $ k = n$, per il teorema del completamento, si ottiene che l'insieme di vettori $\{v_1, \dots, v_k\}$ è una base di $V$. Se $v \in V$, allora $v$
  si può scrivere come combinazione lineare di quei vettori e quindi, dato che gli stessi
  sono elementi di $S$, si ha che $v \in S$ e $V \subset S$. Quindi $V = S$.
\end{proof}

\begin{definition}[Nomi degli spazi vettoriali]
  Sia $n \in \mathbb{N} -\{0\}$. Sia $V$ uno spazio vettoriale di dimensione $n$ su un campo $\mathbb{K}$.\\
  Un sottospazio vettoriale di V di dimensione 1 si dice \textbf{retta} (vettoriale).\\
  Un sottospazio vettoriale di V di dimensione 2 si dice \textbf{piano} (vettoriale).\\
  Un sottospazio vettoriale di V di dimensione $n - 1$ si dice \textbf{iperpiano} (vettoriale). 
\end{definition}

\clearpage
\section{Somma di sottospazi e formula di Grassmann}
\begin{definition}[Definizione del prodotto cartesiano tra due spazi vettoriali]
  Siano $V$ e $W$ due sottospazi vettoriali su uno stesso campo $\mathbb{K}$. Allora sul prodotto cartesiano
$V \times W$ si possono definire un'operazione di somma ed una di moltiplicazione degli scalari
nel modo seguente:
\begin{align}
    (v, w) + (v', w') &= (v + v', w + w') \\
    c(v, w) &= (cv , cw) 
\end{align} 
$ \forall v \in V, w \in W, c \in \mathbb{K} $. Da notare che questa definizione non
implica il prodotto scalare delle componenti ma indica le coppie di vettori $(v, w)$. 
Se lo spazio $A$ ha $n$ elementi e lo spazio $B$ ha $m$ elementi, allora lo spazio prodotto
cartesiano avrà $mn$ elementi. Un caso importante è $A = \mathbb{R}$, il prodotto $A \times A = \mathbb{R}^{2}$ 
sarà dunque il piano cartesiano e gli elementi sono i punti del piano $\mathbb{R}^{2} = \{(x, y) \ x \in \mathbb{R}, \ y \in \mathbb{R}\}$. 
\end{definition}

\begin{lemma}[Dimensione dello spazio prodotto cartesiano]
  Siano $V, W$ due spazi vettoriali di dimensione finita: allora su uno stesso campo $\mathbb{K}$
  \begin{align}
    \dim(V \times W) = \dim(V) + \dim(W).
  \end{align}
\end{lemma}

\begin{proof}
  Supponendo che la dimensione di entrambi gli spazi vettoriali sia maggiore di zero,
  se $\{v_1, \dots, v_k\}$ è una base di $V$ e $\{w_1, \dots, w_m\}$ è una base di $W$,
  si verifica che i $k + m$ elementi $(v_i, 0), (0, w_j)$ formano una base di $V \times W$.
\end{proof}

\begin{definition}[Sottospazio somma]
  Se $A$ e $B$ sono due sottospazi di uno spazio vettoriale $V$ 
  si definisce il \textbf{sottospazio somma} come il più piccolo
  sottospazio vettoriale contenente $A$ e $B$ 
  \begin{align}
      A + B := \{a + b \ : \ a \in A, b \in B\}
  \end{align}
\end{definition}

\begin{example}[Esempio di un sottospazio somma]
  Se $A = \left< e_1 \right> $ e $B = \left< e_2 \right>$ sottospazi di $\mathbb{R}^3$
  allora il sottospazio somma è dato proprio dalla somma delle due basi
  \begin{gather*}
      A + B = \left< e_1, e_2 \right> 
  \end{gather*} 
  (In generale questo non è sempre vero ma dipende dalla dimensione
  dell'intersezione dei due spazi vettoriali).
\end{example}

\begin{observation}[La somma diretta di due sottospazi di uno spazio è anch'essa sottospazio dello spazio di partenza]
  Siano $A, B$ due sottospazi vettoriali di uno spazio $V$ allora $A + B$ è sottospazio di $V$.
\end{observation}
\begin{proof}
  La dimostrazione deriva dal fatto che il sottospazio somma $A + B$
  rappresenta il più piccolo sottospazio vettoriale contenente $A$ e $B$.
  Data allora una base di $A$ $\{a_1, \dots, a_k\}$ e una di $B$
  $\{b_1, \dots, b_k\}$, si osserva, dato che $A \subset V, B \subset V$ che
  i vettori di entrambe le basi sono una combinazione 
  lineare della base di $V$ $\{v_1, \dots, v_k\}$. \\
  Prendendo in considerazione
  \begin{gather*}
      A + B = \{a + b : a \in A, b \in B\}
  \end{gather*}
  Si osserva subito che anche la somma dei vettori appartenenti ai due spazi vettoriali
  appartiene a $V$:
  \begin{gather*}
      a \in V, b \in V, \ \forall a, b \in A, B \ \Longrightarrow \  a + b \in V
  \end{gather*}
  In quanto $V$ è uno spazio vettoriale e quindi, per definizione, è chiuso per la somma,
  allora, se $a + b \in V$, necessariamente $A + B \subset V$.
\end{proof}

\begin{definition}[Somma diretta di due sottospazi]
  Se l'intersezione tra due sottospazi vettoriali $A$ e $B$
  di un certo spazio vettoriale $V$ è vuota, allora si dice che
  $A + B$ è \textbf{somma diretta} e si denota con
\begin{align}
    A \oplus  B
\end{align}
\end{definition}

\begin{observation}[Ogni elemento nella somma diretta è unico]
  Se la somma di due sottospazi è somma diretta, allora ogni elemento
  della somma si scrive in modo unico come somma di un elemento di
  $A$ e uno di $B$.
\end{observation}
\begin{proof}
  Se $a \in A, b \in B$, allora se ogni elemento somma non fosse
  unico si avrebbe:
  \begin{gather*}
      a + b = a' + b' \ \Longrightarrow \ a - a' = b' - b
  \end{gather*}
  Dato che $A$ e $B$ sono chiusi per la somma allora 
  \begin{gather*}
    a - a' \in A, \qquad b' - b \in B
  \end{gather*}
  Entrambi appartengono all'intersezione (che per ipotesi è vuota) e dunque si ha che
  \begin{gather*}
      a - a' = 0 = b' - b \ \Longrightarrow \  a = a', \ b = b'
  \end{gather*}
\end{proof}

\begin{theorem}[Formula di Grassmann]
    Se $A, B$ sono due sottospazi di dimensione finita di uno spazio vettoriale $V$ allora
    vale la seguente formula:
    \begin{align}
        \dim( A + B) + \dim(A \cap B) = \dim(A) + \dim(B)
    \end{align}
\end{theorem}

\begin{proof}
  Sia $\{v_1, \dots, v_k\}$ una base di $A \cap  B$ allora è possibile estendere questo insieme
  di vettori indipendenti in modo da avere una base di $A$ e di $B$:
  \begin{gather*}
      A = \left<v_1, \dots, v_k, u_1, \dots, u_t\right> \qquad B = \left<v_1, \dots, v_k, w_1, \dots, w_s\right>
  \end{gather*}
  Quindi essendo $\dim(A) = k + t$ e $\dim(B) = k + s$ si dimostra che:
  \begin{gather*}
      \dim(A + B) = k + t + k + s - k = k + t + s
  \end{gather*}
  Questo si dimostra provando che $ \{v_1, \dots, v_k, u_1, \dots, u_t , w_1, \dots, w_s\}$ è una
  base di $A + B$. \\
  Si dimostra che è un insieme di generatori $A + B$. $\forall v \in A + B, \exists a \in A,
  b \in B : v = a + b$. $a$ è un elemento di A ed è combinazione lineare della sua base e anche
  $b$ è combinazione della base di $B$ quindi il vettore $v = a + b$ sarà combinazione della base di $A + B$. \\
  Dimostriamo ora che la base è linearmente indipendente: Siano $\lambda_k, \gamma_t, \mu_s \in \mathbb{K}$ 
  tali che:
  \begin{gather*}
      \lambda_1 v_1 + \dots + \lambda_k v_k + \gamma_1 u_1 + \dots + \gamma_t u_t + \mu_1 w_1 + \dots + \mu_s w_s = 0. \\
      -\lambda_1 v_1 \dots - \lambda_k v_k - \gamma_1u_1 \dots - \gamma_t u_t = \mu_1 w_1 + \dots + \mu_s w_s.
  \end{gather*}
  Il secondo membro appartiene a $B$ ed il primo ad $A$ quindi entrambi appartengono a $A \cap B$.
  Il secondo membro si può scrivere come combinazione lineare dell'intersezione:
  \begin{gather*}
      \mu_1 w_1 + \dots + \mu_s w_s = \delta_1 v_1 + \dots + \delta_k v_k \\
      \mu_1 w_1 + \dots + \mu_s w_s - \delta_1 v_1 \dots - \delta_k v_k  = 0
  \end{gather*} 
  Dato che $v$ e $w$ sono indipendenti, si deduce dall'ultima uguaglianza che
  $\mu_1 = \dots = \mu_s = \delta_1 = \dots = \delta_k = 0$. Dunque anche il primo membro di quelle prima è indipendente
  perché è appartenente ad $A$ e quindi $\lambda_1 = \dots = \lambda_k = \gamma_1 = \dots = \gamma_t = 0$ e quindi tutti i vettori
  sono linearmente indipendenti.
\end{proof}

\begin{observation}[Dimensione dell'intersezione uguale a zero]
  Dalla formula di Grassmann si deduce che, nel caso della somma diretta, la sua
  dimensione è proprio la somma della dimensione di $A$ e della dimensione di $B$.
\end{observation}

\begin{example}
  Siano $A, B$ sottospazi vettoriali di $\mathbb{C}^{8}$ con $\dim A = 5$ e $\dim(A \cap B) = 2$.
  Quale è il valore minimo ed il valore massimo per la dimensione di $B$?
  \begin{gather*}
      \dim A + \dim B = \dim (A + B) + \dim(A \cap B) 
  \end{gather*}
  Allora la dimensione di $B$ sarà proprio
  \begin{gather*}
      \dim B = \dim(A + B) + 2 - 5 \\
      \dim B = \dim (A + B) - 3
  \end{gather*}
  Dato che $A + B$ è sottospazio vettoriale di $\mathbb{C}^8$
  allora necessariamente si deve avere 
  \begin{gather*}
      \dim(A + B) \leq 8 \ \Longrightarrow \ \dim B \leq 5
  \end{gather*}
  Allora dato che $A \cap  B$ è un sottospazio vettoriale di $B$ si deve
  anche avere che $\dim(B) \geq \dim(A \cap B) = 2$ e quindi si ha anche che$\dim(B) \geq 2$.
\end{example}

\clearpage
\section{Dimensione di $M(m \times n, \mathbb{K})$ e di $K_d[x]$}
\begin{observation}[Dimensione dello spazio delle matrici]
  Sia $\mathbb{K}$ un campo e siano $m, n \in \mathbb{N} - \{0\}$. La dimensione di $M(m \times n, \mathbb{K})$ è $mn$. 
  Ma lo spazio vettoriale che definisce è di dimensione pari al numero di pivot
  della matrice (cap. 6).
\end{observation}
\begin{proof}
  Sia $E_{i, j}$ la matrice $m \times n$ con il coefficiente $i, j$ uguale ad
  uno e tutti gli altri nulli. L'insieme di $\{E_{i, j}\}_{i = 1, \dots, m; j = 1, \dots, n}$ è una
  base di $M(m \times n, \mathbb{K})$ infatti se $A \in M(m \times n, \mathbb{K})$ allora si ha:
  \begin{gather*}
      A = \sum_{i , j= 1}^{m, n} a_{i, j}E_{i, j} 
  \end{gather*}
  Quindi l'insieme è un insieme di generatori per $M (m \times n, \mathbb{K})$ e si dimostra che sono
  un insieme di vettori linearmente indipendenti con dimensione $mn$. 
\end{proof}

\begin{definition}[Spazio dei polinomi]
  Sia $\mathbb{K}$ un campo e sia $d \in \mathbb{N}$. Sia $K_d[x]$ l'insieme dei polinomi di grado $\leq d$ definito come:
  \begin{align}
      K_d[x] = \{a_0 + a_1x + \dots + a_dx^{d} | a_0, \dots, a_d \in \mathbb{K}\}
  \end{align}
  Allora tale insieme è uno spazio vettoriale.
\end{definition}
\begin{proof}
  Per soddisfare le condizioni di spazio vettoriale bisogna provare la chiusura
  per la somma e la chiusura per il prodotto con uno scalare.
  \begin{gather*}
      (a_0 + a_1 x + \dots + a_d x^{d}) +  (b_0 + b_1 x + \dots + b_d x^{d}) 
  \end{gather*}
  E ovviamente $\in K_d[x]$: 
  \begin{gather*}
      (a_0 + b_0) + (a_1 + b_0)x + \dots + (a_d + b_d)x^{d} \quad a_0 + b_0 = c_0, \dots \ \Longrightarrow \   \in K_d[x] 
  \end{gather*}
  Per la chiusura di uno scalare:
  \begin{gather*}
      \lambda(a_0 + a_1 x + \dots + a_d x^{d}) \\
      (\lambda a_0) + (\lambda a_1) x + \dots + (\lambda a_d )x^{d} 
  \end{gather*}
  Posto $\lambda a_0 = b_0, \dots, \lambda a_d = b_d$ allora $\in K_d[x]$.
\end{proof}

\begin{observation}[Dimensione dello spazio polinomiale]
  La dimensione di $K_d[x]$ è $d + 1$. L'insieme $\{1, x, \dots, x^{d} \}$ è ovviamente base di $K_d[x]$.
\end{observation}
'insieme $\{1, x, \dots, x^{d} \}$ è ovviamente base di $K_d[x]$.




\chapter{Applicazioni lineari}
\section{Applicazioni lineari, nucleo, immagine e relazioni fondamentale}
\begin{definition}[Applicazione lineare]
  Siano V e W due spazi vettoriali su un campo $\mathbb{K}$. Si definisce $f : V \to W$ \textbf{funzione
  lineare} se valgono le seguenti:
  \begin{enumerate}
    \item $f(v_1 + v_2) = f(v_1) + f(v_2) \quad \forall v_1, v_2 \in V$;
    \item $f(\lambda v) = \lambda f(v) \quad \forall v \in V, \lambda \in \mathbb{K}$.
  \end{enumerate}
\end{definition}

\begin{example}[Applicazione lineare]
  L'applicazione  $f : \mathbb{R} \to \mathbb{R}$ definita come
  \begin{gather*}
      f(x) = 2x
  \end{gather*}
  E' lineare infatti:
  \begin{gather*}
      f(2 + 3) = 2(5) = 10 \quad
      f(2) + f(3) = 2(2) + 2(3) = 10
  \end{gather*}
\end{example}

\begin{example}[Applicazione non lineare]
  L'applicazione $g : \mathbb{R} \to \mathbb{R}$ definita come:
  \begin{gather*} 
      g(x) = x + 1
  \end{gather*}
  Non è lineare perché
  \begin{gather*}
      g(2 + 3) = (5) + 1 = 6 \quad
      g(2) + g(3) = 3 + 4 = 7
  \end{gather*}
\end{example}

\begin{definition}[Nucleo delle funzioni lineari]
  Siano V e W due spazi vettoriali su $\mathbb{K}$ e sia $ f: V \to W$ una funzione lineare. Si
  definisce \textbf{nucleo} o $\ker$(Kernel in inglese) il seguente
  sottoinsieme di $V$: 
  \begin{align}
      \ker(f) = \{v \in V : f(v) = 0_W\}.
  \end{align}
\end{definition}

\begin{definition}[Immagine dell'applicazione lineare]
  Siano V e W due spazi vettoriali su $\mathbb{K}$ e sia $ f: V \to W$ una funzione lineare. Si
  definisce \textbf{immagine} di $f$ come il seguente sottoinsieme di W:
  \begin{align}
      Im(f) = \{w \in W : \exists v \in V : f(v) = w\} = \{f(v) : v \in V\}.
  \end{align}
\end{definition}

\begin{observation}
  Siano $V$, $W$ due spazi vettoriali su di un campo $\mathbb{K}$ e sia $f : V \to W$ una funzione
  lineare, allora:
  \begin{align}
    f(0_V) = 0_W.
  \end{align}
\end{observation}
\begin{proof}
  Si ha che:
  \begin{gather*}
      f(0_V) = f(0_K 0_V) = 0_K f(0_V) = 0_W
  \end{gather*}
  dimostrata per le proprietà degli spazi vettoriali e della linearità di $f$.
\end{proof}

\begin{observation}[Il kernel e l'immagine sono sottospazi vettoriali]
  Siano $V$, $W$ due spazi vettoriali su $\mathbb{K}$, e sia $f : V \to W$ una funzione lineare: allora
  $\ker(f)$ è un sottospazio vettoriale di $V$ e $Im(f)$ è sottospazio vettoriale di $W$.
\end{observation}
\begin{proof}
  Dimostriamo prima che $\ker(f)$ è sottospazio di V: $\ker$ non è vuoto quindi questo significa che contiene $0_V$. Allora dati due
  $v_1, v_2$ due elementi di $\ker(f)$ si ha che: $f(v_1) = 0_W = f(v_2)$;
  si vuole dimostrare che $v_1 + v_2 \in \ker(f)$. Dato che $f$ è lineare si può imporre
  \begin{gather*}
      f(v_1 + v_2) = f(v_1) + f(v_2) = 0_W + 0_W = 0_W \ \Longrightarrow \  v_1 + v_2 \in \ker(f)
  \end{gather*}
  Si prova  ora che è chiuso per la moltiplicazione degli scalari: dato $v \in \ker(f)$
  tale che $ f(v) = 0_W$ e $\lambda \in \mathbb{K}$:
  \begin{gather*}
    f(\lambda v) = \lambda f(v) = \lambda 0_W = 0_W \ \Longrightarrow \ \lambda v \in \ker(f).
  \end{gather*}
  Si dimostra ora che $Im(f)$ è un sottospazio vettoriale di $W$. Questo è non vuoto poiché
  contiene $0_W$, allora dati due elementi $w_1, w_2, \exists v_1, v_2 \in V : 
  f(v_1) = w_1, f(v_2) = w_2$ e, con la seguente, si dimostra che è chiuso per la somma
  \begin{gather*}
      w_1 + w_2 = f(v_1) + f(v_2) = f(v_1 + v_2)
  \end{gather*} 
  Questa vale e quindi $w_1 + w_2 \in W$. Inoltre, per la definizione di applicazione lineare, è chiuso per il prodotto per scalari 
  \begin{gather*}
      \lambda w = \lambda f(v) = f(\lambda v)
  \end{gather*} 
\end{proof}

\begin{definition}[Iniettività]
  Siano V e W due spazi vettoriali su $\mathbb{K}$ e sia $f : V \to W$ una funzione lineare. Allora
  \begin{align}
    f \ \text{iniettiva} \Longleftrightarrow \ker(f) = \{0_V\}
  \end{align}
  Dire che $f$ è iniettiva, vuol dire affermare che due elementi di $V$ hanno la stessa immagine tramite
  $f$ se e solo se sono uguali.
\end{definition}
\begin{proof}
  $\Longrightarrow$ Se è iniettiva allora $f^{-1} (0_W)$ contiene sicuramente
  $0_V$, pertanto è solo l'unico elemento in quanto $f$ è lineare: allora $\ker(f) = 0_V$. \\ 
  $\Longleftarrow$ Supponendo invece che $\ker(f) = 0_V$, dato che deve risultare  $f(v_1) = f(v_2) = 0_W$ 
  se e solo se $v_1 = v_2$:
  \begin{gather*}
      f(v_1 - v_2) = 0_W \ \Longrightarrow \   v_1 - v_2 \in \ker(f) \\
      \ \Longrightarrow \ v_1 - v_2 = 0_V \ \Longrightarrow \  v_1 = v_2.
  \end{gather*}
  Allora è iniettiva poiché i vettori, per avere la stessa immagine, sono uguali.
\end{proof}

\begin{theorem}[Relazione fondamentale delle funzioni lineari]
  Siano V e W due spazi vettoriali su di un campo $\mathbb{K}$ con dimensione di V
  finita, Sia $ f : V \to W$ una funzione lineare, allora vale la seguente relazione
  \begin{align}
      \dim \ker(f) + \dim Im(f) = \dim V.
  \end{align}
\end{theorem}

\begin{proof}
  Siano $k = \dim \ker(f)$, $n = \dim V.$ Ovviamente $k \leq n$ in quanto il $\ker$ è un sottospazio
  vettoriale di $V$. Supponendo che $k < n$ e che $k > 0$, si considera $\{v_1,\dots, v_k\}$ una base di $\ker (f)$. Allora,
  se viene completata rispetto ad una base di $V$, si ottiene la seguente:
  \begin{gather*}
      \{v_1, \dots, v_k, \dots, v_n\}
  \end{gather*}
  Se $k = 0$ questo insieme di vettori sarebbe la base come $\{v_1, \dots, v_n\}$, ossia la base di $V$. 
  Dimostriamo che $\dim Im(f) = n - k$, ossia che gli $n - k$ vettori $f(v_{k + 1}), \dots, f(v_n)$ formino una base di $Im(f)$.
  Posto allora $w \in Im(f) \ \exists v \in V : w = f(v)$. Dato che $\left<v_1,\dots, v_n\right>$
  è base di $V$, allora $\exists \lambda_1, ..., \lambda_n \in \mathbb{K}$:
  \begin{gather*}
      v = \sum_{i = 1}^{n} \lambda_i v_i. \\
      w = f(v) = f\left(\sum_{i = 1}^{n} \lambda_i v_i\right) = \sum_{i = 1}^{n} \lambda_i f(v_i) = \sum_{i = k + 1}^{n} \lambda_i f(v_i).
  \end{gather*}
  L'ultimo passaggio vale perché i vettori $w$ appartengono all'immagine e 
  dunque sono combinazione solo dei $k + 1$ fino a $n$ vettori.
  Si dimostra che sono linearmente indipendenti.
  Posti $\lambda_i \in \mathbb{K}, i = \{k + 1, \dots, n\}$,
  \begin{gather*}
      \sum_{i = k + 1}^{n} \lambda_i f(v_i) = 0.
  \end{gather*}
  Per la linearità di $f$ si ottiene le seguenti:
  \begin{gather*}
      f\left(\sum_{i = k + 1}^{n} \lambda_i v_i\right) = 0 \ \Longrightarrow \ \sum_{i = k + 1}^{n} \lambda_i v_i \in \ker(f).
  \end{gather*}
  Se $\exists \mu_1, \dots, \mu_k \in \mathbb{K} \ $, allora deve necessariamente
  risultare che:
  \begin{gather*}
      \sum_{i = k + 1}^{n} \lambda_i v_i = \sum_{j = 1}^{k} \mu_j v_j \ \Longrightarrow \ \sum_{i = k + 1}^{n} \lambda_i v_i -  \sum_{j =  1}^{k} \mu_j v_j  = 0
  \end{gather*}
  La combinazione lineare di questi vettori deve fare zero in quanto 
  sono base di $V$, questo vuol dire che tutti i coefficienti della combinazione lineare sono nulli, ossia $\lambda_i = 0 \  \forall i \in \{k + 1, \dots, n\}$. Dunque
  i due set di vettori sono linearmente indipendenti tra loro. 
\end{proof}

\begin{definition}[Funzione suriettiva]
  Una funzione è \textbf{suriettiva} se, dati $V, W$ due spazi vettoriali di $\mathbb{K}$
  di dimensione finita e, data l'applicazione lineare $f : V \to W$, risulta che
  \begin{align}
      \dim(W) = \dim(Im) 
  \end{align} 
\end{definition}
\begin{proof}
  La dimostrazione deriva dalle proprietà delle relazioni fondamentali
  e dalla definizione di applicazione lineare.
\end{proof}

\begin{lemma}[Suriettività$\ \Longleftrightarrow \  $iniettività]
  Siano V e W due spazi vettoriali su $\mathbb{K}$ di dimensione finita con
  $\dim(V) = \dim(W)$.
  Sia $f : V \to W$ una funzione lineare,  allora $f$ è iniettiva se e solo se è suriettiva.
\end{lemma}
\begin{proof}
  $f$ è iniettiva se e solo se la dimensione di $\ker(f) = 0$, il che equivale a dire
  che $\dim (V) = \dim Im(f)$. Ma, dato che $\dim(V) = \dim(W)$, allora si ha che
  $\dim(W) = \dim Im(f)$ e questo vuol dire che è anche suriettiva.
\end{proof}

\begin{lemma}[Unicità dell'applicazione lineare tra due spazi con base assegnata]
  Siano V e W due spazi vettoriali su $\mathbb{K}$ e sia $\{v_1, \dots, v_n\}$ una base di V
  allora siano $f$ e $g$ due funzioni lineari da V a W. Se $f(v_i) = g(v_i)\  \forall i \in \{1, \dots, n\}$ 
  allora $f = g$.
\end{lemma}
\begin{proof}
  Supponendo che $f(v_i) = g(v_i) \ \forall i \in \{1, \dots, n\}$ si vuole vedere che $f(v) = g(v) \ \forall v \in V$. \\
  Dato che $\{v_1, \dots, v_n\}$ è una base di $V$, allora si ha che $\exists \lambda_1, \dots, \lambda_n
  \in \mathbb{K} : v = \sum_{i = 1}^{n} \lambda_i v_i$. Dato che $f$ è lineare:
  \begin{gather*}
      f(v) = f\left( \sum_{i = 1}^{n} \lambda_i v_i \right) = \sum_{i = 1}^{n} \lambda_i f(v_i) \\
      g(v)  = g\left( \sum_{i = 1}^{n} \lambda_i v_i \right) = \sum_{i = 1}^{n} \lambda_i g(v_i)
  \end{gather*}
  Le due espressioni coincidono poiché, per ipotesi, si è imposto che $f(v_i) = g(v_i)$,
  allora è dimostrato che $f(v) = g(v) \ \forall v \in V$ e dunque vale che $f = g$. 
\end{proof}

\begin{lemma}[Biettività]
  Siano V e W due spazi vettoriali su $\mathbb{K}$. Sia $\{v_1, \dots, v_n\}$ una base di V
  e siano $w_1, \dots, w_n$ degli elementi di W. Allora esistono una ed una sola applicazione
  lineare da $f: V \to W$ tale che $f(v_i) = w_i \ \forall i \in \{1, \dots, n\}$.
  Se $\{w_1, \dots, w_n\}$ è una base di W, allora $f$ prende il nome di \textbf{biettiva}.
\end{lemma}
\begin{proof}
  L'unicità di $f$ segue dalla proposizione precedente e quindi
  occorre solo dimostrare l'esistenza. Definiamo allora $f$ nel modo
  seguente: un qualsiasi $v \in V$ è combinazione lineare dei generatori e, 
  per definizione di applicazione lineare, sarà lo stesso per i vettori $w$.
  \begin{gather*}
      v = \sum_{i = 1}^{n} \lambda_i v_i, \qquad  f(v) = \sum_{i = 1}^{n} \lambda_i w_i 
  \end{gather*}
  Per dimostrare ora che $f$ è lineare si considerano 
  \begin{gather*}
      v = \sum_{i = 1}^{n} \lambda_i v_i, \qquad v' = \sum_{i = 1}^{n} \lambda_i' v_i
  \end{gather*}
  E allora si dimostra:
  \begin{gather*}
      f(v + v') = f\left(\sum \lambda_i v_i + \sum \lambda_i' v_i \right) 
  \end{gather*}
  E quindi dopo alcuni passaggi diventa:
  \begin{gather*}
      \sum \lambda_i w_i + \sum \lambda_i' w_i = f(v) + f(v')
  \end{gather*}
  Dimostriamo ora che $f$ sia omogenea:
  \begin{gather*}
    f(\mu v) = f\left(\mu \sum \lambda_i v_i\right) = \sum (\mu \lambda_i) w_i = \mu f(v)
  \end{gather*}
  Che vale per la definizione di applicazione lineare di $f$. 
  Si dimostra adesso che se $\{w_i, \dots, w_n\}$ è base di $W$ allora 
  $f$ è biettiva. Dimostriamo che è innanzitutto iniettiva e quindi che se
  $f(v) = f(v')$ allora $v = v'$:
  \begin{gather*}
      f\left(\sum \lambda_i v_i\right) = f\left(\sum \lambda_i' v_i\right)
  \end{gather*}
  E quindi, per come si è definita $f$, si ha la seguente espressione:
  \begin{gather*}
      \sum (\lambda_i - \lambda_i') w_i = 0
  \end{gather*}
  Dato che $\lambda_i \neq \lambda_i'$ per tutti $i \in \{1, \dots, n\}$, allora 
  $f$ è iniettiva. Si vuole ora dimostrare che sia suriettiva.
  Sia $w \in W$ allora, dato che $\{w_1, \dots, w_n\}$ è base di $W$,
  esisteranno $\lambda_1, \dots, \lambda_n \in \mathbb{K}$ tali che 
  \begin{gather*}
      w = \sum \lambda_i w_i, \qquad f\left(\sum \lambda_i v_i\right) = \sum \lambda_i w_i = w
  \end{gather*}
  E allora $w \in Im(f)$. Ed è allora dimostrata.  
\end{proof}

\begin{lemma}[Composizioni di applicazioni lineari]
  La composizione di applicazioni lineari è anch'essa lineare.
\end{lemma}
\begin{proof}
  Dati tre spazi vettoriali $U, V, W$ su $\mathbb{K}$ tali che
  \begin{gather*}
      F : V \to W \\
      G : W \to U \\
      G \circ F : V \to U
  \end{gather*}
  dobbiamo dimostrare che $\forall v_1, v_2, \in V, \forall \lambda_1, \lambda_2 \in \mathbb{K}$: 
  \begin{gather*}
    (G \circ F) (\lambda_1 v_1 + \lambda_2 v_2) = \lambda_1 (G \circ F)(v_1) + \lambda_2(G \circ F) (v_2)
  \end{gather*} 
  Preso allora in considerazione:
  \begin{gather*}
      (G \circ F)(\lambda_1 v_1 + \lambda_2 v_2) \ \Longrightarrow \  G(F(\lambda_1 v_1 + \lambda_2 v_2))\\
      G(\lambda_1F(v_1) + \lambda_2 F(v_2)) 
  \end{gather*}
  Dato che l'applicazione $F$ ha come codominio $W$ allora
  anche $G : W \to U$ è un'applicazione perché gli elementi passati
  da $F$ sono $\in W$ e allora posso dire che quella sopra diventa:
  \begin{gather*}
      \lambda_1 G(F(v_1)) + \lambda_2 G(F(v_2))
  \end{gather*}
\end{proof}

\section{Isomorfismi}
\begin{theorem}[L'applicazione inversa di una certa funzione lineare è anch'essa lineare]
  Siano V e W due spazi vettoriali su di un campo $\mathbb{K}$ e sia $f: V \to W$ una
  applicazione lineare biettiva. Allora l'applicazione $f^{-1} : W \to V$ è lineare.
\end{theorem}
\begin{proof}
  Si dimostra prima di tutto l'additività di $f^{-1}$. Siano allora
  $w_1, w_2 \in W$, si vuole dimostrare che:
  \begin{gather*}
      f^{-1} (w_1 + w_2) = f^{-1} (w_1) + f^{-1} (w_2)
  \end{gather*}
  Dato che è iniettiva si ha che:
  \begin{gather*}
      f(f^{-1} (w_1 + w_2))  =w_1 + w_2
  \end{gather*}
  Data la linearità di $f$:
  \begin{gather*}
      f(f^{-1} (w_1) + f^{-1} (w_2)) = f(f^{-1} (w_1)) + f(f^{-1} (w_2)) = w_1 + w_2
  \end{gather*}
  Si dimostra l'omogeneità di $f^{-1}$, siano $w \in W$ e $\lambda \in \mathbb{K}$; si vuole 
  dimostrare che, dato che $f$ è iniettiva:
  \begin{gather*}
      f(f^{-1} (\lambda w)) = f(\lambda f^{-1}(w))
  \end{gather*}
  Dato che $f$ è lineare si ha l'omogeneità tra i due e allora si è dimostrato
  che $f^{-1}$ è lineare.
\end{proof}

\begin{definition}[Isomorfismo]
  Siano V e W due spazi vettoriali su di un campo $\mathbb{K}$. Si dice \textbf{isomorfismo}
  da V a W un'applicazione lineare biettiva $f : V \to W$. \\
  Un isomorfismo tra due spazi vettoriali vuol dire che esiste una trasformazione
  lineare tra di loro unica e biettiva; inoltre la loro struttura algebrica è la
  stessa. In pratica due spazi isomorfi descrivono, analiticamente, lo stesso spazio vettoriale.
\end{definition}

\begin{definition}[Spazi isomorfi]
  Due spazi vettoriali
  V e W su uno stesso campo $\mathbb{K}$ si dicono \textbf{isomorfi} se esiste
  \begin{gather*}
      f: V \to W \\
      f^{-1}  : W \to V
  \end{gather*}
\end{definition}

\begin{definition}[Endomorfismo]
  Si chiama \textbf{endomorfismo} un'applicazione lineare che ha il dominio
  coincidente con il codominio:
  \begin{gather*}
      f: V \to V
  \end{gather*}
\end{definition}

\begin{definition}[Automorfismo]
  Si chiama \textbf{automorfismo} un'endomorfismo invertibile 
  \begin{gather*}
      f : V \to V \\
      f^{-1}  : V \to V
  \end{gather*}
\end{definition}

\begin{example}[Isomorfismo tra spazio dei polinomi e spazio vettoriale]
  Gli spazi vettoriali $K_n[x]$ e $\mathbb{K}^{n + 1}$ sono isomorfi qualsiasi sia
  $n \in \mathbb{N}$ e $\mathbb{K}$ campo. Un isomorfismo tra essi è proprio:
  \begin{gather*}
      a_o + \dots + a_nx^{n} \to \begin{pmatrix} a_0 \\
      \vdots\\ a_n \end{pmatrix}
  \end{gather*} 
\end{example}


\begin{example}[isomorfismo tra spazio delle matrici e spazio di vettori]
  Gli spazi vettoriali $M (n \times m, \mathbb{R})$ e $K^{mn}$ sono isomorfi.
  In particolare, è possibile \emph{\textbf{srotolare}} la matrice in modo tale
  da ottenere l'isomorfismo
  \begin{gather*}
      A = \begin{pmatrix}
          a & b \\
          c & d
      \end{pmatrix} \ \cong \ \begin{pmatrix} a\\
      b\\
      c\\
      d \end{pmatrix}  
  \end{gather*}
  questo isomorfismo descrive lo stesso spazio vettoriale $\mathbb{R}^{4}$. 
\end{example}


\begin{theorem}[Condizioni di isomorfismo]
  Sia $\mathbb{K}$ un campo. Sia $V$ uno spazio vettoriale su $\mathbb{K}$ e di dimensione
  $n$ e sia $W$ uno spazio vettoriale su $\mathbb{K}$ di dimensione $m$. Gli spazi $V$ e $W$
  allora sono isomorfi $\Longleftrightarrow$ $n = m$.
\end{theorem}
\begin{proof}
  $\Longrightarrow \ $Per la relazione fondamentale delle applicazioni lineari si ha la tesi: 
  infatti posta $f : V \to W$ un isomorfismo, poiché $f$ è iniettiva, la
  $\dim \ker(f) = 0$ e dato che $f$ è anche suriettiva: $Im(f) = W$. Per la
  relazione fondamentale si ha che $\dim(V) = \dim(W)$. \\
  $\Longleftarrow \ $Invece se $\{v_1, \dots, v_n\}$ una base di $V$ e $\{w_1, \dots, w_n\}$ una base
  di $W$, esiste unica $f : V \to W$, per il teorema di unicità di una applicazione 
  lineare, tale che $f(v_i) = w_i \
  \forall i \in  \{1, \dots, n\}$, allora si ha che $f$ è biettiva e dunque esiste
  una trasformazione unica e biettiva.
\end{proof}

\section{Relazioni tra matrici e funzioni lineari}
\begin{definition}[Matrice associata ad una funzione lineare]
  Una matrice $A \in M(m \times n, \mathbb{K})$, si dice \textbf{matrice associata ad una funzione lineare}
  se 
\begin{align}
    f_A : \mathbb{K}^{n}  \to \mathbb{K}^{m} \qquad f_A(x) =  Ax   \qquad x \in \mathbb{K}^{n}
\end{align}
\end{definition}

\begin{lemma}[La matrice di una applicazione lineare è unica]
  Siano $\mathbb{K}$ un campo $m, n \in \mathbb{N} - \{0\}$ e sia
  $f : \mathbb{K}^{n} \to \mathbb{K}^{m} $ lineare. Allora esiste un'unica matrice
  \begin{gather*}
      A \in M(m \times n, \mathbb{K}) : f = f_A 
  \end{gather*}  
\end{lemma}
\begin{proof}
  Sia $A$ la matrice associata che ha come colonne $f(e_1), \dots, f(e_n)$.
  Si vuole dimostrare che $f = f_A$. Basterà allora dimostrare che
  $f(e_j) = f_A(e_j), j \in  \{1, \dots, n\}$ e questo è ovvio in quanto $f_A(e_j)$ è
  $Ae_j$ ossia la $j$-esima colonna di $A$ che è proprio $f(e_j)$. 
\end{proof}


\begin{example}
Si considera una funzione lineare ed una matrice $A \in M(2 \times  3)$:
\begin{gather*}
    f_A : \mathbb{K}^3 \to \mathbb{K}^2 \qquad A = \left(\begin{tabular}{c c c}
        $a$ & $b$ & $c$ \\
        $d$ & $e$ & $f$ 
    \end{tabular}\right) 
\end{gather*}
Con le seguenti condizioni:
\begin{gather*}
    A\begin{pmatrix} x_1 \\
    x_2 \\
    x_3 \end{pmatrix} \to 
    \left( \begin{tabular}{c c c}
        $ax_1$ & $bx_2$ & $cx_3$ \\
        $dx_1$ & $ex_2$ & $fx_3$ 
    \end{tabular} \right)
\end{gather*}
 Questa è la matrice $A$ associata alla funzione lineare $f$.
\end{example}

\begin{definition}[Spazio vettoriale delle soluzioni di un sistema lineare]
In generale le soluzioni di un sistema lineare $Ax = b$ sono:
\begin{gather*}
    f_A^{-1} b = \left\{ x : f_A(x) = b \right\}
\end{gather*}
Mentre per un sistema omogeneo qualunque:
\begin{gather*}
        \ker f_A = \left\{ x : Ax = 0 \right\}
\end{gather*}
Questo ci permette di esprimere i sistemi lineari come delle funzioni lineari.
Pensare agli spazi vettoriali come spazi isomorfici di funzioni è un modo moderno di pensare
alla rappresentazione degli spazi vettoriali stessi. 
\end{definition}

\begin{example}[Esempio di trasformazione di un vettore]
  Data la seguente applicazione lineare
  \begin{gather*}
      g: \mathbb{R}^3  \to \mathbb{R} ^2 : 
      \begin{pmatrix} x_1 \\
      x_2 \\
      x_3 \end{pmatrix} \to \left( \begin{tabular}{c c c}
          $5x_1$ & $12x_2$ & $2024x_3$ \\
          $25x_1$ & $12x_2$ & $7x_3$ 
      \end{tabular} \right) 
  \end{gather*}
  Calcolare $g(e_1)$.
  La matrice associata all'applicazione lineare $g$ è proprio:
  \begin{gather*}
      A = \left( \begin{tabular}{c c c}
          5 & 12 & 2024 \\
          25 & 12 & 7
      \end{tabular} \right)
  \end{gather*}
  Allora $g(e_1)$:
  \begin{gather*}
      g(e_1) = \left( \begin{tabular}{c c c}
          5 & 12 & 2024 \\
          25 & 12 & 7
      \end{tabular} \right)\begin{pmatrix} 1 \\
      0 \\
      0 \end{pmatrix} = \begin{pmatrix} 5 \\
      25 \end{pmatrix} 
  \end{gather*}
\end{example}

\begin{observation}[L'immagine di una applicazione lineare è generata dalle colonne]
  Siano $\mathbb{K}$ un campo e $m, n \in \mathbb{N} - \{0\}$. Sia $A \in M(m \times n, \mathbb{K})$ e sia
  $f : V \to W$ una applicazione lineare con matrice associata $A$. Allora
  l'immagine $f_A$ è generata dalle colonne di $A$. 
\end{observation}
\begin{proof}
  \begin{gather*}
      Im(f_A) = \{f_A(x) : x \in \mathbb{K}^{n} \} = \{A x : x \in \mathbb{K}^{n} \} = \\
      \{x_1 A^{(1)} + \dots + x_n A^{(n)} : x_1, \dots, x_n \in \mathbb{K}\} = \left<A^{(1)}, \dots, A^{(n)}\right> 
  \end{gather*}
  Allora i vettori ottenuti dalle colonne di $A$ generano lo spazio dell'immagine di una 
  applicazione lineare.
\end{proof}

\begin{lemma}[Proprietà delle matrici associate alle applicazioni lineari]
  Siano $\mathbb{K}$ un campo e $m, n, r \in \mathbb{N} - \{0\}$.
  \begin{enumerate}
      \item $\forall A, A' \in M(m \times n, \mathbb{K})$:
      \begin{gather*}
         f_A = f_{A'} \Longleftrightarrow A = A'  
      \end{gather*}
      \item $\forall A \in M(m \times r, \mathbb{K}), B \in M(r \times n, \mathbb{K})$:
      \begin{gather*}
          f_{AB} = f_A \circ f_B = AB
      \end{gather*}
      \item $\forall A \in M(n \times n, \mathbb{K})$ :
      \begin{gather*}
          f_A = I_{\mathbb{K}^{n} } \Longleftrightarrow A = I_n
      \end{gather*}
  \end{enumerate}
\end{lemma}
\begin{proof}
  La dimostrazione è molto semplice:
  \begin{enumerate}
      \item Discende direttamente dalla proprietà sull'unicità della matrice
      associata ad una applicazione lineare;
      \item Per la proprietà di composizione di applicazioni lineari si sa che
      \begin{gather*}
          f_A(f_B(v)) = (f_A \circ f_B)(v)
      \end{gather*}
      Allora, data la matrice associata per ogni applicazione lineare:
      \begin{gather*}
          f_A(f_B(v)) = A(B(v)) \ \Longrightarrow \ ABv
      \end{gather*}
      E dunque è dimostrata.
      \item Discende direttamente dalla proprietà di matrice associata ad una applicazione lineare.
  \end{enumerate}
\end{proof}

\section{Matrici associate a delle basi}
\begin{definition}[Matrice associata a delle basi]
  Dati $V, W$ due spazi vettoriali su di un campo $\mathbb{K}$, si chiama $P = \left< v_1, \dots, v_n \right>$ la base di $V$ e si chiama 
  $A = \left< w_1, \dots, w_n \right>$ la base di arrivo di $W$. Sia quindi $f : V \to W$
  tale applicazione lineare, posti $\lambda_{i, j} \in \mathbb{K} \ \forall i \in {1, \dots, m}
  \wedge  j \in \{1, \dots, n\}$ tali che:
  \begin{align}
      f(p_j) = \sum_{i = 1}^{m} \lambda_{i, j}a_i.
  \end{align}  
  Definisco una \textbf{matrice $M(m \times n)$ associata ad f nelle basi $P$ ed $A$}
  denotata come:
  \begin{align}
      M_{A, P}(f) : (M_{A, P}(f))_{i, j} = \lambda_{i, j}, \ \forall i \in \{1, \dots, m\} \wedge 
      j \in \{1, \dots, n\}. 
  \end{align} 
\end{definition}

\begin{example}[Matrice associata a delle basi di arrivo e partenza]
  Siano le due basi $P = \{v_1, v_2, v_3\}, A = \{w_1, w_2\}$
  per cui ottengo, in funzione di ogni vettore, le seguenti:
  \begin{gather*}
      f(v_1) = a w_1 + d w_2 \\
      f(v_2) = b w_1 + e w_2 \\
      f(v_3) = c w_1 + f w_2 
  \end{gather*}
  Per ogni elemento di $v$ associo un elemento di $A$, il quale per
  ogni $v$ deve mantenere la colonna (per questo nella prima espressione
  si ha "$a$" e "$d$").  Per cui la matrice finale è:
  \begin{gather*}
      M_{A, P}(f) = \left( \begin{tabular}{c | c | c}
          $a$ & $b$ & $c$ \\
          $d$ & $e$ & $f$
      \end{tabular} \right) \Rightarrow  \\
      \left( f(v_1) \ | \ f(v_2) \ | \ f(v_3)\right) 
  \end{gather*}
  Per cui una funzione lineare del tipo:
  \begin{align}
      f: V^{n} \to W^{m} \Rightarrow \ matrice \ m \times n 
  \end{align}
  I valori di $m$ ed $n$ si scambiano.
\end{example}

\begin{observation}[Isomorfismo tra applicazione lineare e coordinate rispetto ad una base]
  Sia V uno spazio vettoriale su un campo $\mathbb{K}$ di dimensione finita $n$. Fissata allora
  una base ordinata $B$ di V si definisce l'isomorfismo: $\phi_B : V \to \mathbb{K}^{n}$:
  $\forall v \in V, \phi_B(v)$ è l'$n$-upla delle coordinate di v rispetto alla base
  B. Posso, in pratica, ricavare l'immagine di un vettore tramite una applicazione lineare
  conoscendo la matrice associata all'applicazione lineare in certe basi. Si ottiene da questa
  applicazione le coordinate per le componenti del vettore $v$ rispetto alla base $B$.
\end{observation}

\begin{lemma}[Mappa cambio base di una applicazione lineare]
  Siano $V$ e $W$ due spazi vettoriali su di un campo $\mathbb{K}$ di dimensione finita $n$ ed
  $m$ rispettivamente. Sia $P$ una base di $V$ e $A$ una base di $W$. Sia allora $v \in V$
  e $x$ la $n$-upla delle coordinate di $v$ rispetto a $P$. Allora, detta $y$ la $m$-upla delle 
  coordinate di $f(v)$ rispetto alla base $A$ si ha che:
  \begin{align}
      y = M_{A, P}(f)x.
  \end{align}
\end{lemma}
\begin{proof}
  Posta $P = \{p_1, \dots, p_n\}$ e $A = \{a_1, \dots, a_m\}$ allora si ha:
  \begin{gather*}
      f(v) = f\left( \sum_{j = 1}^{n} x_jp_j  \right) = \sum_{j = 1}^{n} x_j f(p_j) = \\
      \sum_{j = 1}^{n} x_j\left(\sum_{i = 1}^{m} M_{A, P}(f)_{i, j}a_i\right) = \sum_{i = 1}^{m} \left( \sum_{j = 1}^{n} M_{A, P}(f)_{i, j}x_j\right)a_i = \\
      \sum_{i = 1}^{m} (M_{A, P}(f)x)_{i, 1}a_i.
  \end{gather*}
  Infine si ha la tesi $\forall i \in \{1, \dots, m\}$ poiché quest'ultima esprime ogni vettore
  $\in W$ come combinazione lineare delle colonne della matrice $M_{A, P}(f)$.  
\end{proof}

\begin{corollary}[Basi associate allo stesso spazio vettoriale]
  Sia V uno spazio vettoriale di dimensione $n$ e siano $B$ e $B'$ due basi di $V$;
  ora se $v \in V$ e $x$ è l'$n$-upla delle coordinate di $v$ rispetto a $B$ e $x'$ è
  l'$n$-upla delle coordinate di $v$ rispetto a $B'$, allora si ha che:
  \begin{align}
      x' = M_{B', B}(I_V)x
  \end{align}
  Deriva dalla proposizione precedente (no dim).
\end{corollary}

\begin{theorem}[Teorema della composizione]
  Siano V,W,U tre spazi vettoriali di dimensione finita su di un campo $\mathbb{K}$ e siano
  P, B, A basi ordinate rispettive agli spazi considerati. Allora siano
  $f : V \to W$ e $g: W \to U$:
  \begin{align}
      M_{A, P} (g \circ f) = M_{A, B}(g) M_{B, P}(f).
  \end{align}
\end{theorem}

\begin{theorem}[Teorema del cambio di base]
    Siano $V$ e $W$ due spazi vettoriali di dimensione finita su $\mathbb{K}$. Siano $P, P'$
  due basi ordinate di $V$ e $A$ e $A'$ due basi ordinate di $W$. Sia $f : V \to W$ un'applicazione lineare allora:
  \begin{align}
      M_{A', P'}(f) = M_{A', A}(I_W) M_{A, P}(f) M_{P, P'}(I_V).
  \end{align}
  In altri termini la matrice associata alle nuove basi $A', P'$ si calcola
  (da sinistra) come il prodotto tra la matrice associata alle basi $A, P$ per quella
  che ha come colonne i vettori della base $P'$ ($ M_{P, P'}(I_V)$) e poi si torna indietro moltiplicando per 
  l'inversa della matrice che ha come colonne i vettori della base $A$ (ossia la matrice
  $M_{A', A}(I_W)$).
\end{theorem}
Si osserva come il cambio di base "all'indietro" è dato dalle matrici
inverse. \\
Per il cambio di base essenzialmente si trasforma prima un vettore nello spazio
nel quale intendiamo fare la trasformazione e poi si applica la matrice trasformazione
nello spazio considerato e dopo si moltiplica nuovamente (sempre da sinistra a destra) con la matrice
inversa che esprime il cambio di base dallo spazio prima a quello che vogliamo noi, ottenendo così 
la matrice trasformazione che ci permette di calcolare direttamente la trasformazione di un vettore
in uno spazio con le basi associate. 

\begin{example}[Applicazione della formula del cambio base]
  Sia $f : \mathbb{R}^{3} \to \mathbb{R}^{4}$ l'applicazione così definita:  
  \begin{gather*}
      f\begin{pmatrix} x \\
      y\\
      z \end{pmatrix} = \begin{pmatrix} -x  + 2z \\
      3z - x \\
      y + x \\
      2y \end{pmatrix}  
  \end{gather*}
  Ponendo come base della $\epsilon = \{e_1, e_2, e_3\}$ e $\epsilon' = \{E_1, E_2, E_3, E_4\}$ allora
  la matrice trasformazione di $f$ associata alle due basi canoniche è proprio 
  (ricordando che $\epsilon$ è la base di partenza e $\epsilon'$ è quella di arrivo):
  \begin{gather*}
      M_{\epsilon', \epsilon} (f) = \begin{pmatrix}
          -1 & 0 & 2 \\
          -1 & 0 & 3 \\
          1 & 1 & 0 \\
          0 & 2 & 0
      \end{pmatrix}
  \end{gather*}
  Siano ora le basi di arrivo e partenza
  \begin{gather*}
      A = \left\{\begin{pmatrix} 0 \\
      -1 \\
      1 \\
      0 \end{pmatrix}, \begin{pmatrix} 0 \\
      1\\
      0\\
      0 \end{pmatrix}, \begin{pmatrix} 0 \\
      0\\
      0\\
      1 \end{pmatrix}, \begin{pmatrix} 2 \\
      0 \\
      1\\
      0 \end{pmatrix}    \right\} , \qquad P = \left\{\begin{pmatrix} 1 \\
      -1\\
      0 \end{pmatrix}, \begin{pmatrix} 0 \\
      0\\
      1 \end{pmatrix}, \begin{pmatrix} 2 \\
      1\\
      0 \end{pmatrix}   \right\}
  \end{gather*}
  Ci sono due modi per calcolare la matrice associata nelle basi appena definite:
  attraverso la formula di cambio base oppure analiticamente. Utilizzando 
  il procedimento analitico si calcola un vettore attraverso la base di partenza e lo si esprime
  come combinazione lineare dei vettori della base di arrivo. Quindi si arriva a trovare la matrice
  di cambio di base. Utilizzando la formula di cambio base invece, dato che l'applicazione
  trasforma da $\mathbb{R}^{3}$ a $\mathbb{R}^{4}$, allora si ha che la formula:
  \begin{gather*}
      M_{A, P}(f) = M_{A, \epsilon'}(I_{\mathbb{R}^{4}} ) M_{\epsilon', \epsilon}(f) M_{\epsilon, P}(I_{\mathbb{R}^{3}} )
  \end{gather*} 
  Si trova allora che 
  \begin{gather*}
      M_{\epsilon, P}(I_{\mathbb{R}^{3} }) = \begin{pmatrix}
          1 & 0 & 2 \\
          -1 & 0 & 1 \\
          0 & 1 & 0
      \end{pmatrix}, \qquad \qquad M_{A, \epsilon'} = M_{A, A}^{-1}  =  \begin{pmatrix}
          -\frac{1}{2} & 0 & 1 &  0 \\
          - \frac{1}{2} & 1 & 1 & 0 \\
          0 & 0 & 0 & 1 \\
          \frac{1}{2} & 0 & 0 & 0 
      \end{pmatrix}
  \end{gather*}
  Si ottiene moltiplicando o seguendo la risoluzione analitica che la matrice che si cerca è
  \begin{gather*}
      \begin{pmatrix}
          \frac{1}{2} & -1 & 4 \\
          - \frac{1}{2} & 2 & 2 \\
          -2 & 0 & 2 \\
          -\frac{1}{2} & 1 & -1
      \end{pmatrix}
  \end{gather*}
\end{example}

\begin{corollary}
  Sia V uno spazio vettoriale su di un campo $\mathbb{K}$ e prese due base B e B'. Allora
  \begin{align}
      M_{B, B'}(I_V) = M_{B', B}(I_V)^{-1} 
  \end{align}
\end{corollary}
\begin{proof}
  Infatti, per il teorema precedente si ha che
  \begin{gather*}
      M_{B, B'}(I_V) M_{B', B}(I_V) = M_{B, B}(I_V) = I_n
  \end{gather*}
  E dunque è verificata.
\end{proof}

\begin{observation}
  Siano $\mathbb{K}$ un campo e $m, n \in \mathbb{N} - \{0\}$. Sia $A \in M(m \times n, \mathbb{K})$,
  allora la matrice associata ad $f_A$ nelle basi canoniche è proprio A.
\end{observation}

\begin{example}[Applicazioni lineari di matrici]
  Sia 
  \begin{gather*}
      B = \begin{pmatrix}
          3 & 1 \\
          0 & -2
      \end{pmatrix} \qquad \qquad F: M(2 \times 2, \mathbb{R}) \to M(2 \times 2, \mathbb{R}) \ : \ F(A) = AB - BA
  \end{gather*}
  Si provi per quali $m \in \mathbb{Z}$ le potenze $B^m \in \ker(f)$ dove $m < 0$ si intende $B^m = (B^{-1} )^{(-m)}$.
  Questo si ha calcolando in $F$ la matrice $B^m$.
  \begin{gather*}
      F(B^{m}) = B^{m}B - BB^{m} = 0, \forall m : B^{m} \in \ker(F), \forall m \in \mathbb{Z}   
  \end{gather*}
  Si può determinare ora la matrice $F$ secondo la seguente considerazione:
  \begin{gather*}
      F\begin{pmatrix}
          a & b \\
          c & d
      \end{pmatrix} = \begin{pmatrix}
          3a & a - 3b \\
          3c & c - 2d
      \end{pmatrix} - \begin{pmatrix}
          3a + c & 3b + d \\
          -2c & -2d
      \end{pmatrix} = 0
  \end{gather*}
  Utilizzando l'isomorfismo tra spazio matriciale $m \times n$ e spazio
  vettoriale $\mathbb{K}^{mn}$, si può interpretare
  \begin{gather*}
      F(A) \ \text{come} \  F\begin{pmatrix} a \\
      b\\
      c\\
      d \end{pmatrix} 
  \end{gather*}
  Ottenendo allora $F$ come
  \begin{gather*}
      F = \begin{pmatrix}
          0 & 0 & -1 & 0 \\
          1 & -5 & 0 & -1 \\
          0 & 0 & 5 & 0 \\
          0 & 0 & 1 & 0
      \end{pmatrix}
  \end{gather*}
\end{example}
\clearpage

\section{Spazio delle applicazioni lineari tra due spazi vettoriali, spazio duale}
\begin{definition}[Insieme delle applicazioni lineari]
Siano $V$ e $W$ due spazi vettoriali su di uno stesso campo $\mathbb{K}$. Allora sia $\hom_\mathbb{K} (V, W)$,
l'insieme delle applicazioni lineari da $V$ a $W$, indicabile anche con $\hom(V, W)$ se il campo è chiaro dal contesto.
L'insieme $\hom_\mathbb{K}(V, W)$ si può dotare di un'operazione
di somma e di prodotto per scalare:
\begin{enumerate}
    \item $\forall f, g \in \hom(V, W), \quad f + g : (f + g )(x) = f(x) +g(x) $.
    \item $\forall f \in \hom(V, W), \quad \lambda \in \mathbb{K}, \lambda f : (\lambda f)(x) = \lambda f(x) $.
\end{enumerate}
Allora $\hom(V, W)$ è proprio uno spazio vettoriale su $\mathbb{K}$. Allora se $\dim V = n$ e $\dim W = m$ si ha
che se fissiamo P una base di V e A una base di W si ottiene l'applicazione:
\begin{align}
    f \to M_{A, P}(f),\text{ è isomorfismo tra} \ \hom(V, W) \ \text{e} \ M(m \times n, \mathbb{K}).
\end{align}
Questo vuol dire che 
\begin{align}
    \dim\hom(V, W) = \dim V \times \dim W
\end{align}
\end{definition}
\begin{proof}
  Date $f, g$ due funzioni lineari, $f + g$ è ancora lineare poiché
  \begin{gather*}
      (f + g)(\lambda v_1 + \mu v_2) = f(\lambda v_1 + \mu v_2) + g(\lambda v_1 + \mu v_2) 
  \end{gather*}
  Per la linearità di $f$ e per la definizione di $f + g$:
  \begin{gather*}
      \lambda f(v_1) + \mu f(v_2) + \lambda g(v_1) + \mu g(v_2) = \lambda (f(v_1) + g(v_1)) + \mu (f(v_2) + g(v_2)) \\
      \lambda (f + g)(v_1) + \mu (f + g)(v_2)
  \end{gather*}
  Inoltre si dimostra che commutano:
  \begin{gather*}
      (f + g)v = f(v) + g(v) = g(v) + f(v) = (g + f)v \ \Longrightarrow \ f + g = g + f
  \end{gather*}
  Si dimostra che $f \to M_{A, P}(f)$ è lineare ed è un isomorfismo con $\hom$. Dato $\lambda, \mu \in \mathbb{K}$ e $f, g \in \hom(V, W)$,
  \begin{gather*}
      M_{A, P}(\lambda f + \mu g) = \lambda M_{A, P}(f) + \mu M_{A, P}(g) 
  \end{gather*}
  E dunque è isomorfismo
\end{proof}

\begin{observation}
  Lo zero in $\hom(V, W)$ è una funzione lineare nulla. Ossia
  \begin{gather*}
    0(v) = 0_W \quad \forall v \in V
  \end{gather*}
\end{observation}
\begin{definition}[Spazio duale]
  Sia V uno spazio vettoriale su di un campo $ \mathbb{K}$. Allora si definisce:
  \begin{align}
      V^{\vee} = \{f : V \to \mathbb{K} : f \ \text{lineare}\} = \hom(V, \mathbb{K}) \ \Longrightarrow \ \dim(V^{\vee}) = \dim(V) \times \dim(\mathbb{K})
  \end{align}
  Su questo insieme definiamo somma e prodotto tra scalari nel modo seguente:  se $f$ e $g$ sono
  due elementi di $V^{\vee}$ si hanno le seguenti proprietà:
  \begin{enumerate}
      \item $(f + g)v = f(v) + g(v) \quad \forall v \in V $ 
      \item $(\lambda f) (v) = \lambda f(v) \quad \forall v \in V$. 
  \end{enumerate}
  In questo modo si ottiene che $V^{\vee}$ è uno spazio vettoriale chiamato \textbf{spazio vettoriale duale} di V.
  Mentre $V^{\vee\vee}$ è chiamato \textbf{spazio vettoriale biduale}, ossia il duale del duale, che gode della proprietà
  di essere naturalmente isomorfico a $V$.
\end{definition}

\begin{theorem}[Valutazione]
  Sia I un insieme e sia $B = \{v_i\}_{i \in I}$ una base di V; allora $\forall i \in I$ si definisce
  $v_i^{\vee}$ come l'unica applicazione lineare da V in $\mathbb{K}$ tale che:
  \begin{align}
      v_i^{\vee} : V^{\vee} \to \mathbb{K} \ \Longrightarrow \ v_i^{\vee} (v_j) = \delta_{i, j} = \left\{\begin{array}{l}
        1 \quad i = j\\
        0 \quad i \neq j 
      \end{array}\right.
  \end{align} 
  Dato che $v_i^{\vee} \in V^{\vee}$ prendono il nome di \textbf{covettori} e,
  dato che il prodotto scalare con i vettori $v_i \in V$ deve essere uno scalare,
  sono dei vettori di riga.
  \begin{align}
      B^{\vee} = \{v_i^{\vee} \}_{i \in I} 
  \end{align}
  Dove $B^{\vee}$ è l'insieme di covettori linearmente indipendenti in $V^{\vee}$. Se V ha dimensione finita
  allora $B^{\vee}$ è una base di $V^{\vee}$ e quindi $V^{\vee}$ ha $\dim V^{\vee} = \dim  V$ proprio
  perché sussiste l'isomorfismo visto prima. 
  Si definisce allora la valutazione di un vettore $v$ come la seguente trasformazione lineare
  \begin{align}
      val_v : V^{\vee} \to \mathbb{K} \quad \forall v \in V \ \Longrightarrow \   val_v (f) = f(v) \ \forall f \in V^{\vee} 
  \end{align}    
  Come la valutazione di $f$ in $v$. L'applicazione di $val_v$ è lineare e
  $val_v \in V^{\vee\vee}$; di conseguenza la seguente applicazione lineare,
  che manda un vettore $v \in V$ alla valutazione $val_v$, è definita come
  \begin{align}
      val: V \to V^{\vee\vee} \ \Longrightarrow \  v \to val_v 
  \end{align} 
  che permette di ottenere la valutazione del vettore $v$ come scalare per $f \in V^{\vee}$; inoltre questa funzione 
  è lineare ed iniettiva e se $V$ ha dimensione finita ha quindi un'isomorfismo con $V^{VV }$.  
\end{theorem}
\begin{proof}
  Se $\{v_1, \dots, v_n\}$ è un insieme di vettori linearmente indipendenti e formano una base di $V$, 
  si vuole dimostrare che $\{v_1^{\vee}, \dots, v_n^{\vee}\}$ sia base di $V^\vee$. Innanzitutto 
  si considerano $\lambda_i \in \mathbb{K}$, dunque si valutano le combinazioni lineari
  dei covettori per $v_1, \dots, v_n$:
  \begin{gather*}
      \lambda_{1}v_{1}^{\vee}(v_{1}) + \dots + \lambda_{n}v_{n}^{\vee}(v_{1}) = 0 \quad \dots \quad \lambda_nv_2^{\vee}(v_n) + \dots + \lambda_nv_n^{\vee}(v_n) = 0
  \end{gather*}
  Dunque, per ognuna
  \begin{gather*}
      \lambda_1v_1^{\vee}(v_1) = 0 \quad\dots \quad \lambda_nv_n^{\vee}(v_n) = 0
  \end{gather*}
  Secondo la definizione di $v_i^\vee$ si ottiene che $\lambda_i = 0$, dunque  $B^{\vee}$ è un insieme di vettori linearmente indipendenti che genera $V^{\vee}$. \\
  Se $V$ ha dimensione finita, allora $B^{\vee}$ è anche un insieme di generatori 
  di $V^{\vee}$, infatti sia $f \in V^{\vee}$, allora si può anche dimostrare che
  \begin{gather*}
      f = \sum_{i \in I} f(v_i)v_i^{\vee} 
  \end{gather*}
  Infatti $f(v_i)v_i^{\vee}$ è un vettore riga, dunque la somma dei singoli
  vettori riga per ogni $i \in I$ mi costruisce, una ad una, le righe 
  della funzione lineare, infatti:
  \begin{gather*}
      f(v_j) = \sum_{i  = 1}^{n} f(v_i)v_i^{\vee}(v_j) = \sum_{i  = 1}^{n} f(v_i)\delta_{i, j}  
  \end{gather*}
  Dimostriamo ora che $val$ è iniettiva: sia $v \in V$ tale che $val_v = 0$. 
  Allora dato che $B = \{v_i\}_{i \in I}$ è una base di $V$ allora
  esistono $1, \dots, k \in I$, $\lambda_{1}, \dots, \lambda_{k} \in \mathbb{K}$ 
  tali che 
  \begin{gather*}
      v = \lambda_{1} v_{1} + \dots + \lambda_{k} v_{k}
  \end{gather*}
  Dato allora che $val_v = 0$, si deve avere che $val_v(v_{i}^{\vee}) = 0$, ossia, che per $s = 1, \dots, k$, si abbia 
  \begin{gather*}
      v_{s}^{\vee}(\lambda_{1}v_{1} + \dots + \lambda_{k}v_{k}) = 0
  \end{gather*}
  Da cui si ottiene che per avere $\lambda_{i} v_{i}^{\vee}(v_{i}) = 0$ quando $i = s$, si deve avere $\lambda = 0$. Allora $f(v) = 0$, da cui $v = 0$. 
\end{proof}

\begin{observation}[I covettori sono vettori riga]
  I covettori di uno spazio duale sono dei vettori di riga poiché il loro prodotto con
  i vettori dello spazio non duale deve essere necessariamente un numero.
\end{observation}

\begin{definition}[Funizione dello spazio duale]
  Siano V e W due spazi vettoriali su di uno stesso campo $\mathbb{K}$. Sia allora $f : V \to W$
  un'applicazione lineare. Definiamo allora:
  \begin{align}
      f^{\vee} : W^{\vee} \to V^{\vee}   
  \end{align}
  come la funzione lineare trasposta che vale
  \begin{align}
      f^{\vee} (\alpha) = \alpha \circ f  
  \end{align}
  Dove $\alpha$ è una funzione lineare definita da $W \to \mathbb{K}$. Anche la funzione $f^\vee$ è lineare.
\end{definition}


\begin{lemma}[$f^\vee$]
  Siano $V$ e $W$ due spazi vettoriali su di uno stesso campo $\mathbb{K}$ di dimensione finita e sia 
  $f : V \to W$ un'applicazione lineare. Sia $P = \left<p_1, \dots, p_n\right>$ una base
  di $V$ e $A = \left<a_1, \dots, a_m\right>$ una base di W. Allora si ha:
  \begin{align}
      M_{P^{\vee}, A^{\vee} }(f^{\vee} ) = ^{t}M_{A, P}(f) 
  \end{align}
  Ossia la matrice associata alla funzione duale è la trasposta di quella associata alla funzione $f$ non duale. 
\end{lemma}

\begin{proof}
  La dimostrazione che è la trasposta non si fa perché così il Dio Ottaviani ha decretato. Si 
  dimostra che è lineare, poste $\alpha_1 + \alpha_2 \in W$:
  \begin{gather*}
      f^{\vee} (\alpha_1  + \alpha_2) \circ f = \alpha_1 \circ f + \alpha_2 \circ f = f^{\vee} (\alpha_1) + f^{\vee}(\alpha_2)
  \end{gather*}
  È anche omogenea:
  \begin{gather*}
      f^{\vee}(\lambda \alpha) = \lambda (\alpha \circ f) = \lambda f^{\vee} (\alpha)
  \end{gather*}
\end{proof}


\begin{example}[Utilizzo dei duali come mappe lineari]
  Posto $V$ come sottospazio vettoriale di $\mathbb{R}^{2}$ e posta la mappa
  \begin{gather*}
      \omega  : \mathbb{R}^{2} \to \mathbb{R} \Rightarrow (x_1, x_2) \to x_1 + 2x_2, \qquad \omega \in V^{\vee} 
  \end{gather*}
  Dato che è una mappa lineare, possiamo suddividere questo covettore in due
  vettori duali diversi
  \begin{gather*}
      pro_1 : \mathbb{R}^{2} \to \mathbb{R}, \Rightarrow  (x_1, x_2) \to x_1 \\
      pro_2 : \mathbb{R}^{2} \to \mathbb{R}, \Rightarrow (x_1, x_2) \to 2x_2  \\
      \omega : \mathbb{R}^{2} \to \mathbb{R} = pro_1 + 2pro_2 
  \end{gather*}
  Qualsiasi vettore che viene moltiplicato alla mappa $\omega$ diventa un 
  numero reale il cui valore vale la somma della sua proiezione sulla sua prima
  componente più due volte la proiezione sulla sua seconda componente. \\
  Si osserva inoltre come adesso si ottiene una base per $V^{\vee}$ in
  termini di $pro_1$ e $pro_2$. Passato il vettore $(3, -1)$ si ha
  \begin{gather*}
      \omega(3, -1) = 3 + 2(-1) = 1 \\
      \omega(2, -1) = pro_1(3, -1) + 2pro_2(3, -1) = 3 + 2(-1) = 1
  \end{gather*}
\end{example}

\begin{example}[I duali visti come l'insieme delle operazioni negli integrali e nelle derivate]
  Preso lo spazio vettoriale $V$ come sottospazio delle funzioni continue
  in un intervallo $C^{0}([a, b])$. Possiamo prendere un covettore $I \in V^{\vee}$, questa
  mappa ci permette di prendere una funzione in questo intervallo e ottenere 
  un valore reale dallo spazio delle funzioni continue. Possiamo allora 
  definire il covettore $I$ come
  \begin{gather*}
      I : C^{0}([a, b]) \to \mathbb{R}, \Rightarrow f \to \int_{a}^{b} f(x) \ dx
  \end{gather*} 
  Che ci da esattamente un numero
\end{example}

\begin{example}[Gli spazi duali con i polinomiali (derivando)]
  Possiamo definire uno spazio vettoriale $V \in P_{n}[x]$. Se consideriamo
  la mappa lineare appartenente al duale di questo spazio $D_a \in V^{\vee}$.
  \begin{gather*}
      D_a : P_n[x]  \to \mathbb{R} \Rightarrow P \to P'(a)
  \end{gather*}
  Questo covettore mi permette di prendere un vettore dello spazio polinomiale
  e calcolarne la derivata in un punto specifico $a$. Il calcolo della
  derivata in un punto specifico è un covettore dello spazio polinomiale
  così come il calcolo del valore del polinomio in un certo punto.
\end{example}

\begin{example}[Esempio da un esercizio di esame]
  Sia $V = \mathbb{R}[x]_{\leq 2}$ lo spazio vettoriale dei polinomi di grado $\leq 2$. Sia
  allora $B = \{1, x, x^{2} \}$ base di $V$ e sia $B^{\vee} = \{v^1, v^2, v^3\}$ la base
  duale. Per ogni $a \in \mathbb{R}, a \geq 0$ si definisca $L_a \in V^{\vee}$ come $L_a(f) = \int_{0}^{a} f(x) \ dx, \forall f \in V$. \\
  (i) Si trovino le coordinate di $L_a$ rispetto a $B^{\vee}$. \\
  (ii) Si provi che $\{L_1, L_2, L_3\}$ formano una base di $V$. \\
  (iii) I vettori $\{L_0, L_1, L_2\}$ formano una base di $V$.\\
  (iv) E' vero che $L_a + L_b = L_{a+b}, \forall a, b \in \mathbb{R}, a, b \geq 0$ ? \\ \\
  i) Vale, dato che sono coordinate rispetto ad uno spazio duale
  \begin{gather*}
      L_a = L_a(1)v^{1} + L_a(x)v^{2} + L_a(x^{2} )v^{3}   
  \end{gather*}
  Dato che $L_a(f) = \int_{0}^{a}f(x) \ dx$ si ha che
  \begin{gather*}
      L_a = av^{1} + \frac{a^{2} }{2}v^{2} + \frac{a^{3} }{3}v^{3}   
  \end{gather*}
  E allora le coordinate sono i fattori rispetto ai vettori della
  base duale $\left(a; \frac{a^{2} }{2}; \frac{a^{3} }{3}\right)$. \\
  ii) Si calcolano le coordinate rispetto alla base duale in funzione di $a$ e allora
  \begin{gather*}
      L_1 = \left(1; \frac{1}{2}; \frac{1}{3}\right) \\
      L_2 = \left(2; 2; \frac{8}{3}\right) \\
      L_3 = \left(3; \frac{9}{2}; 9\right) 
  \end{gather*}
  Mettendoli in colonna ed affiancandoli per formare la matrice
  della base si ottiene che ha rango massimo e dunque è una base di $V^{\vee} $.\\
  iii) Si calcola $L_0$ che non avevamo prima
  \begin{gather*}
      L_0 = \left(0; 0; 0\right)
  \end{gather*}  
  Allora non è una base poiché gli altri due vettori non sono sufficienti a 
  raggiungere il rango $3$. \\
  iv) Calcolando le coordinate di $L_{a + b}$, dato che
  è una applicazione lineare possiamo supporre che sia uguale alle coordinate rispetto
  a $L_a  + L_b$ e quindi
  \begin{gather*}
      \left(a + b; \frac{a^{2} }{2} + \frac{b^{2} }{2}; \frac{a^{3} }{3} + \frac{b^{3} }{3}\right)
  \end{gather*}
  Mentre le coordinate di $L_{a + b}$ sono:
  \begin{gather*}
      \left(a + b; \frac{(a + b)^{2} }{2}; \frac{(a + b)^{3} }{3}\right)
  \end{gather*}
  E quindi in generale non sono uguali
\end{example}


\chapter{Complementi alle matrici, il determinante}
\section{Spazi generati da righe e colonne e rango}
Sia $\mathbb{K}$ un campo e sia $m, n \in \mathbb{N} - \{0\}$ e sia $A \in M(m \times n, \mathbb{K})$. Ad essa
si associano due spazi vettoriali: un sottospazio $\mathbb{K}^{m}$ generato dalle colonne
di A $\left< A^{1},  \dots, A^{n}  \right>$ ed il sottospazio generato dalle righe
di A in $\mathbb{K}^{n}$ : $\left< A_{1}, \dots, A_{m} \right> $. Per essere precisi in realtà
si ha che nel secondo caso sarebbe il sottospazio generato dalle trasposte di A in quanto
$\mathbb{K}^{n}$ è generato dalle matrici $n \times 1$, mentre le righe sono matrici $1 \times n$.
Ovviamente il sottospazio generato dalle colonne di A ed il sottospazio generato dalle
righe di A non sono uguali ma la loro dimensione è la stessa.

\begin{example}
  \begin{gather*}
    A = \left( \begin{tabular}{c c c c}
      3 & 0 & 3 & -3 \\
      0 & 1 & 2 & 1 \\
      0 & 2 & 4 & 2
    \end{tabular} \right)
\end{gather*} 
Le sue colonne generano il seguente sottospazio di $\mathbb{R}^{3}$:
\begin{gather*}
    \left< \begin{pmatrix} 3 \\
    0\\
    0 \end{pmatrix}, \begin{pmatrix} 0 \\
    1\\
    2 \end{pmatrix}, \begin{pmatrix} 3\\
    2\\
    4 \end{pmatrix}, \begin{pmatrix} -3\\
    1\\
    2 \end{pmatrix}  \right>  = \left< \begin{pmatrix} 3 \\
    0\\
    0 \end{pmatrix}, \begin{pmatrix} 0 \\
    1\\
    2 \end{pmatrix}   \right>. 
\end{gather*} 
Mentre lo spazio generato dalle righe è:
\begin{gather*}
    \left< \begin{pmatrix} 3\\
    0\\
    3\\
    -3 \end{pmatrix} , \begin{pmatrix} 0\\
    1\\
    2\\
    1 \end{pmatrix}  \right>. 
\end{gather*}
Entrambi hanno dimensione 2.
\end{example}

\begin{observation}
  Siano A e B due matrici a coefficienti in $\mathbb{K}$ tali che il numero delle colonne di A
  è uguale al numero delle righe di B. Allora lo spazio generato dalle colonne di AB è contenuto
  nello spazio generato dalle colonne di A. Inoltre lo spazio generato dalle righe di AB è contenuto
  nello spazio generato dalle righe di B.
\end{observation}
\begin{proof}
  Per le proprietà del rango delle matrici, ora questo spazio vettoriale
  è contenuto all'interno dello spazio vettoriale delle colonne di $^{t} B$
  poiché, data la riga $j$-esima $b_j$ della trasposta, la colonna $j$-esima della matrice $AB$
  sarà data da $AB^{(j)} = Ab_j$. Prese le colonne di $A$, possiamo esprimere allora questa
  colonna come combinazione lineare delle colonne di $A$ con coordinate le coordinate 
  della trasposta della colonna $b_j$. Si è dimostrato allora che $C(AB) \subset C(A)$. \\
  Possiamo indicare lo spazio generato dalle colonne di $A$ come $C(A)$ e
  lo spazio generato dalle righe di $B$ come $R(B)$:
  \begin{gather*}
      R(AB) = C(\ ^{t} (AB) ) = C(\ ^{t}B \ ^{t}A  )
  \end{gather*}
  Allora possiamo dire che $C(\ ^{t}B \ ^{t}A  ) \subset C(^{t}B ) = R(B)$. 
\end{proof}

\begin{definition}[Rango per righe e rango per colonne]
  Sia $A \in M(m \times n, \mathbb{K})$. Si definisce il \textbf{rango per righe} di A la dimensione del
  sottospazio $\mathbb{K}^{n}$ generato dalle righe di A e si denota come:
  \begin{align}
      rk_r(A).
  \end{align} 
  Si definisce invece il \textbf{rango per colonne} di A la dimensione del sottospazio vettoriale
  di $\mathbb{K}^{m}$ generato dalle colonne di A e si denota come:
  \begin{align}
      rk_c(A).
  \end{align} 
\end{definition}

\begin{observation}[I ranghi sono minori del minimo tra numero di righe e colonne]
  Sia $A \in M(m \times n, \mathbb{K})$. Allora si ha che:
  \begin{align}
      rk_r(A) \leq \min \{m, n\}
  \end{align}
  Analogamente si ha che:
  \begin{align}
      rk_c (A) \leq \min\{m, n\}
  \end{align}
\end{observation}
\begin{proof}
  Infatti $rk_r(A)$ è la dimensione del sottospazio generato dalle righe di $A$, dunque
  è sicuramente minore o uguale di $n$ ma allo stesso tempo è generato da $m$ righe, il che vuol dire
  che sarà anche sicuramente minore o uguale a $m$. Lo stesso vale per le colonne. 
\end{proof}

\begin{observation}
  Lo spazio generato dalle righe di una matrice non cambia facendo operazioni elementari di
  righe e quindi non cambia il suo rango. Lo stesso vale per le colonne.
\end{observation}
\begin{proof}
  Deriva direttamente dalle proprietà degli spazi vettoriali:
  cambiare l'ordine dei generatori, moltiplicarli per uno scalare
  non nullo o fare combinazioni lineari tra loro non altera in alcun modo lo spazio generato.
  Fare quindi operazioni di riga o di colonna non altera gli spazi vettoriali
  generati dalle righe e dalle colonne.
\end{proof}

\begin{lemma}[Il rango di A è uguale al numero di pivot]
  Sia A una matrice a scalini, allora $rk_r(A) =$ numero di pivot. 
\end{lemma}
\begin{proof}
  Sia $k$ il numero degli scalini di A, ossia il numero delle righe non nulle di A. Allora
  basta dimostrare che le prime $k$ righe di A sono linearmente indipendenti, il che si ottiene
  ponendo $j_1, \dots, j_k$ gli indici delle colonne dove compaiono i pivot. Siano 
  $\lambda_1, \dots, \lambda_k \in \mathbb{K}$ tali che:
  \begin{gather*}
      \lambda_1 A_1 + \dots + \lambda_k A_k = 0
  \end{gather*}
  Considerando i $j_1$-esimi coefficienti si ottiene che:
  \begin{gather*}
    \lambda_1 A_{1, j_1} + \dots +  \lambda_k A_{k, j_1} = 0 \ \Longrightarrow \ \lambda_1 A_{1, j_1} = 0 \ \Longrightarrow \  \lambda_1 = 0 
  \end{gather*}
  In quanto $A_{1, j_1} \ne 0$ poiché è un pivot. Allora considerando i $j_2$-esimi coefficienti
  nella prima equazione, si ottiene che anche qui:
  \begin{gather*}
      \lambda_1 A_{1, j_2} + \dots + \lambda_k A_{k, j_2} = 0
  \end{gather*}
  Il che vuol dire che anche qui $\lambda_2 A_{2, j_2} = 0$, allora $\lambda_2 = 0$ 
  in quanto deve essere un pivot. Si ripete dunque il procedimento per tutti i $k$ pivot: 
  \begin{gather*}
      \lambda_1 = \dots = \lambda_k = 0.
  \end{gather*}
\end{proof}

\clearpage
\begin{lemma}[La dimensione del Kernel dipende dal rango della matrice]
  Sia $A \in M(m \times n, \mathbb{K})$ allora si ha che:
  \begin{align}
      \dim (\ker(f_A)) = n - rk_r(A).
  \end{align}
\end{lemma}
\begin{proof}
  Per la definizione di Kernel
  \begin{gather*}
      \ker(f_A) = \{x \in \mathbb{K}^{n} : Ax = 0\}
  \end{gather*}
  Allora si dimostra che:
  \begin{gather*}
      \dim \{x \in \mathbb{K}^{n} : Ax = 0 \} = n - rk_r(A).
  \end{gather*}
  Valendo la proposizione sulle operazioni sulle righe e colonne
  e l'inalterabilità dei ranghi, si ha che la tesi deve essere: 
  \begin{gather*}
      \dim\{x \in \mathbb{K}^{n} : A'x = 0\} = n - rk_r(A') = n - k
  \end{gather*}
  Dove $A'$ è una matrice a scalini, con $k$ scalini, ottenuta a partire dalla matrice di partenza A.
  Occorre allora dimostrare che l'insieme delle soluzioni della 
  matrice incompleta $A'$ ha dimensione $n - k$. Le variabili
  associate ai pivot saranno allora $x_1, \dots, x_k$, che utilizziamo 
  in delle equazioni in funzione di quelle non associate ai pivot $x_{k + 1}, \dots, x_n$ (le variabili libere), ottenendo
  il seguente sistema delle soluzioni:
  \begin{gather*}
    \begin{array}{l}
              x_1 = f_1(x_{k + 1}, \dots, x_n) \\
      \vdots \\
      x_k = f_k(x_{k  + 1}, \dots, x_n)
    \end{array} \ \Longrightarrow \ 
      \left\{
        x_{k + 1}\begin{pmatrix} . \\
        .\\
        1\\
        0\\
        .\\
        0 \end{pmatrix} + \dots + x_n \begin{pmatrix} 
        .\\
        .\\
        .\\
        .\\
        .\\
        1 \end{pmatrix}: x_{k + 1}, \dots, x_n \in \mathbb{K}  
      \right\}
  \end{gather*}
  E quindi il $\ker$ ha dimensione $n - k$.
\end{proof}

\begin{lemma}[La dimensione dell'immagine è uguale al rango della matrice]
  Sia $A \in M(m \times n, \mathbb{K})$ Allora si ha che:
  \begin{align}
      \dim(Im(f_A)) = rk_c(A)
  \end{align}
\end{lemma}
\begin{proof}
  Segue dalla proposizione dell'immagine di una applicazione lineare: infatti $Im(f_A)$ è 
  generata dalle colonne di $A$
  \begin{gather*}
      Im(f_A) = \{f_A(x) : x \in \mathbb{K}^{n} \} = \{Ax : x \in \mathbb{K}^{n} \} = \\
      \{x_1A^{(1)} + \dots + x_nA^{(n)} : x_1, \dots, x_n \in \mathbb{K}^{n}   \} = \left< A^{(1)}, \dots, A^{(n)}   \right> 
  \end{gather*}
  E dunque la dimensione dell'immagine è uguale alla dimensione del sottospazio 
  generato dalle colonne di $A$. 
\end{proof}

\begin{theorem}[Teorema dell'equivalenza del rango]
  Sia $A \in M(m \times n, \mathbb{K})$. Allora si ha che 
  \begin{align}
      rk_r(A) = rk_c(A)
  \end{align}
\end{theorem}
\begin{proof}
  Si ha, per la proposizione precedente, che il rango:
  \begin{gather*}
      rk_c(A) = \dim(Im(f_A)) =
  \end{gather*}
  Per la relazione fondamentale:
  \begin{gather*}
      = n - \dim(\ker(f_A)) =
  \end{gather*}
  Per la proposizione prima della precedente:
  \begin{gather*}
      = rk_r(A).
  \end{gather*}
\end{proof}

\begin{definition}[Rango di una matrice]
  Sia $A \in M(m \times n, \mathbb{K})$ Allora si definisce il \textbf{rango} di una matrice come
  il suo rango per righe o per colonne come $rk(A)$.
\end{definition}

\section{Teorema di Rouché-Capelli}
\begin{theorem}[Teorema di Rouché-Capelli]
  Siano $\mathbb{K}$ un campo ed $m, n \in \mathbb{N} - \{0\}$ e sia $A \in M(m \times n, \mathbb{K})$ e sia $b \in \mathbb{K}^{m}$.
  Il sistema lineare $Ax = b$ ha soluzione se e solo se $rk(A) = rk(A | b)$. 
\end{theorem}
\begin{proof}
  $ \Longrightarrow \ $Il sistema lineare $Ax = b$ ha soluzione se e solo se esistono $x_1, \dots, x_n \in \mathbb{K}$ tali che
  $b$ deve essere combinazione lineare di $A^{(1)}, \dots, A^{(n)}$ :
  \begin{gather*}
      x_1A^{(1)} + \dots + x_nA^{(n)} = b,
  \end{gather*}
  Questo implica che i due sottospazi
  \begin{gather*}
      \left< A^{(1)}, \dots, A^{(n)} \right> = \left< A^{(1)}, \dots, A^{(n)}, b  \right>  
  \end{gather*}  
  sono uguali ed in particolare il primo è contenuto nel secondo. Dire che questi due sono uguali
  equivale a dire che le loro dimensioni sono uguali, dunque $rk(A) = rk(A|b)$.\\
  $\Longleftarrow \ $Se $rk(A) = rk(A|b)$, i due sottospazi
  generati dalle colonne delle due matrici $A$ e $A|b$ sono uguali, ossia che
  \begin{gather*}
      \left< A^{(1)}, \dots, A^{(n)} \right> = \left< A^{(1)}, \dots, A^{(n)}, b  \right>  
  \end{gather*}
  Il che vuol dire che $b$ è combinazione lineare dello spazio generato dalle colonne di $A$, dunque
  il sistema lineare ha soluzione poiché $b$ è combinazione lineare delle colonne della matrice di $A$. 
\end{proof}

\section{Il Determinante}
\begin{definition}[Insieme delle matrici quadrate]
  Sia $\mathbb{K}$ un campo. Denotiamo con $Q_\mathbb{K}$ l'insieme delle matrici quadrate a coefficienti
  in $\mathbb{K}$, ossia:
  \begin{align}
      Q_\mathbb{K} = \cup_{n \in \mathbb{N} - \{0\}}M(n \times n, \mathbb{K}).
  \end{align}
\end{definition}

\begin{definition} [La funzione determinante]
    Una funzione $D$ qualunque, tale che $D: M(n \times n, \mathbb{K}) \to \mathbb{K}$, si dice
    \textbf{multilineare sulle righe} se è lineare come funzione di ciascuna riga.\\
    La parola multilineare vuol dire che la matrice deve essere lineare su tutte le righe,
    si dice anche, se una matrice è $n \times n$, che è lineare $n$ volte. La matrice
    A è tal per cui $A_j = A_j'$ e la $i$-esima riga $A_i = \lambda A_i' + \mu A^{"}_i$ con $j \neq i$. La nostra matrice $A$ è costruita
    in modo tale che tutte le righe delle matrici $A'$ e $A''$ e $A$  siano uguali al di fuori della riga 
    $i$, la quale  è combinazione lineare delle righe delle altre due matrici ottenendo:
    \begin{align}
        D(A) = \lambda D(A') + \mu D(A'')
    \end{align}
    Una funzione $D: M(n \times n, \mathbb{K}) \to \mathbb{K}$ si dice
    \textbf{alternante} sulle righe se 
    \begin{align}
        D(A) = 0
    \end{align} 
    $\forall A \in M(n \times n, \mathbb{K})$ con due righe uguali. \\
        Sia $D(Q_{\mathbb{K}}) \to \mathbb{K}$ una funzione lineare, essa si dice multilineare sulle righe
    rispettivamente \textbf{alternante} se $\forall n > 0 \in \mathbb{N}$ vale che se $D$ è ristretta
    alle matrici $n \times n$ è multilineare sulle righe rispettivamente alternativamente. 
\end{definition}

\begin{lemma}[Inversione di segno del determinante a seguito di uno scambio di righe]
    Sia $D : M(n \times n, \mathbb{K}) \to \mathbb{K}$ multilineare sulle righe. Se $D$ è alternante
    sulle righe e $\tilde{A}$ si ottiene da $A$ scambiando due righe allora vale
    \begin{align}
        D(A) = - D(\tilde{A}).
    \end{align}
    Vale anche il viceversa supponendo se solo se il campo è tale che $1_K + 1_K \neq 0_K$.
    Nel piano (matrici $2 \times 2$), un'operazione del genere vuol dire ribaltare il parallelogramma 
    identificato dalla matrice. 
\end{lemma}
\begin{proof}
  La relazione tra le matrici $\tilde{A}$ e $A$ è la seguente:
  \begin{gather*}
          \begin{tikzpicture}
            \node at (1, 2.5) {$A$};
            \node at (4, 2.5) {$\tilde{A}$};
        \draw(0, 0) rectangle (2, 2);
        \draw(3, 0) rectangle (5, 2); 
        \draw[pattern=north west lines, pattern color = white](0, 0.5) rectangle (2, 0.75);
        \draw[pattern=north west lines, pattern color =white](0, 1.25) rectangle (2, 1.5);
        \draw[pattern=north west lines, pattern color =white](3, 0.5) rectangle (5, 0.75);
        \draw[pattern=north west lines, pattern color =white](3, 1.25) rectangle (5, 1.5);
        \draw[->, thick] (2, 1.35) -- (3, 0.6);
        \draw[->, thick] (2, 0.6) -- (3, 1.35);
        \node at (-0.5, 0.6) {$j$};
        \node at (-0.5, 1.35) {$i$};
        \node at(5.5, 0.6) {$i$};
        \node at (5.5, 1.35) {$j$}; 
    \end{tikzpicture}   
  \end{gather*}
    Considero una nuova matrice $B$ che alla riga $i$ e alla riga $j$ vale
    \begin{gather*}
        B_i = B_j = A_i + A_j.
    \end{gather*}
    E tutte le altre righe sono uguali a quelle della matrice $A$. Dato che $D$ è 
    multilineare ed alternante per ipotesi, allora 
    \begin{gather*}
        D(B) = 0 = D(A) + D(\tilde{A}) + D(A') + D(A'')
    \end{gather*}
    Dove $A'$ e $A''$ compiano a seguito della definizione di $D$:
    \begin{itemize}
        \item $A'$ è la matrice che ha le righe $i$, $j$ uguali a $A_i$ e le altre uguali ad $A$.
        \item $A''$ è la matrice che ha le righe $i$, $j$ uguali a $A_j$ e le altre uguali ad $A$.
    \end{itemize}
    Per l'alternanza, allora si deve avere che
    \begin{gather*}
        D(A') + D(A'') = 0
    \end{gather*}
    Dunque si deve avere per $D(B)$:
    \begin{gather*}
        0 = D(B) = D(A) + D(\tilde{A})
    \end{gather*}
    Da cui la tesi. Viceversa, se $ D$ è tale che
    \begin{gather*}
        D(A) = - D(\tilde{A})
    \end{gather*}
    E se $\tilde{A}$ è ottenuta da $A$ mediante lo scambio di due righe,
    allora, se $A$ è una matrice con due righe uguali, si deve avere $D(A) = - D(A)$. Dunque
    $D(A) = 0$. 
\end{proof}

\begin{lemma}[Invarianza del determinante per operazioni di Gauss di terzo tipo]
    Sia $D: M(n \times n, \mathbb{K}) \to \mathbb{K}$ multilineare sulle righe e alternante.
    Se $A'$ si ottiene da $A$ con una operazione elementare di tipo 3 di Gauss sulle
    righe allora $D(A') = D(A)$. In altre parole questa funzione è invariante
    per operazioni elementari di tipo 3. 
\end{lemma}
\begin{proof}
  Si considera una matrice $A'$ che ha tutte le righe uguali alla matrice $A$, ma
  \begin{gather*}
      A_i' = A_i + \lambda A_j 
  \end{gather*}
  Tuttavia, dato che la funzione determinante è multilineare ed alternante per definizione,
    \begin{gather*}
        D(A') = D(A) + \lambda D(A'') \ \Longrightarrow \ D(A'') = 0
    \end{gather*}
    Dove $A''$ è la matrice che ha le stesse righe di $A$, ma alla riga
    $i$ ha $\lambda A_j$ e alle altre righe le stesse di $A$. Dunque si ha la tesi se si porta fuori
    $\lambda$ per multilinearità e, avendo la matrice $A''$ due righe uguali, il
    suo determinante è esattamente zero.   
\end{proof}

\begin{lemma}[Se una riga è nulla il determinante è nullo]
  Sia $D: M(n \times n, \mathbb{K}) \to \mathbb{K}$ multilineare sulle righe e alternante.
  Se $A$ è una matrice con una riga nulla, allora il determinante vale zero.
\end{lemma}
\begin{proof}
  Dato che, per definizione, il determinante di una matrice si ottiene
  \begin{gather*}
      D(A) = \lambda D(A') + \mu D(A'')
  \end{gather*}
  Allora questo vuol dire che la riga $i$-esima è ottenuta attraverso la 
  combinazione lineare delle righe della altre due matrici. Dato che la riga
  della matrice $A$ è nulla, allora si ha che gli unici valori per i coefficienti sono $\lambda = \mu = 0 \ \Longrightarrow \  D(A)$ è nullo
  in quanto le matrici $A'$ e $A''$ sono diverse.
\end{proof}

\begin{lemma}[Determinante di una matrice diagonale]
    Sia $D: M(n \times n, \mathbb{K}) \to \mathbb{K}$ multilineare sulle righe e alternante.
    Se $A$ è diagonale, allora il determinante è il prodotto degli elementi sulla diagonale.
\end{lemma}
\begin{proof}
  Si può portare fuori $d_1$ per la linearità, posso fare lo stesso
    con la seconda riga ed ottengo sempre:
    \begin{gather*}
        d_1 d_2\left[ \begin{tabular}{c c c}
            1 \dots \dots \\
            \dots 1 \dots \\
            \dots \dots $d_n$
        \end{tabular}\right]
    \end{gather*}
    Reiterando per $n$ volte si ottiene che la matrice diventa
    la matrice identità di dimensione $n$, per cui il processo
    di normalizzazione è completo ed è dimostrato.
\end{proof}

\begin{observation}
Le ultime tre proposizioni valgono anche se le matrice sono alternanti e multilineari sulle
colonne.
\end{observation}

\begin{definition}[Determinante normalizzato]
    La $D: Q_K \to \mathbb{K}$ si dice normalizzata se, quando è calcolata
    su di una matrice identità, è uguale ad 1:
    \begin{align}
        D(I_n) = 1 \qquad \forall n > 0 \in \mathbb{N}
    \end{align}
\end{definition}

\begin{theorem}[Esiste al massimo una funzione determinante]
    Esiste al massimo una funzione $D : M(n \times n, \mathbb{K}) \to \mathbb{K}$ che sia
    multilineare sulle righe, alternante sulle righe e  normalizzata. 
\end{theorem}
\begin{proof}
    Voglio dimostrare che se $D_1$ e $D_2$ soddisfano tutte le proprietà allora $D_1 = D_2$.
    L'idea è usare gauss e ricondursi ad una forma diagonale. \\
    Presa $A \in M(n \times n, \mathbb{K})$ devo provare che $D_1 = D_2$ e tutte e due soddisfano
    le proprietà. Si riduce a scalini $A$ con operazioni elementari di tipo $I, III$ con scambi e
    somma di multipli. Sia $A'$ questa nuova matrice a scalini ottenuta con $k$ scambi: ogni volta che si fa uno scambio,
    dato che $D_1$ e $D_2 $ sono delle funzioni determinanti, il segno del risultato
    è cambiato di segno $k$ volte:
    \begin{gather*}
        D_1(A) = (-1)^{k}D_1(A') \\
        D_2(A) = (-1)^{k}D_2(A') 
    \end{gather*}
    Dato che la dimostrazione si divide in due, possiamo supporre che i due tipi di 
    matrici che ci permettono di dimostrare entrambe le parti del teorema siano del
    seguente tipo: una è triangolare superiore con
    elementi diagonali $\neq 0$ (la prima), mentre l'altra ha l'ultima riga nulla. 
    Se $A'$ ha l'ultima riga nulla allora:
    \begin{gather*}
        0 = D_1(A') = D_2(A') \\
        0 = D_1(A) = D_2(A)
    \end{gather*}
    E dunque $D_1 = D_2$, invece, se si considera la matrice triangolare superiore,
    si dovrebbe applicare Gauss all'indietro con operazioni di terzo
    tipo ottenendo una forma diagonalizzata 
    \begin{gather*}
        D_1(A') = D_1(A) \\
        D_2(A') = D_2(A)
    \end{gather*}
    Ottenendo ancora la tesi per cui $D_1 = D_2$.
\end{proof}

\begin{observation}[La funzione determinante è unica]
    $\exists !$ funzione $: D : Q_K \to \mathbb{K}$ con le stesse proprietà
    del determinante.
\end{observation}
\begin{proof}
    Ci si riconduce ad una complessità sempre minore per il determinante, 
    per le matrici $n \times n$ ci si riconduce alle matrici $n - 1 \times n - 1$
    ed il caso di partenza è proprio la matrice $1 \times 1$, che è lineare e
    multilineare, normalizzata e l'alternanza non ha senso.  \\
    Scelta una riga qualunque il determinante non cambia ed è sempre quello:
    scegliendo la riga $i$ e posso fare lo sviluppo del determinante su questa riga:
    \begin{gather*}
        a_{i, 1} \dots a_{i, n}
    \end{gather*}
    Posso creare una matrice con segni alternati a modo di scacchiare, lo sviluppo
    di Laplace consiste nel prendere il coefficiente di ogni riga in modo da togliere 
    la riga e la colonna corrispondente di quel determinato $a_{i, j}$. Si prende
    il coefficiente col segno e si moltiplica con il determinante $n - 1 \times n-1$ corrispondente:
    Il Determinante di $A$ è:
    \begin{gather*}
        \det(A) = \sum_{j = 1}^{n} (-1)^{i + j}a_{i, j}\det A_{\hat{i} , \hat{j} }  
    \end{gather*}
    Funziona qualunque sia la riga $i$. Inoltre, con $\hat{i}$ e $\hat{j}$ si indica
    la matrice con la riga $i$-esima eliminata e la colonna $j$-esima eliminata. Questa definizione permette di calcolare ricorsivamente
    il determinante di ogni matrice continuando ad applicare all'indietro per matrici sempre
    più semplici (LA DIM NON E' DA FARE)
\end{proof}
\begin{example}
  Calcolare il determinante di una matrice 2 per 2:
\begin{gather*}
    \left( \begin{tabular}{c c}
        $a_{1, 1}$ $a_{1,2} $ \\
        $a_{2, 1}$ $a_{2,2}$
    \end{tabular}\right) \quad \text{quindi} \quad \left(\begin{tabular}{c c}
        + - \\
        - + 
    \end{tabular}\right) \ \Longrightarrow \ a_{1, 1} a_{2, 2} - a_{1, 2} a_{2, 1}
\end{gather*}
Con lo sviluppo di Laplace si ottiene il determinante di una matrice $1 \times 1$
per cui la matrice è normalizzata, lineare e multilineare e quindi esiste un unico
determinante calcolabile che è esattamente l'unico elemento della matrice.
Si può applicare Laplace anche alle colonne e non solo
alle righe.
\end{example}

\begin{definition}[Il determinante delle righe è uguale a quello delle colonne]
  Si definisce \textbf{determinante} di una matrice quadrata A lo scalare $\det^{R}(A)$ 
  oppure, equivalentemente, lo scalare $\det^{C}(A)$. 
  \begin{align}
      \det^{R}(A) = \det^{C}(A) = \det(A)   
  \end{align}
\end{definition}

\begin{theorem}[Il determinante di una matrice è uguale a quello della sua trasposta ]
  $\forall n \in \mathbb{N} - \{0\}$ per qualsiasi campo $\mathbb{K}$ e per 
  qualsiasi $A \in M(n \times n, \mathbb{K})$ si ha che:
    \begin{align}
        \det(A) = \det(^{t}A)
    \end{align}
\end{theorem}
\begin{proof}
  Dato che 
  \begin{gather*}
      \det(A) = \det^{C} (A) = \det^{R}(A) 
  \end{gather*}
  Allora vale anche per le trasposte
  \begin{gather*}
      \det(^{t} A) = \det^{C} (^{t}A ) = \det^{R}(^{t}A )  
  \end{gather*}
  Sappiamo, per definizione di matrice trasposta la seguente condizione, la quale ci porta alla tesi:
  \begin{gather*}
      \det^{C} (A) = \det^{R}(^{t}A ) 
  \end{gather*}
\end{proof}

\begin{example}
  Calcolare:
\begin{gather*}
    \left[ \begin{tabular}{c c c}
        2 0 -1 \\
        2025 7 $\pi$ \\
        3 0 $\sqrt{2}$ 
    \end{tabular}\right]
\end{gather*}
Conviene usare Laplace sulla seconda colonna ottenendo:
\begin{gather*}
    7\left| \begin{tabular}{c c}
        2 -1 \\
        3 $\sqrt{2}$ 
    \end{tabular} \right| = 7(2\sqrt{2} + 3) 
\end{gather*}
\end{example}

\section{Relazione tra rango e determinante, la formula di Cauchy-Binet}
\begin{theorem}[Il rango di una matrice invertibile è massimo se e solo se il suo determinante non è zero]
  Sia $A \in M(n \times n, \mathbb{K})$, allora la matrice ha le seguenti proprietà:
  \begin{align}
      A \text{ invertibile} \Longleftrightarrow rk(A) &= n \Longleftrightarrow  \det(A) \neq 0
  \end{align}
  Altrimenti non è invertibile.
\end{theorem}
\begin{proof}
  Se $A$ è invertibile, allora si ha che $\exists B : AB = BA = I_n$
  se e solo se la funzione $f_A$ nella composizione $f_A \circ f_B = f_B \circ f_A = I$ è invertibile $\Longleftrightarrow$
  iniettiva $\Longleftrightarrow$ suriettiva, dunque la funzione $f_A$ va da uno spazio 
  di dimensione $n$ ad uno spazio di dimensione $n$ e dunque le colonne di $A$ sono indipendenti
  per la relazione fondamentale e hanno rango $n$. \\  
  Dimostriamo che $rk(A) = n \Longleftrightarrow  \det(A) \neq  0$:
  riducendo A a scalini, allora $rk(A) = n \Longleftrightarrow  \exists$ 
  la matrice ridotta a scalini $A' :  \det(A') \neq 0$ e quindi
   $A'$ è triangolare superiore e quindi $rk(A) = rk(A')$
  e quindi $\det(A) = \pm \det(A')$.
\end{proof}

\begin{theorem}[Criterio dei minori orlati]
  Siano $n, k \in \mathbb{N} - \{0\}$ e $\mathbb{K}$ un campo. Sia $A \in M(n \times n, \mathbb{K})$:
  \begin{gather*}
      rk(A) = k \Longleftrightarrow \exists B(k \times k), k < n : \det(B) \neq 0 \\
      \Longleftrightarrow \exists C, D, \dots \in M(k + 1 \times k + 1, \mathbb{K}), k + 1 < n 
  \end{gather*}
  tali che queste matrici sono più grandi di $B$ che la orlano e hanno determinante uguale a zero. \\
  In altri termini, posso dire che il rango di una matrice è uguale al numero di
  sottomatrici estrapolate da essa da $0, \dots, k$, dove $k$ è il rango finale della
  matrice, con determinante diverso da zero se e solo se tutte le matrici $k + 1, \dots, n$
  avranno determinante nullo.
\end{theorem}
\begin{proof}
  Prima della dimostrazione, una conseguenza importante è sicuramente quella del rango della matrice
  $A$ che è sicuramente $\geq$ rispetto al numero delle righe della matrice orlata $\Longleftrightarrow \ \exists$ una sottomatrice $k \times k$ di $A$ con $\det \neq 0$.  \\ 
  $\ \Longrightarrow \ $ Come ipotesi assumo che $rk_C(A) = k$, esistono degli
  indici $\{i_1, \dots, i_k\}$ tali che $\left<A_{i_1}, A_{i_2}, \dots, A_{i_k}\right>$ sia lo spazio delle
  righe di dimensione $k$ che generano tutte le righe di $A$. Allora esistono anche degli indici $j_1, \dots, j_k$ 
  tali per cui lo spazio generato dalle colonne è anch'esso linearmente independence
  e genera tutte le colonne della matrice $A$. Allora per il teorema sulle matrici invertibili,
  la sottomatrice $A (k \times k)$ ha determinante diverso da zero:
  \begin{gather*}
      \det\left( A_{i_1, \dots, i_k} ^{j_1 , \dots, j_k} \right) \neq 0
  \end{gather*}
  Chiamata $B$ questa matrice, se per assurdo $\exists$ la matrice orlata con colonne
  $j_1, \dots, j_{k + 1}$ tale che $B' = A_{i_1, \dots, i_{k + 1}}^{j_1, \dots, j_{k + 1}}$ se
  $\det(B') \neq 0$ allora lo spazio delle colonne di $B'$ deve avere dimensione $k + 1$ e le
  colonne sono linearmente indipendenti. Allora anche le colonne di
  $A^{j_1, \dots, j_{k + 1}}$ avranno come dimensione massima $k + 1$ e saranno indipendenti.
  Se avessi combinazione lineare nulla di queste colonne
  allora avrei lo stesso anche per la matrice $A$, il che
  sarebbe una contraddizione in quanto ha rango $k$ e non $k + 1$. \\
  $\Longleftarrow$ Chiamata 
  \begin{gather*}
      B = A_{i_1, \dots, i_k} ^{j_1 , \dots, j_k}
  \end{gather*}
  Tale che per ipotesi ha il determinante diverso da zero ed il determinante della 
  matrice che la orla è uguale a zero, si vuole dimostrare che il rango di $A$ è esattamente $k$. Dato che,
  per ipotesi
  \begin{gather*}
            \det\left( A_{i_1, \dots, i_k} ^{j_1 , \dots, j_k} \right) \neq 0
  \end{gather*}
  Sia le colonne, che le righe di questa matrice costituiscono un insieme di vettori linearmente indipendenti. 
  Supponendo per assurdo che esista $j_{k + 1}$ tale che la colonna con quell'indice non sia combinazione
  lineare di quell'insieme di vettori, allora il rango della matrice $A^{j_1, \dots, j_{k + 1}} = k + 1$. Dunque anche
  il rango per le righe deve essere uguale, il che implicherebbe che esista un indice $i_{k + 1}$ tale per cui 
  le righe della matrice $A^{j_1, \dots, j_{k + 1}}$ costituiscano un insieme di vettori indipendenti. Si avrebbe
  dunque che il determinante di questa matrice, che orla la matrice $B$, sia diverso da zero, il che sarebbe assurdo. 
\end{proof}

\begin{corollary}[Algoritmo di Kronecker]
  Per calcolare il $rk(A)$ di una matrice $A \in M(n \times n, \mathbb{K})$ estraggo una sottomatrice $A' \in M(k \times k, \mathbb{K})$
  $k > 1$ tale che $\det(A') \neq 0$, reitero fino a ottenere una matrice di dimensioni 
  meno uno rispetto a quella di partenza oppure fino a che $\det(A') = 0$,
  a quel punto il rango di una matrice è esattamente il numero di determinanti diversi
  da zero.
\end{corollary}

\begin{example}[Applicazione dell'Algoritmo di Kronecker]
  \begin{gather*}
    A = \left( \begin{tabular}{c c c c c}
      1 & 0 & 2 & 3 & -1 \\
      -2 & 0 & -4 & -6 & 2 \\
      2 & 1 & 2 & -6 & 2 \\
      0 & 1 & -2 & -12 & 4
    \end{tabular} \right)
\end{gather*}
Esiste una sottomatrice $2 \times 2$ con $\det \neq  0$ ossia.
\begin{gather*}
    B = \left( \begin{tabular}{c c}
      -2 & 0 \\
      2 & 1
    \end{tabular} \right)
\end{gather*}
e tutte le matrici $3\times 3$ hanno $\det = 0$ per cui il rango di A è proprio 2.
\end{example}


\begin{theorem}[Cauchy-Binet]
  Sia $\mathbb{K}$ un campo e sia $n \in \mathbb{N} - \{0\}$. Allora si ha che:
  \begin{align}
      \det( AB) = \det(A)\det(B), \qquad \forall A, B \in M(n \times n, \mathbb{K})
  \end{align}
\end{theorem}
\begin{proof}
  Se $\det(B)= 0$ allora $rk(B) < n$ e quindi le colonne di $B$ sono dipendenti; pertanto
  $\exists x \in \mathbb{K}^{n} - \{0\} : Bx = 0 \ \Longrightarrow \  ABx = 0$. Le colonne di $AB$ sono linearmente
  dipendenti e quindi, per la proposizione sullo spazio delle colonne e delle righe
  delle matrici prodotto: 
  \begin{gather*}
      \det(AB) = 0
  \end{gather*} 
  Se $\det(B) \neq 0$ allora si considera la funzione $d : M(n \times n, \mathbb{K}) \to \mathbb{K}$ tale che
  \begin{gather*}
      d(A) := \frac{\det(AB)}{\det(B)}
  \end{gather*}
  Per dimostrare che questa funzione è esattamente il determinante si dimostra prima che è
  multilineare e poi che è alternante sulle righe.
  Sia $A_{(k)} = \lambda v + \mu w$ per qualche $\lambda, \mu \in \mathbb{K}$ e 
  $v, w \in M(1 \times n, \mathbb{K})$ posta la $A'$ la matrice che ha la riga $k$-esima uguale a $v$
  e tutte le altre uguali a quelle di $A$ e sia $A''$ la matrice con la riga $k$-esima uguale
  a $w$ e tutte le altre uguali alle rispettive di $A$, allora : 
  \begin{align*}
      (AB)_{(k)} &= A_{(k)}B = (\lambda v + \mu w)B = \lambda vB + \mu w B = \\
      &= \lambda A'_{(k)} B + \mu A^{''}_{(k)}B = \lambda(A'B)_{(k)} + \mu(A^{''}B)_{(k)} \\
      (AB)_{(i)} &= A_{(i)}B = A'_{(i)}B = (A'B)_{(i)} \\
      (AB)_{(i)} &= A_{(i)}B = A^{''}_{(i)}B = (A^{''}B)_{(i)} 
  \end{align*}
  A questo punto per $i \neq k$ si ha il determinante e quindi si verifica la multilinearità. \\
  Dimostriamo ora che vale l'alternanza sulle righe: se $A$ ha due righe uguali allora si suppone:
  \begin{gather*}
      A_{(h)} = A_{(k)}
  \end{gather*}
  con $h \neq k$ allora:
  \begin{gather*}
      (AB)_{(h)} = A_{(h)}B = A_{(k)}B = (AB)_{(k)}
  \end{gather*}
  Quindi anche $AB$ ha due righe uguali e quindi $\det(AB) = 0 = d(A)$. Verifichiamo
  che sia normalizzata: dato che è multilineare ed alternante sulle righe e vale $1$ sulla
  matrice identità, allora si deve avere anche $d = \det$ e quindi
  \begin{gather*}
      \forall A \in M (n \times n, \mathbb{K}), \ d(A) = \det(A) \ \Longrightarrow \  \det(A) = \frac{\det(AB)}{\det(B)}
  \end{gather*}
  e quindi la tesi.
\end{proof}

\begin{corollary}[Il determinante dell'inversa è l'inverso del determinante]
  Sia $\mathbb{K}$ un campo e sia $n \in \mathbb{N} - \{0\}$. Sia $A \in GL(n, \mathbb{K})$,allora:
  \begin{align}
      \det(A^{-1} ) = \frac{1}{\det(A)}
  \end{align}
  In particolare se $A \in GL(n, \mathbb{K})$ e $B \in M(n \times n, \mathbb{K})$ allora:
  \begin{align}
      \det(A^{-1} BA) = \det(B)
  \end{align}
\end{corollary}
\begin{proof}
  Se $A$ è invertibile, allora 
  \begin{gather*}
      1 = \det(I) = \det(AA^{-1} ) = \det(A) \det(A^{-1} ) \ \Longrightarrow \ \det(A^{-1}) = \frac{1}{\det(A)}
  \end{gather*}
  Per la seconda invece: 
  \begin{gather*}
      \det(A^{-1} B A) = \det(A^{-1} ) \det(B) \det(A) = \det(A^{-1} )\det(A) \det(B) = \det(B)
  \end{gather*}
\end{proof}

\begin{lemma}[Determinante di una matrice a blocchi]
  In una matrice a blocchi, il determinante è semplicemente il prodotto dei determinati
  che compongono la matrice a blocchi:
  \begin{align}
      \det\left( \begin{tabular}{c | c}
        A & B \\
        \hline
        0 & C 
    \end{tabular} \right) = \det(A) \cdot  \det(C)
  \end{align}
  $\forall A \in M(n \times n, \mathbb{K}), B \in M(n \times m, \mathbb{K}), C \in M(m \times m, \mathbb{K})$.
\end{lemma}
\begin{proof}
  Si dimostra per induzione a partire dallo sviluppo di Laplace. \\
  $n = 1$: in questo caso il determinante, secondo lo sviluppo di Laplace, è il seguente:
  \begin{gather*}
       \det\left( \begin{tabular}{c | c}
        A & B \\
        \hline
        0 & C 
    \end{tabular} \right)= m_{1, 1} \cdot \det(C). \ \  \forall m \in \mathbb{K} \ \ \text{e} \  \ B \in M(n \times m, \mathbb{K}), \ C \in M(m \times m, \mathbb{K})
  \end{gather*}
  $n - 1 \ \Longrightarrow \ n$: Anche qui si utilizza lo sviluppo di Laplace sulla seguente matrice:
  \begin{gather*}
       \det\left( \begin{tabular}{c | c}
        A & B \\
        \hline
        0 & C 
    \end{tabular} \right) = a_{1, 1} \det(M_{\hat{1}, \hat{1}  }) + \dots + a_{1, k} \det(M_{\hat{1}, \hat{k}  })
  \end{gather*}
  Dunque, per il passo induttivo, il determinante della sottomatrice con $n-1$ è esattamente
  $\det(A) \cdot \det(C)$, dunque, non mi resta che dimostrare che, svolgendo la prima colonna, il determinante non sia altro che
  $a_{1, 1}\det(M_{\hat{1}, \hat{1}  })$. Questo è vero poiché il determinante di tutte le sottomatrici
  \begin{gather*}
      \det(M_{\hat{i}, \hat{1}  }) = \det(A_{\hat{i}, \hat{1}  }) \cdot \det(C) 
  \end{gather*}
  per $i = 1, \dots, k$. Dove le matrici $M_{\hat{i}, \hat{1}  }$ hanno la seguente forma
  \begin{gather*}
    M_{\hat{i}, \hat{1}  } = \begin{pmatrix}
        A_{\hat{i}, \hat{1} } & B_{\hat{1} } \\
        0 & C 
    \end{pmatrix} \ \Longrightarrow \ \det(M) = \det(C) \cdot \sum_{j = 1}^{n}a_{i, 1}(-1)^{i + 1} \det(A_{\hat{i}, \hat{1}  }) = \det(C)\cdot \det(A)
  \end{gather*}
\end{proof}

\begin{theorem}[Determinante di una matrice ortogonale]
  Il determinante di una matrice ortogonale 
  \begin{align}
      \det(A) = \pm 1
  \end{align}
  Si dice che il determinante rovescia l'orientazione se è $-1$ altrimenti mantiene l'orientazione
\end{theorem}
\begin{proof}
  L'inversa di A è la trasposta e quindi per ipotesi
\begin{gather*}
  A \cdot \  ^{t}A = I_n  \ \Longrightarrow \  \det(A \cdot \ ^{t}A) = 1
\end{gather*}
Allora il $\det(A) \cdot  \det(^{t}A ) = \det(A)^{2} = 1$, ossia la tesi. 
\end{proof}

\section{Determinante con le permutazioni}
\begin{definition}[Permutazioni]
  Una permutazione è un insieme finito $S$ di bigezioni da $S$ a $S$, ossia una gruppo 
  simmetrico e biettivo; una permutazione di una funzione $\sigma$ di un
  insieme di $n$ elementi in sé stessa è biunivoca. Quindi l'insieme 
  delle permutazioni forma un gruppo con $n!$ elementi dove l'operazione
  è la composizione e l'elemento neutro è la funzione identità. 
  \begin{enumerate}
      \item Ogni permutazione è composizione di scambi (permutazioni più semplici);
      \item La parità del numero di scambi è ben definita per ogni permutazione;
      \item $\sigma_n$ è un gruppo simmetrico con $n!$ elementi che contiene
      il gruppo $a_n$ chiamato gruppo alterno formato solo dalle permutazioni
      pari, ossia ogni permutazione è ottenuta con un numero pari di scambi
      l'identità (0 scambi) è nel gruppo alterno. 
  \end{enumerate}
  Ogni permutazione ha un segno ( definito come $\epsilon(\sigma)$) che può essere $\pm 1$: se è positivo
  si ottiene con numero pari di scambi, altrimenti se è $-1$ si ottiene con numero dispari di scambi.
\end{definition}\
\begin{example}
    \begin{gather*}
      \begin{pmatrix} 1 \\
      3\\
      2\\
      \end{pmatrix} \Rightarrow 
      \begin{array}{l}
          \sigma(1) = 1 \\
          \sigma(2) = 3 \\
          \sigma(3) = 2
      \end{array} 
  \end{gather*}
  Questa permutazione scambia 2 e 3.
\end{example}
\begin{theorem}[Formula del detrminante mediante le permutazioni]
  Se $A \in M(n \times n, \mathbb{K})$ allora il determinante di $A$ è
  \begin{align}
      \sum_{\sigma \in S_n}  \epsilon(\sigma) a_{1,\sigma(n)} \cdot  \dots \cdot  a_{n,\sigma(n)}
  \end{align} 
  Dove la colonna scelta dipende dal segno del $\sigma$.
  Ossia per una matrice $3 \times 3$ per le colonne col segno $+$ del $\sigma$:
  \begin{gather*}
      a_{1, 1} \cdot a_{2, 2} \cdot  a_{3, 3} + a_{1, 3} \cdot a_{2, 2} \cdot  a_{3, 1} + a_{1, 2} \cdot a_{2, 3} \cdot  a_{3, 1}
  \end{gather*} 
  Per le colonne invece col segno meno del $\sigma$:
  \begin{gather*}
      -a_{1, 1} \cdot a_{2, 2} \cdot  a_{3, 3} - a_{1, 3} \cdot a_{2, 2} \cdot  a_{3, 1} - a_{1, 2} \cdot a_{2, 3} \cdot  a_{3, 1}
  \end{gather*}
  Dove il secondo numero al pedice è il valore $\sigma$.
\end{theorem}

Il determinante nel 1800 era definito con questa formula e 
presentava diversi vantaggi:
\begin{enumerate}
  \item E' esplicita e se $a_{i, j} \in \mathbb{Z}$ mostra che il determinante $\in \mathbb{Z}$;
  \item Il determinante è un polinomio di grado $n$.
  \item Bella!
\end{enumerate}
Svantaggi:
\begin{enumerate}
  \item E' intrattabile dal punto di vista computazionale poiché $n!$ è enorme;
  \item Teoricamente alcuni risultati sono difficili 
\end{enumerate}
Il problema $P = NP$ è un problema nel quale ci si chiede se si
possa calcolare la permutazioni di A con un numero di operazioni
polinomiali in $n$ dove le permutazioni
\begin{gather*}
  Perm(A) = \sum_{\sigma \in S_n}  a_{1\sigma(n)} \cdot  \dots \cdot  a_{n\sigma(n)}
\end{gather*}

\section{Formula per l'inversa}
\begin{lemma}[Applicazioni alle matrici inverse]
  Sia $A \in M(n \times n,\mathbb{K})$, se questa è una matrice invertibile, allora esiste una matrice
  $B : AB = I_n$. Si dimostra che questa matrice è sia l'inversa destra, che l'inversa sinistra:
  \begin{align}
      AB = BA = I_n \ \Longrightarrow \ B = A^{-1} 
  \end{align}
\end{lemma}
\begin{proof}
  Dato che $AB = I_n$, si deve avere che
  \begin{gather*}
      f_A \circ f_B = I
  \end{gather*}
  Considero $f_A : \mathbb{K}^{n} \to \mathbb{K}^{n}$ allora:
  \begin{gather*}
      1_K = f_B = f_A \circ f_B \ \Longrightarrow \  f_A \text{ suriettiva}
  \end{gather*}  
  poiché
  \begin{gather*}
      f_A\left( f_B(x) \right) = x ,  \qquad \forall x \in \mathbb{K}^{n} 
  \end{gather*}
  e allora $f_A$ è suriettiva se e solo se il rango è massimo (è biunivoca)
  $\Longleftrightarrow rk(A) = n \ \Longleftrightarrow \ \det(A) \neq  0$.
  Per l'inversa sinistra invece: 
  \begin{gather*}
      BA = I_n, \quad f_B \circ f_A(y) = y \qquad \forall y \in \mathbb{K}^{n} 
  \end{gather*}
  Dato che $f_B$ è suriettiva, allora $\exists x \in \mathbb{K}^{n} : y = f_B(x)$ e quindi: 
  \begin{gather*}
      f_Bf_A(y) = f_B(f_A(f_B(x))) = f_B(I(x)) = f_B(x) = y.
  \end{gather*}
\end{proof}

\begin{theorem}[Formula per l'elemento della matrice inversa]
  Se $A$ è invertibile l'elemento alla posizione $i, j$ della matrice inversa sarà dato dalla formula
  \begin{align}
      \boxed{(A^{-1} )_{i, j} = \frac{(-1)^{i + j}  \det(A_{\hat{j}, \hat{i}})}{\det(A)}} 
  \end{align}
  Dove $\hat{j}, \hat{ i}$ vuol dire con la riga $j$ e la colonna $i$ escluse.  
\end{theorem}

\begin{observation}
  Se $a_{i, j} \in \mathbb{Z}$ e $\det(A) = \pm 1$, allora $A^{-1}$ ha coefficienti
  reali in $\mathbb{Z}$. 
\end{observation}

\begin{example}[Utilizzo dell'eliminazione di Gauss per ottenere l'inversa]
  \begin{gather*}
    A = \left( \begin{tabular}{c c c}
        1 & 0 & 2 \\
        0 & 2 & 1 \\
        -1 & 0 & 0
    \end{tabular} \right) 
  \end{gather*}
  Esempi di coefficienti: il determinante di A conviene calcolarlo
  sulla terza riga, quindi viene $\det(A) = 4$, togliendo la priam riga e la
  prima colonna viene:
  \begin{gather*}
    A = \left( \begin{tabular}{c c}
        2 & 1 \\
        0 & 0
    \end{tabular} \right)
  \end{gather*}
  Mettendo nella formula viene $0$ mentre l'altra matrice nella
  formula è:
  \begin{gather*}
    A = \left( \begin{tabular}{c c}
        0 & 2 \\
        2 & 1
    \end{tabular} \right)
  \end{gather*}
  che nella formula è -1. Un metodo più efficace è quello di Gauss ossia 
  affiancare ogni colonna della matrice invertita alla matrice normale ed eseguire
  eliminazioni fino ad ottenere nella matrice di partenza la matrice identità; a quel punto
  quello che rimane nella colonna affiancata è esattamente la colonna della matrice invertita.
  \begin{gather*}
    A = \left( \begin{tabular}{c c c | c}
        1 & 0 & 2  & 1\\
        0 & 2 & 1 & 0\\
        -1 & 0 & 0 & 0
    \end{tabular} \right)
  \end{gather*}
  Parametrizzando si può ottenere tutte le colonne in un colpo solo 
  mettendo accanto nel sistema la matrice identità:
  \begin{gather*}
    A = \left( \begin{tabular}{c c c | c c c}
        1 & 0 & 2  & 1 & 0 & 0\\
        0 & 2 & 1 & 0 & 1 & 0\\
        -1 & 0 & 0 & 0 & 0 & 1
    \end{tabular} \right)
  \end{gather*}
  Con le eliminazioni di Gauss si può ottenere:
  \begin{gather*}
      \begin{pmatrix}
          0 & 0 & -1 \\
          -\frac{1}{4} & \frac{1}{2} & -\frac{1}{4} \\
          \frac{1}{2} & 0 & \frac{1}{2}
      \end{pmatrix}
  \end{gather*}
\end{example}

La matrice che ho trovato a destra della barra verticale è proprio l'inversa
poiché l'identità è traslata sulla sinistra e a destra si ottiene l'inversa
(si può anche fare la cosa inversa, ossia mettere a destra come termini noti
le colonne di A e ottenere l'inversa sulla sinistra).


\chapter{Sottospazi affini}
\section{Definizione e prime proprietà}
\begin{definition}[Sottospazi affini]
    Sia $\mathbb{K}$ un campo, e sia $n \in \mathbb{N} - \{0\}$. Sia
    $S$ un sottoinsieme di $\mathbb{K}^{n}$. Si dice che $S$ è un sottospazio
    affine se $\exists v \in \mathbb{K}^{n} \wedge \exists Z$ sottospazio vettoriale di $\mathbb{K}^{n}$ tale che:
    \begin{align}
        S = v + Z.
    \end{align}   
    Dove 
    \begin{gather*}
        v + Z = \{v + z : z \in Z\}
    \end{gather*} 
    Un sottospazio affine è un sottospazio traslato
    rispetto al sottospazio contenuto in uno spazio vettoriale qualsiasi. 
\end{definition}
\begin{definition}[Direzione del sottospazio]
    Il sottospazio dal quale si ricava il sottospazio affine
    $S$ si chiama \textbf{direzione} di $S$, o si scrive anche:
    $Z = dir(S)$ ed è univocamente determinata.
\end{definition}
\begin{observation}[La direzione è univoca]
    La direzione è univocamente determinata
\end{observation}
\begin{proof}
    Sia $S$ un sottospazio affine con direzione $Z$  quindi:
    \begin{gather*}
        S = v + Z, \quad  S = v' + Z'
    \end{gather*}
    Con $v, v' \in \mathbb{K}^{n}$. Dato che $z \in Z$ e $z' \in Z'$, con $Z, Z'$ sottospazi
    vettoriali di $\mathbb{K}^{n}$, si ha che
    \begin{gather*}
        \{v + z : z \in Z\} = \{v' + z' : z' \in Z'\}
    \end{gather*} 
    Deve esistere un vettore $z' \in Z'$ tale per cui
    $v = v' + z'$, dunque si ha che $Z' \ni v - v' = z' \in Z'$. Deve anche $\exists z \in Z : v' = v + z$ e quindi
    $Z \ni v' - v = z \in Z$. Ho dimostrato che $v - v'$ ed il suo opposto $\in Z, Z'$
    allora devo dimostrare che $Z \subset Z' \wedge Z' \subset Z$. \\ 
    Preso $t \in Z$, io so che $ v + t \in v + Z$ e deve risultare che $v + t \in v' + Z'$ e che
    quindi $\exists t' \in Z'$ tale che $v + t = v' + t'$
    Allora posso scrivere:
    \begin{gather*}
        t = v' - v + t'
    \end{gather*} 
    Allora entrambi i pezzi del secondo membro sono in $Z'$ poiché è uno spazio vettoriale, dunque $t \in Z'$. Allora è dimostrato che $Z \subset Z'$, e si dimostra
    analogamente anche  $Z' \subset Z$, dunque $Z = Z'$. 
\end{proof}
\begin{definition}[Dimensione del sottospazio affine]
    La dimensione di un sottospazio affine è proprio la dimensione
    della direzione del sottospazio affine.
\end{definition}

\begin{example}
  \begin{gather*}
    r = \left\{ \begin{pmatrix} 1 \\
    2 \end{pmatrix} + t \begin{pmatrix} 1 \\
    1\end{pmatrix} : t \in \mathbb{R} \right\}
\end{gather*}
Questo sottospazio sarà proprio la retta che passa dal punto
$(1, 2)$ e tutti i punti della retta sono ottenuti sommando come multiplo di t
che diventa allora la sua direzione. 
\end{example}

\begin{definition}[Nomi dei sottospazi affini]
    Un sottospazio affine di dimensione $1$ è una retta, di dimensione $2$ 
    è piano, di dimensione $n - 1$ si dice iperpiano. 
\end{definition}

\begin{theorem}[Teorema di Struttura coi sottospazi affini]
  Il teorema di struttura può essere rivisto con i sottospazi affini: infatti,
  sia $A \in M(m \times n, \mathbb{K})$ una matrice e $b \in \mathbb{K}^{m}$ un
  vettore. Si definisce l'insieme delle soluzioni non nulle del sistema $A|b$ come
  il sottospazio affine:
  \begin{align}
      S := \{x \in \mathbb{K}^{n} : Ax = b \}
  \end{align}
  E l'insieme delle soluzioni nulle come il sottospazio vettoriale
  \begin{align}
      S_0 := \{x \in \mathbb{K}^{n} : Ax = 0 \} = \ker(f_A)
  \end{align}
  Se $S$ è non vuoto, chiamo un suo elemento come $\overline{x}$ e dunque 
  \begin{align}
      S = \overline{x} + S_0 
  \end{align}
  Per cui
  \begin{gather*}
      dir(S) = S_0
  \end{gather*}
  La dimensione di $S_0$ (che è dunque la dimensione di $S$) è
  esattamente $n - rk(A)$. 
\end{theorem}

\begin{example}[Insieme delle soluzioni come sottospazio affine]
  Sia 
  \begin{gather*}
      S = \left\{x \in \mathbb{R}^{4} : \begin{array}{l}
          x_1 - x_2 + x_3 = 1 \\
          2x_1 -3x_4 = 0
      \end{array}\right\}
  \end{gather*} 
  E sia
  \begin{gather*}
      A = \begin{pmatrix}
          1 & -1 & 1 & 0 \\
          2 & 0 & 0 & -3
      \end{pmatrix} \qquad b = \begin{pmatrix} 1 \\
      0 \end{pmatrix} 
  \end{gather*}
  Si ha che $rk(A) = rk(A|b) = 2$ e dunque $S \neq \emptyset$. 
  S è dunque un sottospazio affine di direzione
  \begin{gather*}
      S_0 = \left\{x \in \mathbb{R}^{4} : \begin{array}{l}
          x_1 - x_2 + x_3 = 1 \\
          2x_1 -3x_4 = 0
      \end{array}\right\}
  \end{gather*}
  Da cui si ottiene che
  \begin{gather*}
      \dim S = \dim S_0 = n - rk(A) = 2 
  \end{gather*}
\end{example}

\begin{lemma}[Sottospazi affini come luogo degli zeri]
    Sia $\mathbb{K}$ un campo, e sia $n \in \mathbb{N} - \{0\}$.
    \begin{enumerate}
        \item Sia $Z$ un sottospazio vettoriale
    di $\mathbb{K}^{n}$ di dimensione $r$, allora $\exists A \in M((n - r) \times n, \mathbb{K})$
    di rango $n - r$ tale che
    \begin{align}
        Z = \{x \in \mathbb{K}^{n} : Ax = 0 \}
    \end{align}
    \item Sia S un sottospazio affine di $\mathbb{K}^{n}$ di dimensione $r$
    allora $\exists A \in M((n - r) \times n, \mathbb{K})$ di rango $n -r$
    e $\exists b \in \mathbb{K}^{n - r}$ tale che:
    \begin{align}
        S = \{x \in \mathbb{K}^{n} : Ax = b \}
    \end{align} 
    \end{enumerate}
        Questo vuol dire che un sottospazio affine di dimensione $r$ è
    individuato da un sistema di $n - r$ equazioni, ossia il sottospazio
    affine è il luogo degli zeri di certi vettori $x$; dare più equazioni 
    di $n - r$ non cambia il sottospazio affine individuato.
\end{lemma}
\begin{proof}
    1.) Sia $\{z_1, \dots, z_r\}$ una base del sottospazio $Z$ in $\mathbb{K}^{n} $.
    Sia $A$ una matrice che ha come righe le trasposte della base del sottospazio
    $Z$ 
    \begin{gather*}
      A = \begin{pmatrix} ^{t}z_1 \\
      \vdots \\
      ^{t}z_{r}
    \end{pmatrix}     
    \end{gather*}
    si considera allora l'insieme
    \begin{gather*}
        L = \left\{x \in \mathbb{K}^{n} : (z_i, x)  = 0\quad \forall i \in \{1, \dots, r\}\right\} = \{x \in \mathbb{K}^{n} : Ax = 0\}
    \end{gather*}
    Ossia $L$ è il luogo degli zeri del sistema lineare omogeneo alla matrice $A$, la quale ha rango
    $r$. Pertanto $\dim(L) = n - r$, quindi $\left<l_1, \dots, l_{n - r}\right>$ è una base
    di $L$. Sia $A'$ la matrice che ha come righe le trasposte dei vettori della base di $L$, il rango
    di questa matrice è $n - r$. Sia ora  
    \begin{gather*}
        U = \{x \in \mathbb{K}^{n} : A'x = 0 \} = \left\{x \in \mathbb{K}^{n} : (l_i, x) = 0 \quad \forall i \in \{1, \dots, n - r\}\right\} 
    \end{gather*}
    Dato che si è definito $L$ come l'insieme costituito dai vettori $\perp$ a $Z$ per il prodotto
    scalare standard, $\dim(U) = n -  (n - r) = r$ poiché la dimensione di $Z$ è $r$. Dunque i vettori
    $z_j$, con $j \in \{1, \dots, r\}$, soddisfano la condizione per appartenere ad $U$. $Z$ e $U$ sono 
    uguali perché $Z \subset U$ e hanno dimensione uguale, allora $Z = U$.  \\
    2.) Sia $S$ un sottospazio affine di $\mathbb{K}^{n}$ con $S = v + Z$, di 
    dimensione $r$ con $v \in \mathbb{K}^n$ e $Z$ sottospazio di $\mathbb{K}^{n}$. Sia 
    \begin{gather*}
        S = \{x \in \mathbb{K}^{n} : \exists z \in Z : x = v + z\}  = \{x \in \mathbb{K}^{n} : x - v \in Z\} =
    \end{gather*} 
    \fbox{
    \begin{minipage}{\linewidth}
    Per la prima parte della proposizione si ha che $\exists A \in M((n - r) \times n, \mathbb{K})$ di rango
    $n- r$ tale che
    \begin{gather*}
        Z = \left\{y : Ay = 0\right\}
    \end{gather*}

    \end{minipage}
    }
    \begin{gather*}
        =  \{x \in \mathbb{K}^{n} : A(x - v) = 0\} = \{x \in \mathbb{K}^{n} : Ax = Av\}
    \end{gather*}
    Posto $b = Av$, si vede che $S$ è l'insieme delle soluzioni di un sistema lineare. 
\end{proof}

\begin{observation}[Considerazioni sui sottospazi affini]
  Posso considerare i sottospazi affini come le soluzioni dei sistemi
  lineari delle matrici associate a quel particolare sistema lineare. Da
  questa definizione posso, considerata la nomenclatura dei sottospazi affini,
  definire le equazioni parametriche delle rette e dei piani su $\mathbb{R}^3$.
\end{observation}

\begin{corollary}[Definizione formali di retta e piano]
    Sia $s$ una retta in $\mathbb{R}^{3}$ e quindi un sottospazio affine
    di dimensione $1$, c'è una matrice tale che $s$ è l'insieme
    delle soluzione del sistema lineare $Ax = b$. Sul teorema di prima, preso $n = 3$ e
    $r = 1$, allora $\exists A \in M(2 \times3, \mathbb{R})$ di rango $2$
    ed $\exists b \in \mathbb{R}^{2}$ tale che
    \begin{gather*}
        s = \left\{x : Ax = b\right\} \qquad
        \exists \alpha, \beta, \gamma, \alpha', \beta', \gamma', \delta, \delta' : rk\left(\begin{tabular}{c c c}
            $\alpha$ & $\beta$ & $\gamma$ \\
            $\alpha'$ & $\beta'$ & $\gamma'$
        \end{tabular}\right) = 2 \ \Longrightarrow \  \\
        s = \left\{x \in \mathbb{R}^{3} : \begin{array}{l}
            \alpha x_1 + \beta x_2 + \gamma x_3 = \delta \\
            \alpha' x_1 + \beta' x_2 + \gamma' x_3 = \delta' \\
        \end{array}\right\}
    \end{gather*} 
    Sia $\Pi$ un piano di $\mathbb{R}^{3}$, ossia un sottospazio affine di dimensione $2$, quindi $\exists A \in M(1 \times 3, \mathbb{R})$
    di rango 1 e $\exists b \in \mathbb{R}^{1} :$
    \begin{gather*}
        \Pi = \{x \in \mathbb{R}^{3} : Ax = b\} 
      \end{gather*}  
    $\exists \alpha, \beta, \gamma, \delta$ non tutti nulli 
    tali che mi diano una equazione sola
    \begin{gather*}
        \Pi = \{x \in \mathbb{R}^{3} : \alpha x_1 +  \beta x_2 + \gamma x_3 = \delta\}
    \end{gather*}
    Per essere un piano, almeno uno dei tre deve essere diverso da zero. 
\end{corollary} 

\begin{definition}[Piano]
  \begin{align}
      \pi = \left\{ \left( \begin{array}{l}
        x \\
        y\\
        z
      \end{array} \right) \in  \mathbb{R}^{3}  : ax + by + cz = d \right\}
  \end{align}
\end{definition}
\begin{definition}[Retta]
  \begin{align}
      r = \left\{ \left( \begin{array}{l}
        x\\
        y\\
        z
      \end{array} \right) \in \mathbb{R}^{3} : \begin{array}{l}
          ax + by +cz = d\\
          a'x + b'y +c'z = d'
      \end{array}  \right\}
  \end{align}
\end{definition}

\section{Altre proprietà degli affini}
\begin{definition}[Parallelismo tra sottospazi affini]
    Due sottospazi affini di un campo $ \mathbb{K}^{n}$ si dicono \textbf{paralleli}
    se uno dei due ha la direzione contenuta nella direzione dell'altro.
\end{definition}

\begin{definition}[Sottospazio perpendicolare]
    Sia Z un sottospazio di $\mathbb{R}^{n}$. Definiamo allora
    \begin{align}
        Z^{\perp} = \{v \in \mathbb{R}^{n} : (v, z) = 0, \forall z \in Z\} 
    \end{align} 
    il sottospazio $\perp $ al sottospazio dato e dove $(v, z)$ denota il prodotto scalare standard 
    tra i vettori  $v, z$. In pratica il sottospazio perpendicolare ad un altro sottospazio
    è quel sottospazio di $\mathbb{R}^{n}$ che contiene tutti i vettori
    ortogonali ai vettori dello spazio $Z$ considerato. 
\end{definition}

\begin{lemma}[Proprietà dello spazio perpendicolare]
    Sia $Z$ un sottospazio vettoriale di $\mathbb{R}^{n}$. 
    \begin{enumerate}
      \item $Z^\perp$ è un sottospazio vettoriale di $\mathbb{R}^{n}$.
        \item \begin{align}
            \dim(Z^{\perp} ) = n - \dim(Z);
        \end{align}
        \item   \begin{align}
            \mathbb{R}^{n} = Z \oplus  Z^{\perp}  
        \end{align}
        \item  \begin{align}
            (Z^{\perp})^{\perp}   = Z;
        \end{align}
        \item Se $W$ è un sottospazio vettoriale di $\mathbb{R}^{n}$, allora \begin{align}
            W \subset Z \ \Longrightarrow \  Z^{\perp} \subset W^{\perp}   
        \end{align}
    \end{enumerate}   
\end{lemma}
\begin{proof}
    1.) $Z^{\perp}$ è sottospazio vettoriale in quanto ha la chiusura per
    la somma e la chiusura per il prodotto per uno scalare. Intanto $0_{\mathbb{R}^{n}} \in Z^{\perp}$,
    e siano $v, w \in Z^{\perp}$, siano $\lambda, \mu \in \mathbb{R}$, allora sia
    $z \in Z$, voglio dimostrare che la combinazione lineare $\lambda v + \mu w \in Z^{\perp}$.
    \begin{gather*}
        (\lambda v + \mu w, z) = \lambda(v, z) + \mu(w, z) = \lambda \cdot 0 + \mu \cdot 0 = 0
    \end{gather*}
    Dunque $(\lambda v + \mu w, z) = 0 \ \forall z \in Z$. Dunque $Z^{\perp}$ è un sottospazio
    vettoriale poiché è chiuso per la somma e per il prodotto per scalari e contiene lo zero 
    del campo di riferimento. \\
    2.)  Sia $k = \dim(Z)$ e sia $\{z_1, \dots, z_k\}$ una base di $Z$; allora 
    \begin{gather*}
        Z^{\perp} = \{v \in \mathbb{R}^{n} : (v, z) = 0 \ \forall z \in Z\} = \{v \in \mathbb{R}^{n}: (v, z_i) = 0 \ \forall i \in \{1, \dots, k\}\}
    \end{gather*}
    La seconda uguaglianza vale perché qualsiasi $z \in Z$ è la combinazione lineare
    della base di $Z$:
    \begin{gather*}
        \left(v, \sum \lambda_i z_i\right) \ \Longrightarrow \ \sum \lambda_i\left(v, z_i\right) = 0  \ \Longrightarrow \ Z^{\perp} = \left\{x \in \mathbb{R}^{n} : \begin{pmatrix}
            ^{t} z_1 \\
            \vdots \\
            ^{t}z_k
        \end{pmatrix}x = 0\right\}
    \end{gather*}
    E quindi $Z^{\perp}$ è l'insieme delle soluzioni del sistema
    lineare omogeneo la cui matrice incompleta ha le trasposte 
    (perché i vettori di $Z^{\perp}$ sono tutti vettori riga) 
    di $z_1, \dots, z_k$ e tale matrice ha formato $k \times n$ e quindi,
    avendo rango $k$, lo spazio $Z^{\perp}$ è esattamente il $\ker$ della matrice, dunque 
    \begin{gather*}
        \dim(Z^{\perp} ) = n - k 
    \end{gather*} 
    3.) Si dimostra
    \begin{gather*}
        Z \cap Z^{\perp} = \emptyset 
    \end{gather*}
    perché $v \in Z \cap Z^{\perp}$ allora $(v, v) = 0 \ \Longrightarrow \  v = 0$.
    Per la formula di Grassmann 
    \begin{gather*}
        \dim(Z \oplus Z^{\perp} ) = \dim(Z) + \dim(Z^{\perp} ) = k + n - k = n \ \Longrightarrow \ \mathbb{R}^{n} = Z \oplus Z^{\perp} 
    \end{gather*}
    4.) Dati $Z^\perp$ e $(Z^{\perp})^{\perp}$:
    \begin{gather*}
        Z^{\perp} = \{v \in \mathbb{R}^{n} : (v, z) = 0 \ \forall z \in Z\}  \qquad (Z^{\perp})^{\perp } = \{v \in \mathbb{R}^{n} : (v, z) = 0\  \forall z \in Z^{\perp}  \} 
    \end{gather*}
    Per far sì che sia dimostrata, si deve considerare
    \begin{gather*}
        \dim((Z^{\perp})^{\perp}) = n - \dim(Z^{\perp}) = n - (n - \dim(Z^{\perp})) = \dim(Z)
    \end{gather*}
    Inoltre $Z \subset (Z^{\perp})^{\perp}$ infatti $(z, w) = 0$ per $z \in Z$ e $w \in Z^{\perp}$.
    allora $\forall z \in Z$ appartiene anche a $(Z^{\perp})^{\perp}$ e dunque $Z = (Z^\perp)^{\perp}$. \\
    5.)
    Dato che $W \subset  Z$ e $W \subset \mathbb{R}^{n}$:
    \begin{gather*}
        W \subset Z \ \Longrightarrow  \ \dim(W) < \dim(Z)
    \end{gather*}
    Dato che, per la seconda proprietà
    \begin{gather*}
        \dim(W^{\perp} ) = n - \dim(W), \qquad \dim(Z^{\perp} ) = n - \dim(Z) 
    \end{gather*}
    Allora si ottiene 
    \begin{gather*}
        n - \dim(W^{\perp} ) < n - \dim(Z^{\perp} ) \ \Longrightarrow \ \dim(Z^{\perp} ) < \dim(W^{\perp} ) \ \Longrightarrow  \ Z^{\perp} \subset  W^{\perp}   
    \end{gather*}
\end{proof}

\begin{definition}[Perpendicolarità tra spazi vettoriali]
    Siano $S_1$ e $S_2$ due sottospazi affini di $\mathbb{R}^{n}$ con direzioni
    $Z_1$ e $Z_2$, dico che $S_1$ e $S_2$ sono perpendicolari se valgono le seguenti condizioni
    \begin{align}
        Z_1 \subset Z_2^{\perp} \qquad se \qquad \dim(Z_1) + \dim(Z_2) \leq n \\
        Z_1^{\perp}  \subset Z_2 \qquad se \qquad \dim(Z_1) + \dim(Z_2) \geq n
    \end{align}
    Che derivano direttamente dalla formula di Grassmann e dalle proprietà degli spazi 
    perpendicolari.
\end{definition}

\begin{observation}[Conseguenze delle proprietà degli spazi perpendicolari]
    Supponendo che $\dim(Z_1) + \dim(Z_2) = n$ e se $Z_1 \subset Z_2^{\perp}$ allora
    \begin{align}
        \dim(Z_2^{\perp} ) = n -&\dim(Z_2)  =\dim(Z_1)\\
        Z_1 = Z_2^{\perp} \ \Longrightarrow \ Z_1^{\perp} &= (Z_2^{\perp})^{\perp} = Z_2 \ \Longrightarrow \  Z_1^{\perp} \subset Z_2  
    \end{align}
    Quindi se $\dim(Z_1) + \dim(Z_2) = n$ se vale una delle
    due vale anche l'altra.
\end{observation}

\begin{example}[Esempio di perpendicolarità tra spazi vettoriali]
Siano $\Pi_1 = \left< e_1, e_2 \right>$ e $\Pi_2 = \left< e_2, e_3 \right> \in \mathbb{R}^{3}$
sono vettoriali e quindi sono anche affini, essendo che la loro somma però è maggiore
della dimensione di $\mathbb{R}$, allora devo applicare le condizioni e quindi ottengo che
\begin{gather*}
    Z_1^{\perp} = \{x \in \mathbb{R}^{3} : (x, v) = 0 \forall v \in <e_1, e_2>\} = <e_3>  
\end{gather*}
Allora si ottiene che:
\begin{gather*}
    Z_1^{\perp} \subset Z_2 
\end{gather*}
Quindi sono perpendicolari.
\end{example}

\begin{definition}[Proiezione ortogonale di un punto su di un sottospazio affine]
    Sia $S$ un sottospazio affine di $\mathbb{R}^{n}$ e sia $P$ un punto che
    $\in \mathbb{R}^{n}$, definisco la proiezione ortogonale di P su di S come
    l'unico punto di intersezione tra S ed il sottospazio affine 
    \begin{align}
        P + (dir(S))^{\perp} 
    \end{align}  
\end{definition}

\begin{observation}[L'intersezione tra proiezione ortogonale e un punto è unica]
  Lo spazio vettoriale
    \begin{align}
        S \cap (P + dir(S)^{\perp} ) 
    \end{align}
    è formato da un solo punto.
\end{observation}
\begin{proof}
    Posto $S = Q + Z$, dove $Q$ è un punto in $\mathbb{R}^{n}$ e $Z$ 
    un sottospazio vettoriale di $\mathbb{R}^{n}$ allora si considera il sottospazio affine
    \begin{gather*}
        S' = P + Z^{\perp} 
    \end{gather*}  
    si vuole dimostrare che si intersecano in un punto solo
    \begin{gather*}
        S \cap S'
    \end{gather*}
    Occorre dimostrare che $z \in Z, w \in Z^{\perp}$ esistano e
    siano unici 
    \begin{gather*}
        Q + z = P + w.
    \end{gather*} 
    ossia
    \begin{gather*}
        Q - P = \underset{\in Z^{\perp}}{\underbar{w}} -\underset{\in Z}{\underbar{z}}
    \end{gather*}
    Il primo membro $\in \mathbb{R}^{n}$, per la seconda proprietà
    dei sottospazi affini perpendicolari, anche il secondo membro è $\in \mathbb{R}^{n}$.Quindi ogni elemento è possibile scriverlo come somma diretta di elementi
    unici.    
\end{proof}

\begin{definition}[Distanza di due punti]
    Siano $P$ e $Q$ due punti di $\mathbb{R}^{n}$, definisco la loro
    distanza come la loro norma, ossia il numero reale definito come
    \begin{align}
        d(P, Q) = |P - Q|
    \end{align} 
\end{definition}

\begin{definition}[Distanza tra due spazi vettoriali]
    Siano $A, B \subset \mathbb{R}^{n}$, si definisce la distanza tra due spazi vettoriali
    come :
    \begin{align}
        d(A, B) = \inf\{d(P, Q) : P \in A, Q \in B\}
    \end{align}
    Ossia l'estremo inferiore di quella distanza.
\end{definition}

\begin{example}[Calcolare la distanza tra due spazi vettoriali]
  Sia
  \begin{gather*}
      P = \begin{pmatrix} -3 \\
      2 \\
      0 \end{pmatrix} \qquad r \subset  \mathbb{R}^{3}, r: \left\{\begin{array}{l}
          x + y - z = 7 \\
          2x -- z = 0
      \end{array}\right. 
  \end{gather*}
  La distanza da $P$ si calcola come $d(P, Q)$ con $Q \in R, Q \subset r$ e quindi
  come l'intersezione tra $r$ con il piano perpendicolare ad $r$ e passante per $P$
  \begin{gather*}
      dir(r) = \left< \begin{pmatrix} 1\\
      1\\
      2 \end{pmatrix}  \right> \Rightarrow x + y + 2z = d 
  \end{gather*}
  Si ottiene $d$ specificando l'appartenenza di $P$ e quindi
  \begin{gather*}
    -3 + 2 = d \Rightarrow x + y +2z = -1
  \end{gather*}
  Allora risolvendo il sistema
  \begin{gather*}
      \left\{\begin{array}{l}
          x + y -z = 7 \\
          2x - z = 0 \\
          x + y + 2z = -1
      \end{array}\right. \Rightarrow Q = \begin{pmatrix} -\frac{4}{3} \\
      \frac{17}{3}\\
      -\frac{8}{3} \end{pmatrix} 
  \end{gather*}
  E quindi la distanza è $\sqrt{\frac{70}{3}}$. 
\end{example}

\section{Mutua posizione di rette e piani nello spazio $\mathbb{R}^{3}$}
\subsection{Posizione di due piani}
Presi due piani $\pi, \pi' \in \mathbb{R}^{3}$, esistono allora $a, b, c, d \in \mathbb{R}$
e $a', b', c', d' \in \mathbb{R}$. Considerata la matrice formata dalle equazioni dei
due piani:
\begin{gather*}
    \left( \begin{array}{c c c | c}
      a & b & c & d \\
      a' & b' & c'& d'
    \end{array} \right) 
\end{gather*}
L'insieme delle soluzioni è dunque $\pi \cap \pi'$. Si osserva allora che
essendo 
\begin{gather*}
    1 \leq rk(A) \leq rk(A | D) \leq 2
\end{gather*}
Si distinguono allora tre casi distinti:
\begin{align*}
    &rk(A) = rk(A | D) = 1 \qquad& &\text{I piani coincidono}\\
    &rk(A) = 1, \ rk(A | D) = 2 \qquad& &\text{I piani sono paralleli ma distinti}\\
    &rk(A) = rk(A | D) = 2 \qquad& &\text{I piani si intersecano in una retta}
\end{align*}   

\subsection{Posizione di un piano ed una retta nello spazio}
Sia $\pi$ un piano e $r$ una retta in $\mathbb{R}^{3}$. Esistono allora $a, b, c, d \in \mathbb{R}$ e considerata la
matrice:
\begin{gather*}
    \left( \begin{array}{c c c | c}
      a & b & c & d \\
      a' &  b' & c' & d'\\
      a'' & b'' & c'' & d''
    \end{array} \right) 
\end{gather*}
Allora l'insieme delle soluzioni è chiaramente  $\pi \cap r$ e quindi 
si distinguono i casi:
\begin{align*}
    &rk(A) = rk(A | D) = 2 \qquad& &\text{La retta è contenuta nel piano} \\
    &rk(A) = 2, \ rk(A | D) = 3 \qquad& &\text{La retta è parallela al piano ma distinta}\\
    &rk(A) = rk(A | D) = 3 \qquad& &\text{La retta ed il piano si intersecano in un punto}
\end{align*}

\subsection{Posizione reciproca di due rette nello spazio}
Nel caso della posizione tra due rette nello spazio si
ripetono i procedimenti per gli altri casi ottenendo i seguenti casi:
\begin{align*}
  &rk(A) = rk(A | D) = 2 \qquad& &\text{Le dure rette coincidono}\\
  &rk(A) = 2, \ rk(A | D) = 3 \qquad& &\text{Le due rette sono parallele ma distinte}\\
  &rk(A) = rk(A | D) = 3 \qquad& &\text{Le due rette si intersecano in un punto}\\
  &rk(A) = 3, \ rk(A | D) = 4 \qquad& &\text{Le dure rette sono sghembe}\\
\end{align*}

\chapter{Autovalori, diagonalizzabilità e triangolabilità}
\section{Autovettori e autovalori}
\begin{definition}[Autovettore, autovalore, spettro]
  Sia $V$ uno spazio vettoriale su di un campo $\mathbb{K}$ e sia $f : V \to V$ un'applicazione
  lineare. Preso un vettore $v \in V$, esso è un \textbf{autovettore} di $f$ se è diverso 
  da $0_V$ e se $\exists \lambda \in \mathbb{K}:$
  \begin{align}
      f(v) = \lambda v
  \end{align}
  Si dice che $\lambda$ è un \textbf{autovalore} di $f$ e l'insieme degli
  autovalori si chiama \textbf{spettro}. Autovettori, autovalori e spettro 
  di una matrice quadrata $A \in M(n \times n, \mathbb{K})$ sono per definizione
  autovalori, autovettori e spettro di $f_A : \mathbb{K}^{n} \to \mathbb{K}^{n}$. Quindi $x \in \mathbb{K}^{n}$ 
  è un autovettore di $A$ con autovalore $\lambda$ se $x \neq  0_K$
  e se
  \begin{align}
      Ax = \lambda x
  \end{align}   
  Quindi se $V$ è uno spazio vettoriale su $\mathbb{K}$ di dimensione finita $n$ e $B$
  è una base qualsiasi di $V$, allora $v$ è un autovettore per $f$ se e solo
  se, detto $x$ il vettore coordinate di $v$ rispetto a $B$, esso è autovettore
  di $M_{B, B}(f)$.
\end{definition}

\begin{example}[Autovettori ed autovalori]
  Sia $f = f_A : \mathbb{R}^{3} \to \mathbb{R}^{3}$ dove
  \begin{gather*}
      A = \begin{pmatrix}
          1 & 2 & 0 \\
          2 & 1 & 1 \\
          0 & 0 & 1
      \end{pmatrix}, \qquad v = \begin{pmatrix} 1 \\
      1\\
      0 \end{pmatrix} 
  \end{gather*}  
  $v$ allora è autovettore di $f$ con autovalore $\lambda = 3$, infatti:
  \begin{gather*}
      \begin{pmatrix}
          1 & 2 & 0 \\
          2 & 1 & 1 \\
          0 & 0 & 1
      \end{pmatrix} \begin{pmatrix} 1 \\
      1\\
      0 \end{pmatrix} = \begin{pmatrix} 3 \\
      3\\
      0 \end{pmatrix} 
  \end{gather*}
\end{example}

\begin{lemma}[Combinazioni lineari di autovettori sono anch'esse autovettori]
  Sia $f: V \to V$ un'applicazione lineare e siano $v$ e $w$ due autovettori
  con autovalori $\lambda$, allora ogni combinazione lineare di questi 
  due vettori  è anch'essa un autovettore con autovalore $\lambda$.
\end{lemma}
\begin{proof}
  \begin{gather*}
      f(v) = \lambda v, \qquad f(w) = \lambda w
  \end{gather*}
  Siano allora $r, s \in \mathbb{K}$ tale che $rv + sw \neq  0$ allora 
  per la linearità delle applicazioni lineari:
  \begin{gather*}
      f(rv + sw) = f(rv) + f(sw) = \lambda(rv + sw)
  \end{gather*}
\end{proof}

\begin{observation}[Elementi del kernel e autovalori]
  Sia $f : V \to V$ un'applicazione lineare con $\ker (f) \neq 0$, allora 
  ogni elemento non nullo del kernel è un autovettore di $f$ con autovalore 0: 
  \begin{align}
      v \in \ker(f) - \{0\} \qquad f(v) = 0_V = 0_Kv
  \end{align}
  Tutti gli autovettori con autovalori zero sono gli elementi non nulli del kernel.
\end{observation}

\begin{definition}[Polinomio caratteristico]
  Il \textbf{polinomio caratteristico} di una matrice $A \in M(n \times n, \mathbb{K})$ è il polinomio
  definito come 
  \begin{align}
      P_A(t) = \det(A - tI_n)
  \end{align}
\end{definition}

\begin{example}[Esempio del polinomio caratteristico]
  \begin{gather*}
      B = \left( \begin{array}{c c c c}
          1 & 2 & 0 & 0 \\
          0 & 3 & 0 & 0 \\
          0 & 0 & 4 & 5 \\
          0 & 0 & 5 & 0            
      \end{array} \right)
  \end{gather*}
  Il suo polinomio caratteristico è proprio:
  \begin{gather*}
      P_B(t) = \det\left( \begin{array}{c c c c}
          1 - t & 2 & 0 & 0 \\
          0 & 3 - t & 0 & 0 \\
          0 & 0 & 4 - t & 5 \\
          0 & 0 & 5 & -t            
      \end{array} \right) = ((4 - t)(-t) - (25))(3- t)(1 - t)
  \end{gather*}
\end{example}

\begin{definition}[Polinomio dell'applicazione lineare]
  Sia $V$ uno spazio vettoriale di dimensioni finite e sia
  la funzione $f : V \to V$ un'applicazione lineare. Definisco
  allora il polinomio caratteristico dell'applicazione lineare come:
  \begin{align}
      P_f(t) = \det(M_{B, B} (f) - tI_n)  
  \end{align}
\end{definition}

\begin{definition}[Matrici simili]
  Siano $A$ e $B$ due matrici quadrate a coefficienti in $\mathbb{K}$, si dicono \textbf{simili}
  se esiste una matrice invertibile $C \in GL(n, \mathbb{K})$ tale che:
  \begin{align}
      A = C^{-1} BC 
  \end{align}
\end{definition}

\begin{lemma}[Indipendenza del polinomio caratteristico dalla base]
  La definizione del polinomio caratteristico della funzione
  non dipende dalla scelta della base $B$ di V. Matrici simili
  hanno lo stesso polinomio caratteristico.
\end{lemma}
\begin{proof}
  Dimostriamo allora che matrici simili hanno lo stesso polinomio
  caratteristico, per la definizione di matrici inverse ed il determinante dell'inversa:
  \begin{gather*}
      \det(A) = \det(C^{-1} BC) = \det(C^{-1} ) \cdot  \det(B) \cdot  \det(C) = \det(B)
  \end{gather*}
  Allora il polinomio caratteristico di $A$ è dato da:
  \begin{gather*}
      P_A(t) = \det(A - tI_n)  = \det(C^{-1} BC - tI_n) = \det(C^{-1} (B - tI_n)C) = \\
      \det(B - tI_n) = P_B(t)
  \end{gather*}
  Prese $B$ e $B'$ due basi di $V$, si può utilizzare la formula di cambiamento di base:
  \begin{gather*}
      M_{B', B'}(f) = M_{B', B}(I_V) M_{B, B}(f) M_{B, B'}(I_V) = \\
      M_{B, B'}(I_V)^{-1} M_{B, B}(f)M_{B, B'}(I_V) 
  \end{gather*}
  Allora sono simili e quindi
  \begin{gather*}
    M_{B', B'}(f) = M_{B, B}(f) = 
      P_{M_{B, B}}(f) = P_{M_{B', B'}}(f)
  \end{gather*}
  E quindi matrici simili hanno lo stesso polinomio caratteristico che è indipendente
  dalla base scelta.
\end{proof}

\begin{theorem}[Relazione tra autovalori e radici del polinomio caratteristico]
  Sia $V$ uno spazio vettoriale su di un campo $\mathbb{K}$, di
  dimensione finita $n$ e sia $f : V \to V$  una applicazione lineare. Sia $\lambda \in \mathbb{K}$.
  Affermo allora che $\lambda$ è radice del polinomio caratteristico se
  e solo se $\lambda$ è autovalore di $f$. 
\end{theorem}
\begin{proof}
  $\ \Longrightarrow \ $Sia $B$ una base ordinata di $V$, allora se $\lambda$ è radice del
  polinomio caratteristico
  \begin{gather*}
      P_f(\lambda) = 0 \ \Longrightarrow \ \det(M_{B, B}(f) - \lambda I_V) = 0  \ \Longrightarrow \ rk(M_{B, B}(f - \lambda I_V)) < n
  \end{gather*}
  Posta $x$ come l'$n$-upla delle coordinate di $v$ rispetto
  alla base ordinata $B$, allora
  \begin{gather*}
      (M_{B, B}(f - \lambda I_V))x = 0 
  \end{gather*}
  Ma questo vuol dire che
  \begin{gather*}
      (f - \lambda I_V)v = 0 \ \Longrightarrow \ f(v) = \lambda I_V(v) \ \Longrightarrow \ \lambda \in \text{spettro}(f)
  \end{gather*}
  $\ \Longrightarrow$ Se $\lambda$ è autovalore di $f$, allora 
  \begin{gather*}
      \exists v \in V - \{0\} : f(v) = \lambda v \ \Longrightarrow \ f(v) - \lambda I_V(v) = 0 
  \end{gather*}
  Allora, posto $x$ come l'$n$-upla delle coordinate di $v$ rispetto
  alla base $B$ di $V$, si ha che
  \begin{gather*}
    (M_{B, B}(f - \lambda I_V))x = 0 \ \Longrightarrow \  rk M_{B, B}(f - \lambda I_V) < n \ \Longrightarrow \ \det(M_{B, B}(f) - \lambda I_V) = 0
  \end{gather*}
  Dunque $P_f(\lambda) = 0$. 
\end{proof}

\begin{lemma}[Proprietà del polinomio caratteristico]
  Sia $A \in M(n \times n, \mathbb{K})$ dove $\mathbb{K}$ è un campo e $n \in \mathbb{N} - \{0\}$, 
  allora sono vere le seguenti:
  \begin{enumerate}
      \item Il termine di grado zero di $P_A$ è $\det(A)$;
      \item $P_A$ ha grado $n$ e scrivendo i termini di tutti i gradi si ha:
      \begin{align}
          P_A(t) = (-1)^{n} t^{n} + (-1)^{n - 1}\text{tr}(A)t^{n - 1} \dots + \det(A)    
      \end{align}
  \end{enumerate}
\end{lemma}
\begin{proof}
  1) Per calcolare il termine di grado zero basta porre
  $x = 0$ e quindi il termine di grado zero è proprio il determinante poiché
  \begin{gather*}
      P_A(0) = \det(A - 0I_n) = \det(A)
  \end{gather*}
  2) Si procede per induzione su $n$, si considera, per $n = 1$:
  \begin{gather*}
      A = (a) \ \Longrightarrow \   P_A(t) = \det((a) - tI_1) = a - t = -t + a
  \end{gather*}
  Procedendo ora per il passo induttivo $n - 1 \to n$, si scrive allora il polinomio di $A$:
  \begin{gather*}
      P_A(t) = \det(A - t I_n) = \det \begin{pmatrix}
          a_{1, 1 } - t & a_{1, 2} & a_{1, 3} & . & . \\
          a_{2, 1} & a_{2, 2} - t & a_{2, 3} & . & . \\
          a_{3, 1} & a_{3, 2} & a_{3, 3} - t & . & . \\
          . & . & . & . & . \\
          . & . & . & . & .
      \end{pmatrix}
  \end{gather*}
  Quindi sviluppando la prima colonna si ottiene il 
  determinante:
  \begin{gather*}
      = (a_{1, 1} - t) \det((A - t I)_{\hat{1}, \hat{1}  }) - a_{2, 1} \det((A  - tI)_{\hat{2}, \hat{1}  }) + \dots
  \end{gather*}
  Allora si osserva che è possibile riscrivere il determinante ignorando i termini di grado $\leq n - 2$ in $t$:
  \begin{gather*}
      = (a_{1, 1} - t)\det(A_{\hat{1}, \hat{1}  } - tI_{n - 1}) + \text{termini di grado } \leq n - 2, \text{per } t \\
      (a_{1, 1 - t})P_{A_{\hat{1}, \hat{1}  }} (t) + \text{termini di grado } \leq n - 2, \text{per } t.
  \end{gather*}
  Per l'ipotesi induttiva si ha che  
  \begin{align*}
      &= (a_{1,1} - t) ((- 1)^{n - 1}t^{n - 1} + (-1)^{n - 2} tr(A_{\hat{1} \hat{1} })t^{n - 2} + \text{termini di grado } \leq n - 3  ) + \text{termini } \leq n - 2 \\
      &= (-1)^{n}  a_{1, 1}(- 1)^{n - 1}t^{n - 1} + (-t)(-1)^{n - 2} tr(A_{\hat{1} \hat{1} })t^{n - 2} + \text{termini } \leq n - 2 \\
      &= (-1)^{n} t^{n} + (-1)^{n - 1}t^{n - 1} (a_{1, 1} + tr(A_{\hat{1}, \hat{1}  })) + \text{termini } \leq n - 2   
  \end{align*}
  Si ottiene dunque la tesi riassumendo le conclusioni:
  \begin{gather*}
      (-1)^{n} t^{n} + (-1)^{n - 1}t^{n - 1} tr(A) + \text{termini di grado } \leq n - 2    
  \end{gather*}
\end{proof}

\begin{corollary}[Somma e determinante della matrice in relazione agli autovalori]
  Sia $A \in M(n \times n, \mathbb{K})$ se
  \begin{gather*}
      P_A(t) = (\lambda_1 - t)\cdot ... \cdot  (\lambda_n - t),
  \end{gather*}
  Ossia se il polinomio caratteristico della matrice è
  completamente fattorizzabile in termini di primo grado, allora valgono
  \begin{align}
      \det(A) &= \lambda_1 \cdot  ... \cdot  \lambda_n \\
      tr(A) &= \lambda_1 + \dots + \lambda_ n
  \end{align}
\end{corollary}
\begin{proof}
  Per la proposizione precedente si scrive l'espressione del polinomio caratteristico:
  \begin{gather*}
      P_A(t) = (-1)^{n}t^{n} + (-1)^{n - 1} tr(A) t^{n - 1} + \dots + \det(A)    
  \end{gather*}
  Per ipotesi si ha che
  \begin{gather*}
      P_A(t) = (\lambda_1 - t) \cdot  ... \cdot (\lambda_n - t)
  \end{gather*}
  Dunque si può scrivere esplicitamente il polinomio caratteristico: 
  \begin{gather*}
      (-t)^{n} + (-t)^{n - 1}(\lambda_1 + \dots + \lambda_n) + \dots + \lambda_1 \cdot  ... \cdot  \lambda_n = \\
      (-1)^{n}t^{n} + (-1) t^{n - 1}(\underset{tr(A)}{\underline{\lambda_1 + \dots + \lambda_n}}) + \dots + \underset{\det(A)}{\underline{\lambda_1 \cdot  ... \cdot  \lambda_n     }}
  \end{gather*}
  E ottenere la tesi. 
\end{proof}


\section{Autospazi}
\begin{definition}[Autospazio]
  Sia $V$ uno spazio vettoriale su di un campo $\mathbb{K}$ allora,
  se $f$ è l'applicazione lineare su quello spazio e $\lambda$ un
  suo autovalore, si definisce \textbf{autospazio di $f$ relativo a $\lambda$} 
  il seguente sottospazio vettoriale di $V$:
  \begin{align}
      \ker(f - \lambda I_V)
  \end{align}
  che coincide con 
  \begin{gather*}
      \{v \in V : v \ \text{autovettore di $f$ con autovalore} \ \lambda\} \cup \{0_V\}
  \end{gather*}
  I due insiemi sono uguali perché
\begin{gather*}
  v \in \ker(f - \lambda I_V)\  \Longleftrightarrow \ (f - \lambda I_V)(v) = 0\  \Longleftrightarrow \ f(v) - \lambda I_V(v) = 0\ \Longleftrightarrow \ f(v) = \lambda v 
\end{gather*}
\end{definition}

\begin{definition}[Molteplicità geometrica]
  Si definisce \textbf{molteplicità geometrica di $\lambda$} come la dimensione
  dell'autospazio e si indica come $m_g(\lambda)$. Per cui $\lambda$ è autovalore
  per $f$ se e solo se $m_g(\lambda) \geq 1$. 
\end{definition}

\begin{definition}[Molteplicità algebrica]
  Supponiamo adesso che la dimensione di $V$ sia finita, allora 
  posso definire \textbf{molteplicità algebrica di $\lambda$} il massimo
  $s \in \mathbb{N}$ tale che:
  \begin{align}
      (t - \lambda)^{s}  
  \end{align}
  divide $P_f$ e si denota come $m_a(\lambda)$. Per cui $\lambda$ è autovalore
  per $f$ se e solo se $m_a(\lambda) \geq 1$. 
\end{definition}

\begin{example}[Trovare molteplicità algebriche e geometriche]
  \begin{gather*}
      A = \begin{pmatrix}
          8 & 0 & 0 & 0 \\
          5 & 8 & 0 & 0 \\
          0 & 0 & 2 & 6 \\
          0 & 0 & 0 & 3
      \end{pmatrix}
  \end{gather*}
  Voglio trovare lo spettro della matrice A e per ogni
  $\lambda$ entrambe le molteplicità:
  \begin{gather*}
      P_f(t) = \det\begin{pmatrix}
        8 - t & 0 & 0 & 0 \\
        5 & 8 - t & 0 & 0 \\
        0 & 0 & 2 - t & 6 \\
        0 & 0 & 0 & 3 - t
    \end{pmatrix} = \det\begin{pmatrix}
        8 - t & 0 \\
        5 & 8 - t
    \end{pmatrix} = \det \begin{pmatrix}
        2 - t & 6 \\
        0 & 3 - t
    \end{pmatrix} \\
    = (8 - t)^{2} (2  -t)(3 - t) \Rightarrow \text{ spettro } = \{8, 2, 3\}. 
  \end{gather*}
  Allora le molteplicità algebriche sono date dal grado di ogni fattore
  nel polinomio caratteristico:
  \begin{gather*}
      m_a(8) = 2 \\
      m_a(3) = 1 \\
      m_a(2) = 1
  \end{gather*}
  Per le molteplicità geometriche invece si calcolano tutte le dimensioni
  degli autospazi:
  \begin{gather*}
      \dim(\ker(f - 8I)) = \dim \ker \begin{pmatrix}
          0 & 0 & 0 & 0 \\
          5 & 0 & 0 & 0 \\
          0 & 0 & -6 & 6 \\
          0 & 0 & 0 & -5
      \end{pmatrix} \ \Longrightarrow \ m_g(8) = 1 \\
      \dim(\ker(f - 2I)) = \dim \ker \begin{pmatrix}
          6 & 0 & 0 & 0 \\
          5 & 6 & 0 & 0 \\
          0 & 0 & 0 & 6 \\
          0 & 0 & 0 & 1
      \end{pmatrix} \ \Longrightarrow \ m_g(2) = 1 \\
      \dim(\ker(f - 3I)) = \dim \ker \begin{pmatrix}
          5 & 0 & 0 & 0 \\
          5 & 5 & 0 & 0 \\
          0 & 0 & -1 & 6 \\
          0 & 0 & 0 & 0
      \end{pmatrix} \ \Longrightarrow \ m_g(3) = 1
  \end{gather*}
\end{example}

\begin{lemma}[Proprietà di matrici triangolari superiori e diagonali]
  Sia $V$ uno spazio vettoriale di dimensione finita in $\mathbb{K}$ e sia $f : V \to V$ 
  una sua applicazione lineare. 
  \begin{enumerate}
      \item Se $M_{B, B}(f)$ è una matrice triangolare
      allora gli autovalori di $f$ sono gli elementi sulla diagonale e,
      $\forall \lambda$ coefficiente sulla diagonale, vale che $m_a(\lambda)$ è il numero 
      di volte che $\lambda$ compare sulla diagonale. 
      \item Se $A$ è diagonale e $\lambda$ è un elemento sulla diagonale, allora:
      \begin{align}
          m_g(\lambda) = m_a(\lambda) = \text{ comparse di } \lambda \text{ sulla diagonale }
      \end{align}
  \end{enumerate}
\end{lemma}
\begin{proof}
  Si dimostrano le due:\\
  1): Data una matrice triangolare il cui polinomio caratteristico è:
  \begin{gather*}
      P_A(t) = \det\begin{pmatrix}
          a_{1, 1} - t& a_{2, 2} & a_{1, 3} &  . & . \\
          0 & a_{2, 2} - t& a_{2, 3} & . & . \\
          0 & 0 & a_{3, 3} - t & . & . \\
          . & . & . & . & . \\
          . & . & . & . & .
      \end{pmatrix} = (a_{1, 1} - t)(a_{2, 2} - t)\cdot  ... \cdot (a_{n, n} - t)
  \end{gather*}
  Ossia è scomponibile in fattori di primo grado, dunque gli autovalori sono $a_{1, 1}, \dots, a_{n, n}$ e $m_a(\lambda)$ uguale
  al numero di volte che $\lambda$ compare sulla diagonale, anche se non è detto che siano uguali a $m_g(\lambda)$. \\
  2): Data $A$ diagonale, il suo polinomio caratteristico 
  è il determinante di una matrice diagonale. Dunque sulla diagonale ci saranno
  $k$ elementi uguali, allora, per la prima parte del teorema: $m_a(\lambda) = k$.
  \begin{gather*}
      rk(A - \lambda I) = n - k \\
      \dim \ker(A - \lambda I) = n - rk(A - \lambda I) = k = m_g(\lambda)
  \end{gather*}
\end{proof}

\begin{theorem}[La molteplicità algebrica è sempre maggiore o uguale di quella geometrica]
  Sia $V$ uno spazio vettoriale  su di un campo $\mathbb{K}$ e sia $f : V \to V$
  una sua applicazione lineare. Sia $\lambda$ un suo autovalore,
  allora si ha che:
  \begin{align}
      m_g(\lambda) \leq m_a(\lambda)
  \end{align}
\end{theorem}
\begin{proof}
  Considerata una base $\left<v_1, \dots, v_k\right>$ di $\ker(f - \lambda I)$, per qualche $\lambda \in \text{spettro}(f)$,
  si suppone che $m_g(\lambda) = k$. Dunque $\{v_1, \dots, v_k\}$
  è un insieme di vettori linearmente indipendenti di $V$. Si può dunque
  estendere una base di $V$:
  \begin{gather*}
      B = \{v_1, \dots, v_k, v_{k + 1}, \dots, v_n\}
  \end{gather*}
  Considerando ora l'applicazione lineare associata alla base $B$
  per qualche matrice $X, Y, I_k$ dove
  $I_k$ è la matrice identità $k \times k$, è possibile scrivere la matrice
  associata alla base $B$ per la funzione lineare come
  \begin{gather*}
      M_{B, B} (f) = \begin{pmatrix}
          \lambda I_k& X \\
          0 & Y
      \end{pmatrix}
  \end{gather*}
  Gli autovettori, per essere tali, devono soddisfare
  \begin{gather*}
      f(v_k) = \lambda v_k
  \end{gather*}
  Dunque la matrice $I_k$ compare perché l'immagine dei primi $k$ vettori 
  produce esattamente il blocco $\lambda I_k$ poiché sono un insieme di vettori 
  linearmente indipendente che genera l'autospazio relativo a $\lambda$. Adesso, per la definizione
  di autovalore, si calcola il polinomio caratteristico 
  \begin{gather*}
      P_f(t)  = \det(M_{B, B}(f) - tI_n) = \det\left(\begin{pmatrix}
        \lambda I_k & X \\
        0 & Y
      \end{pmatrix} -t I_n\right) = \\
      \det((\lambda - t) I) \det(Y - t I_{n -k}) = (\lambda - t)^{k} \cdot  \det(Y   - t I_{n - k}) 
  \end{gather*}
  $(\lambda - t)^{k}$ si ottiene con la definizione di linearità del determinante, secondo la quale
  se un coefficiente moltiplica tutte le righe di una matrice, per poterlo portare fuori si
  deve elevare quel coefficiente al numero di righe della matrice in questione. Dunque si ottiene la tesi dato che $(\lambda - t)^{k}$ divide il polinomio
  caratteristico (più precisamente so che $m_a(\lambda)$ è sicuramente maggiore o uguale a $k$): 
  \begin{gather*}
      m_a(\lambda) = k \geq m_g(\lambda).
  \end{gather*}
\end{proof}

\begin{definition}[Matrice degenere e non degenere]
  Si definisce una matrice \textbf{non degenere} se ha tutte le molteplicità
  $< 2$, altrimenti si definisce \textbf{degenere} una matrice
  con le molteplicità $ \geq 2$.
\end{definition}

\section{Diagonalizzabilità}
\begin{definition}[Diagonalizzabilità]
  Sia $V$ uno spazio vettoriale in $\mathbb{K}$ e sia $f : V \to V$ una sua
  applicazione lineare. Si dice allora che $f$ è \textbf{diagonalizzabile} se
  esiste una base di $V$ composta da autovettori di $f$. Se $V$  ha dimensione
  finita, questo equivale a dire che esiste una base ordinata B di $V$
  tale che $M_{B, B}(f)$ è diagonale. \\
  Dire che $M_{B, B}(f)$ è diagonale equivale proprio a dire che $f(v_j)$ 
è un multiplo di $v_j, \forall j \in \{1, \dots, n\}$. 
\end{definition}

\begin{definition}[Una matrice diagonalizzabile è simile alla sua matrice diagonalizzata]
  Sia $n \in \mathbb{N} - \{0\}$ e sia $\mathbb{K}$ un campo allora, posta $A \in M(n \times n, \mathbb{K})$, si dice che essa è \textbf{diagonalizzabile} se $f_A : \mathbb{K}^{n} \to \mathbb{K}^{n}$
  è diagonalizzabile. Questo equivale a dire che $A$ è simile ad 
  una matrice diagonale, ossia 
  \begin{gather*}
      \exists C \in GL(n, \mathbb{K}) : C^{-1} A C \text{ è diagonale}
  \end{gather*}  
\end{definition}

\begin{theorem}[Equivalenza delle due definizioni]
  Le due definizioni sono equivalenti.
\end{theorem}
\begin{proof}
  La funzione $f_A$ è diagonalizzabile se e solo
  se $\exists C \in GL(n, \mathbb{K})$ tale che $C^{-1} AC$ è diagonale.
  $\ \Longrightarrow \ $ Per la definizione di diagonalizzabilità, $f_A$ è diagonalizzabile ed
  $\exists B$ base di $\mathbb{K}^{n}$ tale che $M_{B, B}(f_A)$ è diagonale e quindi, per la formula del cambio di base, si ottiene:
  \begin{gather*}
      M_{B, B}(f_A) = M_{\epsilon, B}(I_{\mathbb{K}^{n} })^{-1} M_{\epsilon, \epsilon}(f_A)M_{\epsilon, B}(I_{\mathbb{K}^{n} }) = M_{\epsilon, B}(I)^{-1}  AM_{\epsilon, B}(I)
  \end{gather*}
  La quale è quindi diagonale. Ma se $f_A$ è diagonalizzabile, allora $A$ è simile
  ad una matrice diagonale poiché la matrice $C$ che la diagonalizza è proprio definita come:
  \begin{gather*}
      C := M_{\epsilon, B}(I) \ \Longrightarrow \  C^{-1} AC \text{ diagonale}
  \end{gather*}
  $\Longleftarrow$:  supponendo che
  $\exists C \in GL(n, \mathbb{K})$ tale che $C^{-1} AC$ sia diagonale, si vuole dimostrare che la funzione
  sia diagonalizzabile e che quindi esista una base di $\mathbb{K}^{n}$
  tale che la funzione sia diagonale. Posta $B$ una base del tipo: 
  \begin{gather*}
      B = \{C^{(1)}, \dots, C^{(n)}  \}
  \end{gather*} 
  La quale è una base poiché la matrice $C$ è invertibile, allora osservo che:
  \begin{gather*}
      M_{\epsilon, B}(I_{\mathbb{K}^{n} }) = C
  \end{gather*}
  Per calcolare la prima colonna della matrice C devo 
  applicare $I_{\mathbb{K}^{n} }$ al primo elemento della matrice $B$
  \begin{gather*}
      M_{\epsilon, B}(I_{\mathbb{K}^{n} }) ^{(1)} = C^{(1)}  
  \end{gather*} 
  Dato che $C^{-1} AC$ è diagonale si ha, per le formule del cambiamento di
  base delle matrici, che la seguente matrice è diagonale e uguale a
  \begin{gather*}
      M_{\epsilon, B}(I)^{-1}  M_{\epsilon, \epsilon}(f_A) M_{\epsilon, B}(I) = M_{B, B}(f_A)
  \end{gather*}
\end{proof}

\begin{lemma}[Autovettori relativi ad autovalori distinti sono indipendenti]
  Sia $V$ uno spazio vettoriale su di un campo $\mathbb{K}$. Sia allora 
  $f : V \to V$ un'applicazione lineare e siano
  $\lambda_1, \dots, \lambda_k$ degli autovalori di $f$ distinti e siano
  $v_1, \dots, v_k \in V$ degli autovettori con autovalori quelli prima,
  allora questo insieme di vettori sono linearmente indipendenti.  
\end{lemma}
\begin{proof}
  Per induzione su $k$ possiamo dire che
  \begin{gather*}
      k = 1 \{v_1\}
  \end{gather*}
  è un insieme di vettori indipendenti poiché è autovettore e non nullo.
  Allora fino a $k - 1 \ \Longrightarrow \  k$ do per buono che se
  ho $k - 1$ autovettori per la funzione con autovalori
  distinti allora essi sono indipendenti. Siano allora
  \begin{gather*}
      v_1, \dots, v_k \text{ autovettori con autovalori } \lambda_1, \dots, \lambda_k, \ \lambda_i \neq  \lambda_j, \ i \neq j
  \end{gather*}
  Siano allora $a_1, \dots, a_k \in \mathbb{K}$ tale che 
  \begin{gather*}
      a_1 v_1 + \dots + a_k v_k = 0_V
  \end{gather*} 
  Applicando entrambi i membri all'applicazione lineare ottengo allora
  \begin{gather*}
      f(a_1 v_1 + \dots + a_k v_k) = f(0_V) \ \Longrightarrow \  a_1 \lambda_1 v_1 + \dots + a_k\lambda_kv_k= 0_V
  \end{gather*}
  Moltiplicando la scorsa equazione per $\lambda_1$ e sottraendole quest'ultima espressione, 
  \begin{gather*}
      a_2v_2(\lambda_2 - \lambda_1) + \dots + a_k (\lambda_k - \lambda_1) v_k = 0_V 
  \end{gather*}
  per ipotesi induttiva, $\{v_2, \dots, v_k\}$ è un insieme di vettori 
  linearmente indipendenti e dunque, dato che $\lambda_i - \lambda_1 \neq 0 \ \forall i \in \{2, \dots, k\}$,
  si ottiene che $a_i = 0 \ \forall i \in \{2, \dots, k\}$. Si dimostra che anche $a_1 = 0$
  semplicemente sottraendo all'espressione di prima un $\lambda$ diverso.
\end{proof}

\begin{corollary}
  Sia $V$ uno spazio vettoriale su di un campo $\mathbb{K}$ e sia $f : V \to V$ un'applicazione
  lineare, dati $\lambda_1, \dots, \lambda_k$ un insieme
  di autovalori di $f$ dei rispettivi autovettori distinti $w_1, \dots, w_k \in V$ tali che $\lambda_i \neq \lambda_j, i \neq j$ e tali che
  ogni $w_i \in \ker (f - \lambda_i I)$, se risulta
  \begin{align}
      \sum_{i = 1}^{k} w_i = 0 \ \Longrightarrow \ w_i = 0 \qquad \forall i = \{1, \dots, k \}
  \end{align}
\end{corollary}
\begin{proof}
  So che
  \begin{gather*}
      \sum_{i = 1}^{n} w_i = 0 
  \end{gather*}
  Allora posso supporre per assurdo che qualcuno dei $w_i$ siano
  non nullo e che quindi siano un autovettore, allora
  considerando solo quelli non nulli avrei una combinazione
  lineare nulla con coefficienti 1 di autovettori
  relativi ad autovalore, il che sarebbe assurdo per il teorema di prima.
\end{proof}

\begin{theorem}[Criterio di diagonalizzabilità]
  Sia $V$ uno spazio vettoriale su di un campo $\mathbb{K}$
  di dimensione finita e sia $f : V \to V$ un'applicazione lineare:
  allora la funzione è diagonalizzabile se e solo se valgono:
  \begin{enumerate}
      \item $P_f$ sia uguale ad un prodotto di polinomi di grado 1 a coefficienti in $\mathbb{K}$;
      \item $\forall \lambda \in \mathbb{K}$ autovalore di $f$ tale che $m_a(\lambda) = m_g(\lambda)$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  $\Longrightarrow $ Dato che la funzione $f$ è diagonale esiste allora una matrice B tale che:
  \begin{gather*}
      M_{B, B} (f) = \begin{pmatrix}
          d_1 & \dots & 0 \\
          \dots & \dots & \dots \\
          0 & \dots & d_n
      \end{pmatrix} \ \Longrightarrow \ P_f(t) = P_{M_{B, B}(f)}(t) = (d_1 - t)\cdot ...\cdot (d_n - t).
  \end{gather*}
  La condizione 1 è soddisfatta. 
  Se $\lambda$ è un autovalore, questo risiede sulla diagonale e dunque vale 
  \begin{gather*}
      m_a(\lambda) = \text{ numero di apparizioni di } \lambda \text{ sulla diagonale } = m_g(\lambda)
  \end{gather*}
  E quindi la condizione 2 è soddisfatta. \\
  $\Longleftarrow$ Supponendo che le condizioni 1 e 2 siano verificate, si vuole dimostrare
  che esista una base $B$ di $V$ formata da autovettori di $f$. Sia allora
  \begin{gather*}
      \text{spettro}(f) = \left< \lambda_1, \dots, \lambda_k \right> 
  \end{gather*}
  Allora per la seconda proprietà si deve avere
  \begin{gather*}
      m_g(\lambda_i) = m_a(\lambda_i) \quad \forall i = \{1, \dots, k\}, \  \lambda_i \neq  \lambda_j, \ i \neq j.
  \end{gather*}
  Dato che vale anche la prima condizione:
  \begin{gather*}
      P_f(t) = \underset{\in \mathbb{K}}{c} (\lambda - t)^{n_1}\cdot ... \cdot (\lambda_k - t)^{n_k} \ \Longrightarrow \ \deg P_f = n 
  \end{gather*}
  Si è denotato la molteplicità algebrica di ogni autovalore come $n_i$, con $i \in \{1, \dots, k\}$. Questo 
  mi permette di dire che il grado di $P_f = n = n_1 + \dots + n_k$. 
  Presi in considerazioni i $k$ autospazi relativi ai $k$ autovalori:
  \begin{gather*}
      \ker(f - \lambda_1 I), \dots, \ker(f - \lambda_k I)
  \end{gather*}
  Si può considerare una base per ciascuno di essi con $i \in \{ 1, \dots, k\}$:
  \begin{gather*}
      B_i = \left< v_1^{i}, \dots, v_{n_i}^{i}   \right> 
  \end{gather*}
  Se si unisse tutte le basi insieme, si otterrebbe la seguente base:
  \begin{gather*}
      B = \left< v_j^{i}  \right> \qquad 
        \begin{array}{l}
            \forall i \in \{1, \dots, k\} \\
            \forall j \in \{n_1, \dots, n_i\}
        \end{array}
  \end{gather*}
  Se si dimostra che questa base è una base di $V$, allora si è trovato una base di $V$ formata da autovettori di $f$ e quindi 
  si è dimostrato che $f$ è diagonalizzabile secondo la definizione di matrice
  diagonalizzabile. Si dimostra allora che ciascuna di queste basi è un insieme di vettori linearmente 
  indipendenti (dato che considero $n_1 + \dots  + n_k = n = \dim V$), siano allora 
  \begin{gather*}
      \mu_j^{i} \in \mathbb{K} : \sum_{i, j = 1}^{k, n_i} \mu_j^{i}v _j^{i} = 0     \qquad
        \begin{array}{l}
            \forall i \in \{1, \dots, k\} \\
            \forall j \in \{1, \dots, n_i\}
        \end{array}
  \end{gather*}
  Definendo allora il vettore $w_i \in \ker(f - \lambda_i I)$, per il corollario,
  si ha che la sommatoria di questi vettori è zero, quindi i vettori
  considerati sono linearmente indipendenti e $B$ è una base
  di $V$. Infine, $V$ è un insieme di autovettori di $f$.  
\end{proof}

\begin{example}[Verifica diagonalizzabilità]
  Sia la matrice
  \begin{gather*}
      A = \begin{pmatrix}
          4 & 1 & 3 \\
          0 & 5 & -3 \\
          0 & 1 & 1
      \end{pmatrix} \in M (3 \times 3, \mathbb{R})
  \end{gather*}
  $\exists C \in GL(3, \mathbb{R})$ tale che $C^{-1} AC$ è diagonale? 
  Calcoliamo il polinomio
  \begin{gather*}
      P_A(t) = \det \begin{pmatrix}
          4 - t & 1 & 3 \\
          0 & 5 -t & -3 \\
          0 & 1 & 1- t
      \end{pmatrix} = (4 -t)((5 - t)(1 - t) + 3) = (4 - t)^{2}(2 - t) 
  \end{gather*}
  e quindi le soluzioni diventano $4, 2$ e la prima condizione è soddisfatta.
  Calcoliamo lo spettro di A:
  \begin{gather*}
      \begin{tabular}{c | c | c}
        & 2 & 4 \\
        \hline
        $m_a$ & 1 & 2 \\
        \hline
        $m_g$ & 1 & 1
      \end{tabular}
  \end{gather*}
  Si osserva che 
  \begin{gather*}
      \left\{\begin{array}{l}
        1 \leq m_g(2) \leq m_a(2) = 1 \\
        m_a(4) \neq  m_g(4)
      \end{array}\right.
  \end{gather*}
  Non è allora diagonalizzabile. 
\end{example}

\begin{observation}
    Sia $B \in GL(n, \mathbb{K})$, con operazioni di riga di tipo 1 e 3 si
    può trasformare in una matrice con $m$ scalini. Con ulteriori operazioni di
    riga si può trasformare nella matrice identità. Cioè esistono matrici invertibili
    \begin{gather*}
        L_1 \cdot ... \cdot L_k \cdot B = I \qquad B = L_k^{-1} \cdot ... \cdot L_1^{-1}I 
    \end{gather*}
\end{observation}

\begin{lemma}[Lo spazio di una applicazione lineare è la somma diretta degli autospazi]
  Sia $V$ uno spazio vettoriale su $\mathbb{K}$ di dimensione finita
  e sia $f : V \to V$ un'applicazione lineare diagonalizzabile e sia 
  lo $\text{spettro}(f) = \left< \lambda_1, \dots, \lambda_k \right>$. Si chiama allora
  $V_i$ l'autospazio relativo a $\lambda_i$ ossia $\ker(f - \lambda_i I_V)$, dunque si ottiene
  $V$ come somma diretta dei singoli autospazi:
  \begin{align}
      V = V_1 \oplus \dots \oplus V_k
  \end{align}
\end{lemma}
\begin{proof}
  Data la base $B_1$ dello spazio $V_1$, $B_2$ la base di $V_2$ e così via, si vuole dimostrare che $B$ 
  è definita come l'unione della basi e $V$ come la somma diretta degli spazi
  vettoriali. Si può scrivere ogni elemento $v \in V$ come somma di un elemento di $V_1$, uno di $V_2$ e così via
  in maniera unica. Si dimostra dunque che è unica:
  \begin{gather*}
      v = v_1 + \dots + v_k \qquad v = v_1' + \dots + v_k'
  \end{gather*}
  Dato che $v_1 \in V_1$ e $v_2 \in V_2$ e così via, allora 
  \begin{gather*}
      v_1 - v_1' \in V_1 \ \dots \ v_k - v_k' \in V_k
  \end{gather*}
  Dunque se
  \begin{gather*}
      (v_1 - v_1') + \dots + (v_k - v_k') = 0 \ \Longrightarrow \ v_1 = v_1' \ \dots \ v_k = v_k'
  \end{gather*}
  Dunque ogni elemento di $v$ si scrive come combinazione unica di elementi dei vari autospazi.
  Allora definendo $\Pi_i$ come la funzione che mi permette di ottenere l'$i$-esimo
  autospazio 
  \begin{gather*}
      \Pi_i : V \to V_i 
  \end{gather*}
  Si può definire $f$ come 
  \begin{gather*}
      f = \lambda_1 \Pi_1 + \dots + \lambda_i\Pi_i,
  \end{gather*}
  Dato che si può esprimere $v$ come combinazione lineare
  degli elementi degli autospazi
  \begin{gather*}
      v = v_1 +\dots + v_k \ \Longrightarrow \ f(v) = (\lambda_1 \Pi_1  + \dots + \lambda_k \Pi_k) (v).
  \end{gather*}
  E dunque la tesi. 
\end{proof}

\begin{lemma}[Autovettori relativi ad autovalori diversi sono ortogonali tra di loro]
  Sia $A \in M(n \times n, \mathbb{R})$ \textbf{simmetrica} e siano $\lambda$ e $\mu$
  due suoi autovalori distinti. Sia $v$ un autovettore per
  $A$ con autovalore $\lambda$ e sia $w$ lo stesso per $\mu$. Allora $v$ e $w$
  sono ortogonali per il prodotto scalare standard.
\end{lemma}
\begin{proof}
  Considerando $^{t}v A w$ rispettivamente di dimensioni
  $1 \times n$; $n \times n$, $n \times 1$. Allora dato che
  \begin{gather*}
      ^{t}v A w = \ ^{t}v(Aw) = \ ^{t}v(\mu w) = \mu \ ^{t}v w   
  \end{gather*}
  Dato che A è simmetrica allora questo è uguale alla corrispettiva
  con $A$ trasposta e quindi 
  \begin{gather*}
      ^{t}v \ ^{t} Aw = \lambda \ ^{t}v w 
  \end{gather*}
  E quindi dato che la matrice è simmetrica il prodotto scalare è zero
  in quanto $\lambda \neq \mu \neq 0$: 
  \begin{gather*}
    \lambda \ ^{t}v w = \mu \ ^{t}v w \ \Longrightarrow \  (\mu - \lambda) \ ^{t}v w = 0 \ \Longrightarrow \   ^{t}v w = 0 = (v, w) = 0    
  \end{gather*}
\end{proof}

\begin{lemma}[Criterio di diagonalizzabilità ristrestto a $n$ autovalori distinti]
  Sia $A \in M(n \times n, \mathbb{K})$, se questa ha $n$ autovalori distinti in $\mathbb{K}$ allora
  $A$ è diagonalizzabile in $\mathbb{K}$.
\end{lemma}
\begin{proof}
  Se $A$ ha $n$ autovalori distinti in $\mathbb{K}$, allora spettro$(f) = \{\lambda_1, \dots, \lambda_n\}$. 
  Dunque gli autovalori sono, per definizione, radici di $P_A$, ossia $(\lambda_1  - t)$ e tutti gli
  altri, dividono il polinomio caratteristico. 
  si ottiene
  \begin{gather*}
      P_A(t) = c(\lambda_1 - t) \cdot ... \cdot (\lambda_n - t)
  \end{gather*}
  $c = 1$ poiché il grado massimo deve essere $(-t)^{n}$. Dunque
  è soddisfatta la prima condizione del criterio di diagonalizzabilità.
  Il secondo punto del criterio vale poiché la matrice ha $n$ autovalori distinti, 
  ossia $m_a(\lambda_i) = 1 \ \forall i \in \{1 ,\dots, n\}$. Allora, per il 
  teorema della relazione tra $m_a$ e $m_g$, vale anche la seconda condizione del criterio di diagonalizzabilità e dunque
  la matrice è diagonalizzabile in $\mathbb{K}$.
\end{proof}

\begin{theorem}[Teorema del prodotto del Kernel]
  Siano $A \in M(n \times n, \mathbb{K}), B \in M(m \times m, \mathbb{K})$ 
  allora 
  \begin{align}
      \ker \left(\begin{tabular}{c | c}
        A & 0 \\
        \hline 
        0 & B
      \end{tabular}\right) \approx \ker A \times \ker B
  \end{align}
  Ossia lo spazio definito dal kernel della matrice a blocchi, la cui 
  diagonale è composta dalle matrici $A$ e $B$, è esattamente il prodotto cartesiano 
  dei kernel delle matrici.
\end{theorem}
\begin{proof}
  Il kernel della matrice formata dai due blocchi è il seguente spazio vettoriale:
  \begin{gather*}
    \ker \left(\begin{tabular}{c | c}
      A & 0 \\
      \hline 
      0 & B
    \end{tabular}\right) = \left\{x \in \mathbb{K}^{n + m} \left|  \right. \begin{pmatrix}
        A & 0 \\
        0 & B
    \end{pmatrix} x = p\begin{pmatrix}
        A_x \\
        B_x
    \end{pmatrix}=0_{\mathbb{K}^{m + n} } \right\}
  \end{gather*}
  Dati due valori $y \in \mathbb{K}^{n}$ e $z \in \mathbb{K}^{m}$, se questi due vettori appartengono
  al kernel delle due matrici si ha la seguente condizione:
  \begin{gather*}
      Ay = 0_{\mathbb{K}^{n} } \qquad Bz = 0_{\mathbb{K}^{m} }
  \end{gather*}  
  Questo permette di definire il kernel della matrice di partenza come le coppie di vettori tali che si annullano 
  i due $\ker$. Ma questo vuol dire rifarsi alla definizione di prodotto cartesiano tra spazi vettoriali:
  \begin{gather*}
      \ker  A \times \ker B = \left\{(y, z) | y \in \ker A, z \in \ker B\right\}
  \end{gather*}
\end{proof}

\begin{theorem}[Criterio di diagonalizzabilità di una matrice triangolare superiore a blocchi]
  Data una matrice triangolare superiore a blocchi
  \begin{gather*}
      A = \begin{pmatrix}
          A_1 & A_{1, 2} & &&\dots \\
          0 & A_2 & &&\dots  \\
          0 & 0 & A_3 & & \dots \\
          0 & 0 & 0 & \dots & \dots \\
          0 & 0 & 0 & 0 & A_k
      \end{pmatrix}
  \end{gather*}
  Il suo polinomio caratteristico è dato dalla seguente:
  \begin{align}
      P_A = P_{A_1} \cdot ... \cdot P_{A_k}
  \end{align}
  Se $\lambda \in \mathbb{K}$ 
  \begin{align}
      m_a(\lambda) (A) &= \sum_{i = 1}^{k}m_a(\lambda)(A_i) \\
      \text{spettro}(A) &= \text{spettro}(A_1 ) \cup \dots \cup spettro(A_k)
  \end{align}  
  Inoltre se $A_{i, j} = 0 \  \forall i \neq  j$, ossia se $ A$ è diagonale 
  a blocchi (semplice) vale la seguente: 
  \begin{align}
      \forall \lambda \in \mathbb{K} \quad m_g(\lambda)(A) = \sum_{i = 1}^{k}m_g(\lambda)(A_i) 
  \end{align}
  Inoltre $A$ diagonalizzabile $\Longleftrightarrow$ $A_i$ è a blocchi.
\end{theorem}
\begin{proof}
  Supponendo di avere la seguente matrice (triangolare superiore a blocchi):
  \begin{gather*}
      A = \begin{pmatrix}
          A_1 & A_{1, 2} & &&\dots \\
          0 & A_2 & &&\dots  \\
          0 & 0 & A_3 & & \dots \\
          0 & 0 & 0 & \dots & \dots \\
          0 & 0 & 0 & 0 & A_k
      \end{pmatrix}
  \end{gather*}
  Il polinomio caratteristico di $A$ è ovviamente il prodotto dei polinomi caratteristici delle
  matrici sulla diagonale:
  \begin{gather*}
      P_A(t) = \det(A - tI) = \det(A_1 - t I) \cdot  \det(A_2 - tI) \cdot  \dots= P_{A_1} \cdot ... P_{A_k}
  \end{gather*}
  Dunque l'unione delle radici di tutti i polinomi corrispondono alla radice del
  polinomio caratteristico della matrice totale, allora lo spettro di questa è
  esattamente l'unione degli spettri delle matrici sulla diagonale. Questo
  permette anche di affermare che
  \begin{gather*}
      m_a(\lambda) (A) = \sum_{i = 1}^{k}m_a(\lambda)(A_i) 
  \end{gather*}
  Supponendo da adesso che la matrice $A$ sia solo diagonale a blocchi, la molteplicità geometrica
  di ogni autovalore corrisponde alla seguente espressione:
  \begin{gather*}
      m_g(\lambda)(A) = \dim \ker(A - \lambda I) = \dim(\ker(A_1 - \lambda I) \times \dots \times \ker(A_k - \lambda I)) = \star
  \end{gather*}
  Questo vuol dire che la molteplicità geometrica di ogni autovalore è esattamente la somma delle molteplicità
  algebriche per quell'autovalore rispetto a tutti i blocchi della matrice $A$. 
  \begin{gather*}
       \star = \sum_{i = 1}^{k}m_g(\lambda)(A_i) 
  \end{gather*}
  che vale per il teorema del prodotto cartesiano del $\ker$. \\
  $\ \Longleftarrow \ $Supponendo che $A_i$ sia diagonale
  a blocchi e che quindi $\exists C_i \in GL$ tale che $C_i ^{-1} A_i C_i$ sia
  anch'essa diagonale allora la matrice formata dalle matrici a blocchi
  $C_i ^{-1} A_i C_i$ è diagonale, dunque anche $A$ lo è:
  \begin{gather*}
      \begin{pmatrix}
          \boxed{C_1} & \dots & 0 \\
          \vdots & \dots & \vdots \\
          0 & \dots & \boxed{C_k}
      \end{pmatrix}^{-1}  \cdot     \begin{pmatrix}
          \boxed{A_1} & \dots & 0 \\
          \vdots & \dots & 0 \\
          0 & \dots & \boxed{A_k}
      \end{pmatrix}\cdot 
            \begin{pmatrix}
          \boxed{C_1} & \dots & 0 \\
          \vdots & \dots & \vdots \\
          0 & \dots & \boxed{C_k}
      \end{pmatrix} = \begin{pmatrix}
          \boxed{D_1}& & \\
          & \dots &\\
          & &\boxed{D_k}
      \end{pmatrix}
  \end{gather*}
  $\ \Longrightarrow \ $Supponendo adesso che $A$ sia diagonalizzabile, 
  allora valgono le due condizioni di diagonalizzabilità, quindi $P_A$ è fattorizzabile
  in fattori di grado uno e inoltre, dato che $m_g(\lambda_i) = m_a(\lambda_i)$
  per la matrice $A$, allora vale anche 
  \begin{gather*}
      \sum_{i = 1}^{k}m_a(\lambda)(A_i) = \sum_{i = 1}^{k}m_g(\lambda)(A_i)
  \end{gather*}
  Allora anche le singole $A_i$ sono diagonali a blocchi. 
\end{proof}


\begin{definition}[Polinomi di matrici]
  Sia $\mathbb{K}$ un campo e sia $p(x)$ un polinomio a coefficienti in $\mathbb{K}$
  nella variabile $x$ 
  \begin{align}
      p(x) = a_{s}x^{s} + a_{s - 1}x^{s - 1} + \dots + a_0  
  \end{align}
  Sia $n \in \mathbb{N} - \{0\}$ e sia $A \in M(n \times n, \mathbb{K})$ definisco allora
  $p(A)$ come la matrice seguente:
  \begin{align}
      a_s A^{s} + a_{s - 1}A^{s - 1} + \dots +  a_0I_n  
  \end{align}
\end{definition}

\begin{example}[Polinomio di matrici]
  \begin{gather*}
      p(x) = 2x^{2} -3x - 1 \in \mathbb{R}[x], A = \begin{pmatrix}
          1 & 0 & 3 \\
          2 & 1 & 4 \\
          0 & 0 & 1
      \end{pmatrix} \in M(3 \times 3, \mathbb{R})
  \end{gather*}
  Calcoliamo il polinomio:
  \begin{gather*}
      p(A) = 2A^{2} - 3A - I_3
  \end{gather*}
  \begin{gather*}
      P(A) = \begin{pmatrix}
          -2 & 0 & 3 \\
          2 & -2 & 16 \\
          0 & 0 & -2
      \end{pmatrix}
  \end{gather*}
\end{example}

\begin{lemma}
  Sia $A \in M(n \times n, \mathbb{K})$ e sia $v$ autovettore con autovalore
  $\lambda$ allora, se $r \in \mathbb{N} - \{0\}$, si ha che $v$
  è un autovettore per $A^{r}$ con autovalore $\lambda^{r}$.  \\
  Inoltre se $\lambda$ è un autovalore per $A$ allora $\lambda^{r}$ è un
  autovalore per $A^{r}$.  
\end{lemma}
\begin{proof}
  \begin{gather*}
      A^{r} v = A^{r - 1} Av = A^{r - 1}(\lambda v) = \lambda A^{r - 1}v \\
      = \lambda A^{r - 2} A v = \lambda A^{r - 2} \lambda v = \lambda^{2}A^{r - 2}v \dots = \lambda^{r}v        
  \end{gather*}
  Più precisamente dimostro che per induzione su di $r$ (infatti per $r = 1$ è ovvio)
\end{proof}

\begin{lemma}
  Sia $A \in M(n \times n, \mathbb{K})$ e sia il polinomio della matrice
  $p(x) \in \mathbb{K}[x]$. Se $v$ è un autovettore per $A$ con autovalore $\lambda$
  allora $v$ è un autovettore per $p(A)$ con autovalore $p(\lambda)$. 
\end{lemma}
\begin{proof}
  \begin{gather*}
      p(x) = a_sx^{s} + \dots + a_0  \ \Longrightarrow \ p(A) = a_sA^{s} + \dots + a_0I 
  \end{gather*}
  Quindi
  \begin{gather*}
      p(A)v = (a_sA^{s} + a_{s - 1}A^{s - 1} + \dots + a_0 I)v
  \end{gather*}
  Si ottiene dunque che $p(A) = p(\lambda)$, dunque la tesi. 
\end{proof}
\begin{example}
  Se $p(x) = x^{2} - 3x + 1$ e $A \in M(n \times n, \mathbb{R})$ con autovalore
  $5$ allora $p(5) = 11$ è un autovalore per $p(A)$. 
\end{example}

\begin{theorem}
  Sia $\mathbb{K}$ un campo e sia $n \in \mathbb{N}$, sia $p(x) \in \mathbb{K}[x]$ il polinomio della
  matrice $A \in M(n \times n, \mathbb{K})$. Se il polinomio di $A$ è scrivibile 
  come prodotto di fattori di grado $1$ a coefficienti in $\mathbb{K}$, cioè
  se la matrice $A$ soddisfa il primo punto del criterio di diagonalizzabilità,
  allora gli autovalori di $p(A)$ sono tutti del tipo $p(\lambda)$ con
  $\lambda$ autovalore di $A$. 
\end{theorem}

\begin{observation}
  Se togliessi l'ipotesi che $A$ soddisfi la condizione del criterio allora
  l'enunciato non è vero. Un esempio è il seguente.
\end{observation}

\begin{example}
  \begin{gather*}
      A = \begin{pmatrix}
          3 & 0 & 0 \\
          0 & 0 & -1 \\
          0 & 1 & 0
      \end{pmatrix} \in M(3 \times 3, \mathbb{R}). \qquad A^{2} = \begin{pmatrix}
          9 & 0 & 0 \\
          0 & -1 & 0 \\
          0 & 0 & -1
      \end{pmatrix} 
  \end{gather*}
  Voglio trovare l'autovalore per $A^{2}$ reale. Dato che il polinomio
  caratteristico di $A = (3 - t)(t^{2} + 1)$ l'unico autovalore reale è $3$. e
  quindi $A^{2} = 9$, tuttavia questa matrice ha anche  $-1$ come autovalore
\end{example}

\begin{theorem}[Anche la matrice potenza è diagonalizzabile se la matrice base è diagonalizzabile]
  Sia $A \in M(n \times n, \mathbb{K})$ diagonalizzabile $A^{r}$ è diagonalizzabile $\forall r \in \mathbb{N}$
  e, più in generale, $p(A)$ è diagonalizzabile $\forall p \in \mathbb{K} [x]$.
\end{theorem}
\begin{proof}
  Sia $C \in GL(n, \mathbb{K})$ tale che $C^{-1} AC = D$. Allora so che
  \begin{gather*}
      C^{-1} A^{r} C = C^{-1} A \cdot ...\cdot A C = C^{-1} AC \cdot  ... \cdot  C^{-1} AC = \\
      D \cdot ... \cdot D = D^{r} 
  \end{gather*}
  La quale è diagonale.
  Sia $p(x) = a_s x^{s} + \dots  + a_1 x + a_0 \in \mathbb{K}[x]$.  
  Con le stesse considerazioni fatte per le potenze di matrici si arriva alla tesi.
\end{proof}

\clearpage
\section{Prodotto vettoriale}
\begin{definition}[Concordata orientazione]
  Siano $B$ e $B'$ due basi ordinate di $\mathbb{R}^{n}$. Si dicono 
  concordemente orientate se $\det\left(M_{B, B'}(I_{\mathbb{R}^{n} })\right) > 0$. 
\end{definition}

\begin{definition}[Prodotto vettoriale]
  Sia $\{i, j, k\}$ una base di $\mathbb{R}^{3}$ ortogonale 
  per il prodotto scalare standard e concordemente orientate
  con la base ordinata $\{e_1, e_2, e_3\}$. Siano allora $v, w$ due
  vettori in $\mathbb{R}^{3}$ e siano $v = v_1 i + v_2j + v_3k, w = w_1i + w_2j + w_3k$.
  Si definisce allora il prodotto vettoriale di $v,w$ che si denota
  con $v \wedge w$, il determinante formale seguente:
  \begin{align}
      \det\begin{pmatrix}
          i & j & k \\
          v_1 & v_2 & v_3 \\
          w_1 & w_2 & w_3
      \end{pmatrix} =  v \wedge w = (v_2 w_3 - v_3w_2)i - (v_1w_3 - v_3w_1)j + (v_1w_2 - v_2w_1)k
  \end{align}   
  Inoltre la definizione non dipende dalla scelta della base $\{i, j, k\}$.
\end{definition}
\begin{definition}[Proprietà del prodotto vettoriale]
  Si elencano dunque le proprietà del prodotto vettoriale:
  \begin{align}
    v \wedge  w = -w \wedge  v, &\qquad \forall v, w \in \mathbb{R}^{3} \\
    v \wedge  v = 0, &\qquad \forall v \in \mathbb{R}^{3}  \\
    (\lambda v + \mu v') \wedge  w = \lambda v \wedge  w + \mu v' \wedge w &\qquad \forall \lambda, \mu \in \mathbb{R}, \forall v, v', w \in \mathbb{R}^{3} \\
    v \wedge (\lambda w + \mu w') =  v \wedge \lambda w + v \wedge \mu w' & \qquad  \forall \lambda, \mu \in \mathbb{R}, \forall v, v', w \in \mathbb{R}^{3} \\
    (v \wedge  w, v) = (v \wedge  w, w) = 0&\qquad \forall v, w \in \mathbb{R}^{3}\\
    i \wedge  j = k, \ j \wedge  k = i, \ k \wedge  i = j &\qquad \forall v, w \in \mathbb{R}^{3}\\
    |v \wedge  w|= |v||w| \sin\theta &\qquad \forall v, w \in \mathbb{R}^{3}
\end{align}
\end{definition}
\begin{proof}
  1: 
  \begin{gather*}
      v \wedge  w = - w \wedge v \qquad \forall v, w \in \mathbb{R}^{3} 
  \end{gather*}
  I prodotti vettoriali sono identici.\\
  2:
  \begin{gather*}
      v \wedge  v = \det \begin{pmatrix}
          i & j & k \\
          v_1 & v_2 & v_3 \\
          v_1 & v_2 & v_3 
      \end{pmatrix} = (v_2 v_3 - v_3v_2)i - (v_1v_3 - v_3v_1)j + (v_1v_2 - v_2v_1)k = 0
  \end{gather*}
  3:
  \begin{gather*}
    (\lambda v + \mu v') \wedge  w = \lambda v \wedge  w + \mu v' \wedge  w
  \end{gather*}
  \begin{gather*}
      (\lambda v + \mu v') \wedge  w = \lambda \det\begin{pmatrix}
          i & j & k \\
          v_1 & v_2 & v_3 \\
          w_1 & w_2 & w_3
      \end{pmatrix} + \mu \det\begin{pmatrix}
          i & j & k \\
          v_1 & v_2 & v_3 \\
          w_1 & w_2 & w_3
      \end{pmatrix}
  \end{gather*}
  La 4 è analoga. \\
  5: Se $\{i, j, k\}$ è una base allora si considera il prodotto scalare tra due vettori generici:
  \begin{gather*}
      (ai + bj + ck, a'i + b'j + c'k) = \star
  \end{gather*}
  Che si può scomporre nella seguente maniera 
  \begin{gather*}
      \star = (ai, a'i + b'j + c'k) + (bj, a'i + b'j + c'k) + (ck, a'i + b'j + c'k)
  \end{gather*}
  E quindi è uguale per definizione di prodotto scalare standard:
  \begin{gather*}
      aa'(i, i) + bb'(j, j) + cc' (k, k)
  \end{gather*}
  Dato che tutti i prodotti scalari sono uguali ad $1$, si vuole vedere se
  \begin{gather*}
      (v \wedge w, v) = 0
  \end{gather*}
  Eseguendo il prodotto vettoriale, si conclude che
  \begin{gather*}
      (v \wedge w, v) = \left| \begin{array}{c c}
        v_2 & v_3\\
        w_2 & w_3
      \end{array} \right| v_1 - \left| \begin{array}{c c}
        v_1 & v_3 \\
        w_1 & w_3
      \end{array} \right| v_2 + \left| \begin{array}{c c}
        v_1 & v_2 \\
        w_1 & w_2
      \end{array} \right| v_3   = 0 \ \Longrightarrow \ v \wedge w \perp v \quad v \wedge w \perp w.
  \end{gather*}
  6: Si dimostra solo la prima perché le altre sono analoghe:
  \begin{gather*}
      i \wedge  j = \det\begin{pmatrix}
          i & j & k \\
          1 & 0 & 0 \\
          0 & 1 & 0
      \end{pmatrix} = k
  \end{gather*} 
  7:
  Dato $\theta$ l'angolo compreso tra i due vettori $v, w$,  
  si suppone che $v$ e $w$ siano non multipli tra loro. Si sceglie dunque $i, j, k$ nel modo seguente (con $k$ uscente dal foglio):
  \begin{gather*}
    \begin{tikzpicture}
      \draw(0, 0) -- (3, 0);
      \draw[->](1, 0) -- (1, 1) node[at end, left] {$j$};
      \draw[->](1, 0) -- (2, 0) node[at end, below]{$v$};
      \draw[->](1, 0) -- (1.5, 0) node[at end, below] {$i$};
      \draw[->](1, 0) -- (1.5, 1.25) node[at end, right] {$w$};
    \end{tikzpicture}\\
    i = \frac{v}{|v|} \ \Longrightarrow \  |i| = 1
  \end{gather*}
  Ossia $j$ appartiene al semipiano $\left< v, w \right>$ che ha come
  bordo $\left< v \right>$ e che contiene $w$. Si scelgono $i, j, k$ con norma 1. $v, w$ sono dunque  
  \begin{gather*}
      v = |v|i = |v|i + 0j + 0k \qquad
      w = w_1 i + w_2 j + 0k
  \end{gather*}
  Il prodotto vettoriale tra di loro è dato dal determinante della seguente matrice. 
  \begin{gather*}
      \det \begin{pmatrix}
          i & j & k \\
          |v| & 0 & 0 \\
          w_1 & w_2 & 0
      \end{pmatrix} = |v| w_2 k = |v| |w| \sin\theta k
  \end{gather*}
  Per le considerazioni trigonometriche, si osserva perché l'ultima uguaglianza vale:
  \begin{gather*}
      \begin{tikzpicture}
        \draw[->](0, 0) -- (4, 0) node[at end, below] {$v$};
        \draw[->](1, 0) -- (1.5, 0) node[at end, below] {$i$};
        \draw[->](1, 0) -- (1, 0.5) node[at end, left] {$j$};
        \draw[->](1, 0) -- (1, 3);
        \draw[->](2, 0) -- (2, 2.5)node[midway, right] {$w_2$};
        \draw[->](1, 0) -- (2, 2.5) node[at end, above] {$w$};
        \draw(1.5, 0) arc(0: 70: 0.5) node[midway, right] {$\theta$};
      \end{tikzpicture} \qquad \qquad
      \begin{tikzpicture}
        \draw[->](-2, 0) -- (2.5, 0) node[at end, below] {$v$};
        \draw[->](0, -1) -- (0, 2.5);
        \draw(0.5, 0) arc (0: 120: 0.5) node[midway, above] {$\theta$};
        \draw[->](0, 0) -- (-1, 2) node[at end, above] {$w$};
        \draw[|-|](0, 0) -- (0, 2) node[midway, right] {$w_2$};
      \end{tikzpicture}
  \end{gather*}
  \begin{gather*}
    w_2 = |w| \sin \theta \ \Longrightarrow \  \qquad |v\wedge w| = |v| |w| \sin\theta 
  \end{gather*}
  Ossia $w_2$ è ottenuto come la proiezione ortogonale di $w$ su $j$. 
\end{proof}


\chapter{Forme bilineari}
\section{Prime definizioni}
\begin{definition}[Bilinearità]
  Sia $V$ uno spazio vettoriale su di un campo $\mathbb{K}$ e sia $b : V \times V 
  \to \mathbb{K}$ una applicazione lineare. Dico che $b$ è bilineare (o che è una forma lineare) se valgono
  le seguenti proprietà:
  \begin{align}
      b(\lambda v + \mu v', w) = \lambda \cdot  b (v, w) + \mu b (v', w)
  \end{align}
  Ossia la linearità del primo argomento, e la linearità nel secondo argomento
  \begin{align}
      b (v, \lambda w + \mu w') = \lambda b(v, w) + \mu b(v, w')
  \end{align}
\end{definition}

\begin{example}
  Sia $b : \mathbb{R}^{2} \times \mathbb{R}^{2} \to \mathbb{R}$ definita come 
  \begin{gather*}
      b\left(\begin{array}{l}
        x_1 \\
        y_1
      \end{array}, \begin{array}{l}
        x_2 \\
        y_2
      \end{array}\right) = 2x_1 y_2 + x_1 x_2
  \end{gather*}  
  Per vedere se è bilineare devo verificare le due condizioni
  della definizione:
  \begin{gather*}
      b\left(\lambda \begin{array}{l}
        x_1 \\
        y_1
      \end{array}+ \mu \begin{array}{l}
        x_1' \\
        y_1'
      \end{array}, \begin{array}{l}
        x_2 \\
        y_2
      \end{array} \right) = \lambda b\left(\begin{array}{l}
        x_1 \\
        y_1
      \end{array}, \begin{array}{l}
        x_2 \\
        y_2
      \end{array}\right) + \mu b\left(\begin{array}{l}
        x_1' \\
        y_1'
      \end{array}, \begin{array}{l}
        x_2 \\
        y_2
      \end{array}\right)
  \end{gather*}
  Il primo membro è quindi
  \begin{gather*}
      2(\lambda x_1 + \mu x_1')y_2 + (\lambda x_1 + \mu x_1') x_2 = \star
  \end{gather*}
  mentre il secondo membro è 
  \begin{gather*}
      \lambda(2x_1 y_2 + x_1 x_2) + \mu(2x_1'y_2 + x_1'x_2) = \star 
  \end{gather*}
\end{example}

\begin{definition}[Forma bilineare simmetrica]
  Sia $V$ uno spazio vettoriale su $\mathbb{K}$. Sia $b : V \times V \to \mathbb{K}$
  un'applicazione bilineare, si dice \textbf{simmetrica} se
  \begin{align}
      b(v, w) = b(w, v) \qquad \forall v, w \in V
  \end{align}
\end{definition}

\begin{observation}
  Sia $V$ uno spazio vettoriale su di un campo $\mathbb{K}$ 
  e sia $b : V \times V \to \mathbb{K}$ una forma bilineare, allora 
  \begin{align}
      b(0_V, v) = 0 \qquad \forall v \in V \\
      b(v, 0_V) = 0 \qquad \forall v \in V
  \end{align}
\end{observation}
\begin{proof}
  Sia $v \in V$
  \begin{gather*}
      b(0_V, v) = b(0_K \cdot  0_V, v) = 0_K \cdot b(0_V, v) = 0_K
  \end{gather*} 
\end{proof}

\begin{definition}[Definizione delle forme bilineari]
  Sia $V$ uno spazio vettoriale su $\mathbb{R}$. Sia $b : V \times V \to \mathbb{R}$
  una forma bilineare.\\
  Dico che $b$ è \textbf{definita positiva} se $b(v, v) > 0$. \\
  Dico che $b$ è \textbf{definita negativa} se $b(v, v) < 0$. \\
  Dico che $b$ è \textbf{semidefinita positiva} se $b(v, v) \geq 0$. \\
  Dico che $b$ è \textbf{semidefinita negativa} se $b(v, v) \leq 0$. \\
  Dico che $b$ è \textbf{indefinita} se $b$ non è nessuna delle precedenti.
\end{definition}

\begin{observation}
  Se una forma bilineare è definita positiva allora è anche semidefinita positiva
  e se una forma bilineare è definita negativa allora è anche semidefinita negativa.
\end{observation}

\begin{definition}[Matrici associate a forme bilineari]
  Sia $V$ uno spazio vettoriale di dimensione finita e su di un campo $\mathbb{K}$,
  e sia $b$ la sua forma bilineare. Sia $B = \{v_1, \dots, v_n\}$ una sua base 
  ordinata. Definisco allora la \textbf{matrice associata a $b$ nella base $B$}
  quella matrice che, indicata con $M_{B}(b)$, è tale che:
  \begin{align}
      (M_{B}(b))_{i, j} = b(v_i, v_j) \qquad \forall i, j \in \{1, \dots, n\}
  \end{align}
\end{definition}

\begin{example}
  Sia $b : \mathbb{R}^{3} \times \mathbb{R}^{3} \to \mathbb{R}$ la forma bilineare così definita:
  \begin{gather*}
      b(x, y) = 2x_1 y_1 + x_2 y_3 + 3x_3 y_2, \qquad x, y \in \mathbb{R}^{3} 
  \end{gather*}  
  Sia allora la base
  \begin{gather*}
      \epsilon = \{e_1, e_2, e_3\}
  \end{gather*}
  La matrice associata alla base e nella funzione bilineare allora è
  \begin{gather*}
      M_{\epsilon}(b) = \begin{pmatrix}
          b(e_1, e_1) & b(e_1, e_2) & b(e_2, e_3) \\
          b(e_2, e_1) & b(e_2, e_2) & b(e_2, e_2) \\
          b(e_3, e_1) & b(e_3, e_2) & b(e_3, e_3)
      \end{pmatrix} = \begin{pmatrix}
          2 & 0 & 0 \\
          0 & 0 & 1 \\
          0 & 3 & 0
      \end{pmatrix}
  \end{gather*}
\end{example}

\begin{example}
  Sia $V$ uno spazio vettoriale di dimensione $2$ su $\mathbb{R}$ e sia la
  base $B = \{v_1, v_2\}$ una base ordinata di $V$ e sia
  $b: V \times V \to \mathbb{R}$ la forma bilineare tale che
  \begin{gather*}
      b(v_1, v_1) = 5 \qquad b(v_1, v_2) = 6 \\
      b(v_2, v_1) = 0 \qquad b(v_2, v_2) = 3
  \end{gather*}
  Possiamo allora scrivere $M_{B}(b)$  
  \begin{gather*}
      \begin{pmatrix}
          5 & 6 \\
          0 & 3
      \end{pmatrix}
  \end{gather*}
  Mentre la matrice $M_{B'}(b)$, con base $B' = \{v_1 + 2v_2, -v_1\}$
  si ricava attraverso le proprietà delle forme bilineari, oppure
  attraverso un metodo che si vedrà nella prossima proposizione.
  \begin{gather*}
      M_{B'}(b) = \begin{pmatrix}
          b(v_1 + 2v_2, v_1 + 2v_2) & b(v_1 + 2v_2, -v_1) \\
          b(-v_1, v_1 + 2v_2) & b(-v_1, -v_1)
      \end{pmatrix} = \begin{pmatrix}
          29 & -5 \\
          -17 & 5
      \end{pmatrix}
  \end{gather*}
\end{example}

\begin{lemma}[Definizione alternativa di una forma bilineare data la matrice associata]
  Sia $V$ uno spazio vettoriale di dimensione finita su di un campo $\mathbb{K}$
  e sia allora $b : V \times V \to \mathbb{K}$ una forma bilineare. Sia
  $B = \{v_1, \dots, v_n\}$ una base ordinata di $V$ e siano $v$ e $w$ due elementi di $V$, siano $x_1 \dots x_n, y_1 \dots y_m \in \mathbb{K}$ 
  tali che definiscano i seguenti vettori:
  \begin{gather*}
      v = x_1 v_1 + \dots + x_n v_n \qquad \qquad w = y_1 v_1 + \dots + y_n v_n
  \end{gather*}
  $b(v, w)$ sarà definita come
  \begin{align}
            b(v, w) =\  ^{t} x \cdot  M_{B}(b) \cdot  y
  \end{align}
  Dove $x$ ed $y$ sono le $n$-uple delle coordinate dei vettori $v$ e $w$ in $\mathbb{K}$.
\end{lemma}
\begin{proof}
  \begin{gather*}
      b(v, w) = b\left(\sum_{i =1 }^{n} x_i \cdot v_i, \sum_{j =1 }^{n} y_j \cdot v_j\right) = \\
  \end{gather*}
  per la linearità del primo argomento e del secondo argomento si ottiene 
  \begin{gather*}
      \sum_{i = 1}^{n}x_i b\left(v_j, \sum_{j = 1}^{n}y_j v_j \right) = \sum_{i = 1}^{n}x_i \sum_{j = 1}^{n}y_j b(v_i, v_j) 
  \end{gather*}
  Allora, data la definizione di matrice associata ad una forma bilineare:
  \begin{gather*}
      \sum_{i, j = 1}^{n} x_i y_j (M_{B}(b))_{i, j} = \ ^{t}x M_{B}(b) y
  \end{gather*}
  svolgendo le sommatorie si ottiene
  \begin{gather*}
    ^{t}x M_{B}(b)y = (^{t}xM_{B}(b)y)_{1,1} = \sum_{j = 1}^{n} (^{t}xM_{B}(b))_{1, j}y_{j, 1} = \\
    \sum_{i, j = 1}^{n}\  ^{t}x_{i, 1} (M_{B}(b))_{i, j} y_{j, 1} = \sum_{j = 1}^{n} \left(\sum_{i = 1}^{n} \ ^{t}x_{1, i} M_{B}(b)_{i, j}\right) y_{j, 1} =  \\
    \sum_{i = 1}^{n} \sum_{j = 1}^{n} x_{i, 1} y_{j, 1} M_{B}(b)_{i, j} =   \sum_{i, j = 1}^{n} x_i y_j (M_{B}(b))_{i, j}
  \end{gather*}
\end{proof}

\begin{corollary}[Una forma bilineare è simmetrica se e solo se la sua matrice associata è simmetrica]
  Sia $V$ uno spazio vettoriale su di un campo $\mathbb{K}$ di dimensione
  finita e sia $b : V \times V \to \mathbb{K}$ una forma bilineare. Sia allora
  $B = \{v_1, \dots, v_n\}$ una base ordinata di $V$. Allora 
  \begin{align}
      b \text{ simmetrica } \Longleftrightarrow M_{B}(b) \text{ simmetrica}
  \end{align}
\end{corollary}
\begin{proof}
  $\ \Longrightarrow \ $ Dato che $b$ è simmetrica allora si ha che
  \begin{gather*}
      b(v_i, v_j) = b(v_j, v_i) 
  \end{gather*}
  e allora, per la definizione di matrice associata ad una forma bilineare:
  \begin{gather*}
      (M_{B}(b))_{i, j} = (M_{B}(b))_{j, i}
  \end{gather*}
  $\Longleftarrow \ $ Dall'altro verso invece, supponendo che la matrice
  sia simmetrica, si vuole dimostrare 
  la proprietà della simmetria della funzione bilineare.
  Siano allora $x$ l'n-upla delle coordinate di $v$ rispetto a $B$
  e $y$ quelle di $w$. Allora per l'osservazione di prima
  \begin{gather*}
      b(v, w) = \ ^{t}xM_{B}(b) y \qquad b(w, v) = \ ^{t}yM_B(b) x   
  \end{gather*}
  Dato che sono entrambe matrici $1 \times 1$, esse, per definizione, sono invarianti
  per la trasposizione, dunque
  \begin{gather*}
      ^{t}xM_B(b) y = \ ^{t}(\ ^{t}xM_B(b)y ) = \ ^{t}y^{t}M_B(b) x     
  \end{gather*}
\end{proof}

\begin{lemma}[Matrice associata ad una forma bilineare]
  Sia $A \in M(n \times n, \mathbb{K})$, posso definire
  \begin{align}
      M_{\epsilon}(b_A) = A, \qquad \epsilon = \{e_1, e_2, \dots\}
  \end{align} 
\end{lemma}
\begin{proof}
  Infatti 
  \begin{gather*}
      (M_{\epsilon}(b_A))_{i, j} = b_A(e_i, e_j) = \ ^{t}e_i A e_j =  a_{i, j}
  \end{gather*}
  E quindi per ogni $i, j$ si ha che $(M_{\epsilon}(b_A))_{i, j} = a_{i, j}$ e
  allora si ottiene la tesi.
\end{proof}


\begin{definition}[Formula di cambio base]
  Date $B$ e $B'$ due basi ordinate di uno spazio vettoriale
  con applicazione bilineare $V$ si definisce la formula di cambio
  base per le forme bilineari come: 
  \begin{align}
      M_{B'}(b) = \ ^{t}M_{B, B'}(I_V) \cdot  M_{B}(b) \cdot  M_{B, B'}(I_V) 
  \end{align}
\end{definition}

\begin{example}[Versione alternativa dello scorso esempio]
  Sia $V$ di dimensione $2$ e sia $B = \{v_1, v_2\}$ una sua
  base ordinata di $V$. Posta la sua forma bilineare come
  \begin{gather*}
      M_B(b) = \begin{pmatrix}
          5 & 6 \\
          0 & 3
      \end{pmatrix}
  \end{gather*}
  Sia $B' = \{v_1 + 2v_2, -v_1\}$. Con la definizione l'esempio
  non avrebbe senso, quindi applichiamo la formula di cambio base: possiamo comporre
  la matrice associata alla base $M_{B, B'}(I_V)$ attraverso le coordinate di ogni vettore
  della base $B'$ rispetto ai vettori $v_1, v_2$ della base di arrivo $B$:
  \begin{gather*}
      w_1 = v_1 + 2v_2 \\
      w_2 = -v_1 + 0v_2\\
      M_{B, B'}(I_V) = \begin{pmatrix}
          1 & -1 \\
          2 & 0
      \end{pmatrix}
  \end{gather*}
  Allora con il cambiamento di base
  \begin{gather*}
      M_{B'}(b) = \begin{pmatrix}
          29 & -5 \\
          -17 & 5
      \end{pmatrix}
  \end{gather*}
\end{example}

\begin{example}[Un esempio più completo]
  Sia $b : \mathbb{R}^{3} \times \mathbb{R}^{3} \to \mathbb{R}$ la forma bilineare
  \begin{gather*}
      b\left(\begin{pmatrix} x_1\\
      x_2\\
      x_3 \end{pmatrix}, \begin{pmatrix} y_1\\
      y_2\\
      y_3 \end{pmatrix}  \right) = x_1y_1 + 2x_2y_3 + 2x_3y_2
  \end{gather*}  
  Allora applicando la definizione, data la matrice associata a questa
  forma bilineare, posso ottenere la matrice rispetto alla base
  \begin{gather*}
      B = \left\{\begin{pmatrix} 1\\
      2\\
      0 \end{pmatrix}, \begin{pmatrix} 0\\
      0\\
      1 \end{pmatrix}, \begin{pmatrix} 3\\
      1\\
      0 \end{pmatrix}   \right\}
  \end{gather*}
  E' allora, secondo la definizione:
  \begin{gather*}
      M_{\epsilon}(b) = \begin{pmatrix}
          1 & 0 & 0 \\
          0 & 0 & 2 \\
          0 & 2 & 0
      \end{pmatrix} \qquad M_{B}(b) = \begin{pmatrix}
          1 & 4 & 3 \\
          4 & 0 & 2 \\
          3 & 2 & 9
      \end{pmatrix}
  \end{gather*}
  Potremmo ricavare la matrice associata alla base $B$ anche secondo la formula di cambio
  base: infatti, possiamo creare la matrice associata alla base $M_{B, B}(I_V)$ ottenuta 
  ponendo fianco a fianco i vettori della base e moltiplicandola con la matrice
  associata alla canonica e a quella associata alla base $B$ ma trasposta:
  \begin{gather*}
    M_{B}(b) = \ ^{t}M_{B, B'}(I_V) M_{B}(b) M_{B, B'}(I_V) \\
      \begin{pmatrix}
          1 & 4 & 3 \\
          4 & 0 & 2 \\
          3 & 2 & 9
      \end{pmatrix} = \begin{pmatrix}
          1 & 2 & 0 \\
          0 & 0 & 1 \\
          3 & 1 & 0
      \end{pmatrix} \begin{pmatrix}
          1 & 0 & 0 \\
          0 & 0 & 2 \\
          0 & 2 & 0
      \end{pmatrix} \begin{pmatrix}
          1 & 0 & 3 \\
          2 & 0 & 1 \\
          0 & 1 & 0
      \end{pmatrix}
  \end{gather*}
\end{example}


\section{I teoremi di Gram-Schmidt}
\begin{definition}[basi ortogonali ed ortonormali]
  Sia $V$ uno spazio vettoriale su di un campo $\mathbb{K}$, e sia
  $B = \{v_1, \dots, v_n\}$ una sua base ordinata.
  Si dice allora che è \textbf{ortogonale} per $b$ se $M_B(b)$ è
  diagonale, altrimenti dico che $B$ è \textbf{ortonormale} per $b$
  se $M_B(b)$ è diagonale e se $b(v_i, v_j) = \delta_{i, j}$, in altre parole $M_B(b) = I_n$. 
\end{definition}

\begin{theorem}[Primo teorema di Gram-Schmidt]
  Sia $V$ uno spazio vettoriale su di $\mathbb{R}$ con dimensione
  finita e sia $b : V \times V \to \mathbb{R}$ la bilineare simmetrica
  e definita positiva. Allora esiste una base ordinata $B=\{w_1, \dots, w_n\}$ 
  tale che
  \begin{align}
      b(w_i, w_j) = \delta_{i, j} = \left\{\begin{array}{l}
        1 \quad \text{se } i = j \\
        0 \quad \text{se } i \neq j
      \end{array}\right.
  \end{align}
  E quindi $M_B(b) = I_n$, ossia la matrice $B$ è composta da vettori ortonormali tra loro.
\end{theorem}

\begin{proof}
  Per induzione su $n$:
  per $n = 1$  so che $\{v\}$ è una base qualsiasi $V$
  e allora posso definire 
  \begin{gather*}
      v' = \frac{v}{\sqrt{b(v, v)} }
  \end{gather*}
  Dove $\sqrt{b(v ,v)}$ è la norma per il vettore $v$ della forma bilineare
  associata $b$, dunque
  \begin{gather*}
      b(v', v') = \frac{1}{b(v, v)} \cdot  b(v, v) = 1
  \end{gather*}
  allora posso dire che $B = \{v'\}$ è una base tale che $M_B(b) = I_1 = 1$. \\
  Si dimostra ora il passo induttivo $n - 1 \ \Longrightarrow \  n$. \\
  Diamo per buono l'enunciato per spazi vettoriali di dimensioni $n - 1$, si dimostra per 
  $V$ di dimensione $n$: sia $b : V \times V \to \mathbb{R}$ una forma bilineare
  simmetrica definita positivamente e sia $\{v_1, \dots, v_n\}$ una base qualsiasi di $V$, allora si definisce
  normalizza il primo vettore della base di $V$
  \begin{gather*}
      v_1' = \frac{1}{\sqrt{b(v_1, v_1)} }v_1
  \end{gather*}
  secondo la norma di $v$ per $b$, ossia $\sqrt{b(v_1, v_1)}$, si può provare che il vettore è stato ortonormalizzato
  attraverso la segunte espressione:
  \begin{gather*}
      b(v_1', v_1') = b\left(\frac{v_1}{\sqrt{b(v_1, v_1)}}, \frac{v_1}{\sqrt{b(v_1, v_1)}}\right) = 1
  \end{gather*} 
  Che sarebbe valida anche se venisse fatto lo stesso procedimento agli
  altri vettori della base. Adesso posso esprimere gli altri vettori $v_i'$ come i vettori $v_i$ a cui viene tolta la proiezione ortogonale su $v_1$:
  \begin{gather*}
      v_i' = v_i - \frac{b(v_i, v_1)}{b(v_1, v_1)}v_1.
  \end{gather*}
  \fbox{
    \begin{minipage}{\linewidth}
        Se $b$ fosse il prodotto scalare standard si avrebbe che
  \begin{gather*}
      \frac{b(v_i, v_1)}{b(v_1, v_1)}v_1 = |v_i| |v_1|\cos\theta \frac{v_1}{|v_1|^{2} }
  \end{gather*}
    \end{minipage}
  }
  Ovviamente, per come sono stati definiti, il prodotto scalare tra $v_i, v_1$ è uguale a zero:
  \begin{gather*}
      b(v_i', v_1) = b\left(v_i - \frac{b(v_i, v_1)}{b(v_1, v_1)}v_1, v_1\right) = b(v_i, v_1) + b\left(-\frac{b(v_i, v_1)}{b(v_1, v_1)}v_1, v_1\right) = \\
      b(v_i, v_1) - \frac{b(v_i, v_1)}{b(v_1, v_1)}b(v_1, v_1) = 0
  \end{gather*}
  Si è dunque ottenuta $\{v_1', v_2', \dots, v_n'\}$, dove $v_1'$ è ortonormale agli altri. Adesso anch'essa è un insieme di generatori e quindi una base di $V$.
  Adesso si riduce la complessità del problema. A livello pratico, si dovrebbe ripetere il procedimento di ortonormalizzazione
  ma, dato che si è dato per scontato il passo induttivo, si considera la seguente base che genera $W$: $W = \left<v_2', \dots, v_n'\right>$, ossia 
  genera un sottospazio vettoriale di $V$ di dimensione $n - 1$. Si può ora considerare
  \begin{gather*}
      b_{W \times W} : W \times W \to \mathbb{R}
  \end{gather*}
  che è bilineare, simmetrica e definita positivamente. Allora per ipotesi induttiva esiste una base
  $\{v_2'', \dots, v_n''\}$ di $V$ ortonormale per $b_{W \times W}$, i quali sono stati ottenuti
  da $\{v_2', \dots, v_n'\}$ mediante lo stesso processo di ortonormalizzazione. Posto allora $v \in V$; esso si può scrivere come
  una combinazione lineare di $v_1', \dots , v_n'$ e allora posso dire
  \begin{gather*}
      v = x_1 v_1' + \underset{\in W}{\underline{x_2 v_2' + \dots + x_n v_n'}}
  \end{gather*} 
  Per qualche $y_2, \dots, y_n \in \mathbb{R}$ a 
  \begin{gather*}
      v = x_1 v_1' + \underset{\in W}{\underline{x_2 v_2' + \dots + x_n v_n'}} = x_1 v_1' + y_2 v_2'' + \dots + y_n v_n'' 
  \end{gather*}
  E allora posso definire la seguente base
  \begin{gather*}
      B = \{v_1', v_2'', \dots, v_n''\}
  \end{gather*}
  che è un insieme di generatori per $V$ per il passo induttivo della dimostrazione. Voglio dimostrare che i vettori
  di questa base siano ortonormali tra loro per $b$:
  \begin{itemize}
      \item $b(v_1', v_1') = 1$: che si è già calcolato definendo $v_1'$;
      \item $b(v_i'', v_i'') = 1 \quad \forall i = \{2, \dots, n\}$: perché $\{v_2'', \dots, v_n''\}$ è ortonormale a $b_{W \times W} : W \times W \to \mathbb{R}$, da cui
      si ottiene anche che $\forall i, j \in \{2, \dots, n\}, \ i \neq j \ \Longrightarrow \  b(v_i'', v_j'') = 0$.
  \end{itemize}
  Non resta ora che dimostrare che $b(v_1', v_i'') = 0, \forall i = \{2, \dots, n\}$, con $v_i'' \in \left<v_2', \dots, v_n'\right>$,
  per fare in modo che $B$ sia la base richiesta dalla tesi del teorema. 
  Dato che $b(v_1, v_1') = 0$, si ha che ogni elemento di $W$ è ortogonale a $v_1$ e quindi è anche
  ortogonale a $v_2'$ per $b$. Posso dunque definire i vettori $v_i''$ rispetto alla base $\{v_2', \dots, v_n'\}$: 
  \begin{gather*}
      v_i'' = \lambda_2 v_2' + \dots + \lambda_n v_n' 
  \end{gather*}
  Dunque devo dimostrare che i vettori $v_1'$ e $v_i''$ siano ortogonali tra loro
  in modo tale da poter dimostrare che esista la base ortonormale di vettori
  tale per cui si verifica il teorema. Calcolando la forma bilineare per i vettori $v_1', v_i''$ si ha che:
  \begin{gather*}
      b(v_1', v_i'') = b\left(\frac{v_1}{\sqrt{b(v_1, v_1)} }, \lambda_2 v_2' + \dots + \lambda_n v_n'\right) = \\
      \frac{1}{\sqrt{b(v_1, v_1)} } b(v_1, \lambda_2 v_2' + \dots + \lambda_n v_n') = \frac{1}{\sqrt{b(v, v)} } b(v_1, \lambda_2 v_2' + \dots + \lambda_n v_n') = \\
      \frac{1}{\sqrt{b(v_1, v_1)} } \left(\lambda_2 b(v_1, v_2') + \dots \lambda_n b(v_1, v_n')\right) = 0 
  \end{gather*}
  La quale è vera perché tutti i $b(v_i', v_1)$ sono equivalenti a zero per come sono stati 
  definiti i vettori $v_i'$. Allora è dimostrato che la base considerata $B$ è una base 
  ortonormale tale per cui si verifica il teorema.
\end{proof}

\begin{definition}[Isotropo]
  Sia $V$ uno spazio vettoriale su di un campo $\mathbb{K}$, e sia
  $b : V \times V \to \mathbb{K}$ la forma bilineare. Sia allora
  $v \in V$. Dico allora che $v$ è \textbf{isotropo} per $b$ se
  e solo se
  \begin{align}
      b(v, v) = 0
  \end{align}
\end{definition}

\clearpage
\begin{theorem}[Secondo teorema di Gram-Schmidt]
  Sia $\mathbb{K}$ un campo tale che $1_K + 1_K \neq 0$. Sia 
  $V$ uno spazio vettoriale su di $\mathbb{K}$ di dimensione finita $n$. \\
  Sia $b : V \times V \to \mathbb{K}$ una forma bilineare simmetrica, allora $\exists B$
  base ordinata di $V$ ortogonale per $b$, ossia tale che
  $M_B(b)$ sia diagonale. 
\end{theorem}
\begin{proof}
  Distinguo due casi:
  \begin{enumerate}
      \item Tutti gli elementi di $V$ sono isotropi per $b$;
      \item $\exists$ un elemento di $V$ non isotropo per $b$.
  \end{enumerate}
  Nel primo caso so che $b(v, v) = 0, \forall v \in V$, ossia ogni 
  elemento di $V$ è isotropo per $b$.  Si vuole
  allora dimostrare che $b(v, w) = 0, \forall v, w \in V$. Siano
  quindi $v, w \in V$ e calcoliamo allora $b(v + w, v + w)$:
  \begin{gather*}
      0 = b(v + w, v + w) = b(v, v + w) + b(w, v + w) = b(v, v) + b(v, w) + b(w, v) + b(w, w)
  \end{gather*}
  Dato che $b$ è simmetrica, è equivalente a $b(v, w) + b(v, w)$. Dato che deve essere uguale a zero
  allora si ottiene che
  \begin{gather*}
      (1_K + 1_K) \cdot  b(v, w) = 0_K
  \end{gather*}
  Moltiplico allora entrambi i membri per $(1_K + 1_K)^{-1}$
  ottenendo allora
  \begin{gather*}
      1_K b(v, w) = 0_K \ \Longrightarrow \  b(v, w) = 0_K
  \end{gather*}
  e allora so che per ogni base ho che $M_B(b)$ è la matrice nulla in 
  particolare è diagonale. \\
  Nel secondo caso invece, sapendo che esiste un elemento di $V$ non isotropo,
  lo chiamo $v_1$ e allora ottengo che $b(v_1, v_1) \neq  0$.
  Ragionando allora per induzione su $n$ posso dire che, sapendo che
  $b(v_1, v_1) \neq  0$ e quindi $v_1 \neq 0_V$, possiamo allora completare
  $\{v_1\}$ ad una base di $V$:
  \begin{gather*}
      \{v_1, v_2, \dots, v_n\}
  \end{gather*}
  A questo punto si può ragionare applicando Gram-Schmidt 1, che ci permette di ottenere
  una base tale per cui $M_B(b)$ è diagonale e anche ortogonale nell'ipotesi in cui
  solo un elemento di $V$ non è isotropo. 
  Per $i > 1$ si ottiene
  \begin{gather*}
      v_i' = v_i - \frac{b(v_i, v_1)}{b(v_1, v_1)}v_1
  \end{gather*}
  Ho che $b(v_i', v_1) = 0$ e infatti $\{v_1, v_2', \dots, v_n'\}$ si
  ottiene proprio da $\{v_1, \dots, v_n\}$ con operazioni elementari
  e quindi questo è di nuovo un insieme di generatori per $V$ e allora una base.
  Definisco allora $W = \left<v_2', \dots, v_n'\right>$ come sottospazio di $V$ di dimensione
  $n - 1$ considerando che $b|_{W \times W}$ è bilineare simmetrica e quindi per
  ipotesi induttiva $\exists$ una base di $W, \{v_2'', \dots, v_n''\}$ ortogonale
  alla forma bilineare considerata allora la base:
  \begin{gather*}
      B = \{v_1, v_2'', \dots, v_n''\}
  \end{gather*}  
  affermo che è una base di $V$ ortogonale per $b$. Vediamo
  quindi che $B$ è una base di $V$ e sia $v \in V: v = x_1v_1 \underline{+ x_2v_2' + \dots + x_nv_n'}$ 
  la parte sottostimata $\in W$ quindi la posso scrivere come combinazione
  lineare di $v_2'', \dots, v_n''$ e quindi $v$ la posso scrivere come combinazione
  lineare di quei vettori più $v_1$ e quindi  $B$ genera $V$, quindi
  è una base di $V$  e vediamo che la base è ortogonale 
  per $b$:
  \begin{gather*}
      b(v_i'', v_j'') = 0, \forall i, j \in \{2, \dots, n\}
  \end{gather*}
  Distinti poiché $\{v_2'', \dots, v_n''\}$ è ortogonale all'applicazione
  bilineare  e deve anche valere $b(v_1, v_i'') = 0$ con $i = \{2, \dots, n\}$ e 
  \begin{gather*}
      v_i'' \in W = \left<v_2', \dots, v_n'\right> \qquad b(v_1, v_i') = 0 \ \Longrightarrow \ b(v_1, w) = 0 \qquad \forall w \in W
  \end{gather*}
  In particolare $b(v_1, v_i'') = 0$ e quindi è dimostrata.
\end{proof}

\clearpage
\begin{theorem}[3° teorema di Gram-Schmidt]
  Sia $V$ uno spazio vettoriale su di $\mathbb{R}$ di dimensione
  finita $n$ e sia $b : V \times V \to \mathbb{R}$ una forma bilineare simmetrica,
  allora esiste una base $B$ di $V$ tale che $M_B(b)$ è diagonale
  e tutti gli elementi della diagonale di $M_B(b)$ appartengono a $\{1, -1, 0\}$.
\end{theorem}
\begin{proof}
  per il secondo teorema di Gram-Schmidt esiste una base di $V$
  \begin{gather*}
      \{v_1, \dots, v_n\}
  \end{gather*}
  ortogonale per $b$ così tale che la matrice associata sia diagonale
  ossia tale che $b(v_i, v_j) = 0, \forall i \neq j$. Definisco
  \begin{gather*}
      w_i = \left\{\begin{array}{l}
          \frac{v_i}{\sqrt{b(v_i, v_i)} } \qquad  b(v_i, v_i) > 0 \\
          \frac{v_i}{\sqrt{-b(v_i, v_i)} } \qquad b(v_i, v_i) < 0 \\
          v_i, \qquad \qquad  \ b(v_i, v_i) = 0
      \end{array}\right.
  \end{gather*}
  Se $i \neq j, b(w_i, w_j) = \text{elementi di } \mathbb{R} \cdot  b(v_i, v_j) = 0$. \\
  Si osserva inoltre che $\{w_1, \dots, w_n\}$ è ancora una base di $V$ e
  voglio vedere che se $b(w_i, w_i) \in \{0, 1, -1\}$: 
  Se $b(v_i, v_i) > 0$ allora $b(w_i, w_i) = 1$. \\
  Se $b(v_i, v_i) < 0$ allora $b(w_i, w_i) = -1$. \\
  Se $b(v_i, v_i) = 0$ allora $b(w_i, w_i) = 0$. \\
  Che si ottengono semplicemente applicando alla forma bilineare $b$ la 
  definizione di $w_i$. 
  Quindi $B'' = \{w_1, \dots, w_n\}$ è una base di $V$ ortogonale
  per $b$ e gli elementi sulla diagonale di $M_{B''}(b)$ appartengono
  all'insieme $\{1, -1, 0\}$, come volevasi dimostrare.
\end{proof}


\section{Segnatura}
\begin{definition}[Positività, negatività e segnatura]
  Sia $V$ uno spazio vettoriale su $\mathbb{R}$ di dimensione finita. 
  Sia $b : V \times V \to \mathbb{R}$ un'applicazione bilineare simmetrica. Sia allora $B$ una base di $V$ tale che $M_B(b)$ è diagonale e gli elementi della diagonale
  siano $\in \{1, -1, 0\}$, allora per il teorema di Gram-Schmidt $3$ esiste questa base e quindi 
  posso definire gli elementi sulla diagonale come:
  \begin{align}
      p &= \text{numero degli elementi sulla diagonale uguali a} = 1 \\
      \nu &= \text{numero degli elementi sulla diagonale uguali a } = -1
  \end{align}
  Chiamo allora $p$ positività di $b$ e $\nu$ negatività di $b$ e la coppia
  $(p, \nu)$ \textbf{segnatura} di $b$.
\end{definition}

\begin{definition}[Rango di una forma bilineare]
  Sia $V$ uno spazio vettoriale su $\mathbb{R}$ di dimensione finita e sia
  $b : V \times V \to \mathbb{K}$ un'applicazione bilineare. Definisco allora
  il rango di $b$ come $rk(b)$ ossia il rango della matrice
  $M_B(b)$ dove $B$ è una base qualsiasi di $V$. 
\end{definition}

\begin{observation}[Bontà della definizione del rango]
  Se $B$ e $B'$ sono due basi di $V$, sappiamo allora che
  \begin{gather*}
      M_{B'}(b) = \ ^{t}M_{B, B'}(I_V) \cdot  M_{B}(b) \cdot  M_{B, B'} (I_V) 
  \end{gather*}
  Allora
  \begin{align}
      rkM_{B'}(b) = rkM_B(b) 
  \end{align}
  Quindi la definizione di $rk(b)$ è una buona definizione.
\end{observation}

\begin{theorem}[Teorema di Sylvester (o inerzia): la segnatura di una forma bilineare non dipende dalla base scelta]
  Sia $V$ uno spazio vettoriale su $\mathbb{R}$ di dimensione finita $n$. Sia 
  allora $b : V \times V \to \mathbb{R}$ bilineare simmetrica e siano $B$ e $B'$ due basi ordinate di $V$
  tali che $M_B(b)$ e $M_{B'}(b)$ siano diagonali e gli elementi sulla diagonale
  di $M_{B}(b)$ e sulla diagonale di $M_{B'}(b)$ siano tutti $1, -1, 0$. Allora il numero degli elementi sulla diagonale di $M_B(b)$ uguali ad $1$ è uguale al numero degli elementi sulla
  diagonale di $M_{B'}(b)$ uguali ad $1$ e così per tutti gli altri casi. 
\end{theorem}

\begin{proof}
  Sia $B = \{v_1, \dots, v_n\}$ e sia $B' = \{w_1, \dots, w_n\}$ 
  \begin{gather*}
      M_B(b) = \begin{pmatrix}
          1 & \dots & 0 \\
          \vdots & \dots & \vdots \\
          0 & \dots & -1 
      \end{pmatrix} \qquad \qquad       M_{B'}(b) = \begin{pmatrix}
          1 & \dots & 0 \\
          \vdots & \dots & \vdots \\
          0 & \dots & -1 
      \end{pmatrix}
  \end{gather*}
  Sia adesso $p$ il numero di elementi $=1$ sulla diagonale di $M_B(b)$ e $p'$ lo stesso
  per $M_{B'}(b)$. 
  Ragionando per assurdo, se si avesse che $p \neq  p'$, come per esempio $p > p'$, 
  si potrebbe dire che i vettori da 1 a $p$ generano uno spazio vettoriale
  così come i vettori da $p'$ a $n$: 
  \begin{gather*}
      U = \{v_1, \dots, v_p\} \qquad
      W = \{w_{p' + 1}, \dots, w_n\}
  \end{gather*} 
  Questi avrebbero quindi $\dim (U) = p$ e $\dim (W)= n - p'$, per la formula di Grassmann si ha:
  \begin{gather*}
      \dim(U \cap  W) = \dim(U) + \dim(W) - \dim(U + W) > 0 \\
      p + n - p' - \underset{= n}{\dim(U + W)} \geq p - p' > 0 
  \end{gather*}
  Ho dimostrato allora che $\dim(U \cap W) \geq p - p' > 0$. 
  Allora deve necessariamente $\exists v \in U \cap W$ tale che $v \neq  0_V$: calcoliamo allora
  $b(v, v)$ e, sapendo che $v \in U$ e $v \neq 0_V$, allora $\exists x_1, \dots, x_p \in \mathbb{R}$ non tutti nulli
  tali che 
  \begin{gather*}
      v = \sum_{i = 1}^{p} x_i v_i
  \end{gather*}
  allora
  \begin{gather*}
      b(v, v) = b \left(\sum_{i = 1}^{p} x_i v_i, \sum_{i = 1}^{p} x_j v_j \right) = \sum_{i = 1}^{p} x_i b\left(v_i, \sum_{j = 1}^{p} x_j v_j\right) = \\
      \sum_{i = 1}^{p} x_i \sum_{j = 1}^{p} x_j b(v_i, v_j) = \sum_{i, j = 1}^{p} x_i \cdot  x_j b (v_i, v_j) = \\
      M_B(b) = \begin{pmatrix}
          1 & & 0 \\
          & \dots & 0 \\
          0 & & 0 / -1
      \end{pmatrix} = \\
      \sum_{i = 1}^{p} x_i^{2} b(v_i , v_i) > 0\  \Longleftrightarrow  \ b(v_i, v_i) > 0.
  \end{gather*}
  Dato che $b(v_i, v_i) = 1$ dai teoremi di Gram Schmidt, allora vale l'ultima implicazione. Adesso,
  dato che $v$ appartiene all'intersezione, esso si esprimerà anche attraverso la base di $W$, quindi $\exists y_i \in \mathbb{R} :$
  \begin{gather*}
      v = \sum_{i = p' + 1}^{n} y_i w_i \\
      b(v, v) = b\left(\sum_{i = p' + 1}^{n} y_i w_i, \sum_{j = p' + 1}^{n} y_j w_j\right) = \sum_{i, j = p' + 1}^{n} y_i y_j b(w_i, w_j) = \\
      M_{B'}(b) = \begin{pmatrix}
          1 & & 0 \\
          & \dots & 0 \\
          0 & & 0 / -1
      \end{pmatrix} = \sum_{i = p' + 1}^{n} y_i^{2} b(w_i, w_i) \leq 0  
  \end{gather*} 
  L'ultima uguaglianza vale perché $b(w_i, w_j) = 0$ quando $i \neq j$ e la disuguaglianza
  vale perché $b(w_i, w_i) \in \{0, -1\}$ per $i = \{p' + 1, \dots, n\}$. Dunque per gli inidici $i = \{p' + 1, \dots, n\}$,
  visto che sarebbe una moltiplicazione di un numero 
  positivo con un numero negativo che deve essere minore di zero, si arriva ad un assurdo e dunque si deve
  necessariamente avere che $p = p'$. E quindi la positività è ben definita, e dato che
  $p + \nu = rk$ anche il rango è ben definito allora lo è anche
  la negatività secondo una dimostrazione del tutto analoga.
\end{proof}

\begin{observation}[Il segno del determinante non dipende dalla base associata]
  Sia $V$ uno spazio vettoriale $\mathbb{R}$ di dimensione finita e sia 
  $b : V \times V \to \mathbb{R}$ un applicazione bilineare. Allora il segno del
  $\det M_{B}(b )$ non dipende da $B$ base di $V$; ossia se $B$ e $B'$ sono due
  basi di $V$:
  \begin{gather*}
    \det M_{B}(b ) > 0 \Longleftrightarrow  \det M_{B'}(b ) > 0 \\
    \det M_{B}(b ) < 0 \Longleftrightarrow  \det M_{B}(b ) < 0 \\
    \det M_{B}(b ) = 0 \Longleftrightarrow  \det M_{B}(b ) = 0
  \end{gather*}
\end{observation}
\begin{proof}
  Segue dalla formula di cambio base per cui si ha che:
  \begin{gather*}
      M_{B'}(b) = \ ^{t}M_{B, B'}(I_V) M_B(b) M_{B, B'}(I_V) \ \Longrightarrow \  \\
      \det M_{B'}(b) = \det M_{B}(b) \cdot (\det(M_{B, B'}(I_V)))^{2} 
  \end{gather*}
  E quindi il segno del determinante non cambia al cambiare di $B$.
\end{proof}

\begin{observation}[Il segno del determinante determina la segnatura]
  Sia $V$ uno spazio vettoriale su $\mathbb{R}$ di dimensione finita e sia
  $b : V \times V \to \mathbb{R}$ un'applicazione bilineare di segantura $(p, \nu)$ 
  e sia $A$ una base qualsiasi di $V$:
  \begin{align}
      \det(M_A(b)) &> 0 \ \Longrightarrow \  p + \nu = n, \nu \text{ pari} \\
      \det(M_A(b)) &< 0 \ \Longrightarrow \  p + \nu = n, \nu \text{ dispari} \\
      \det(M_A(b)) &= 0 \ \Longrightarrow \  p + \nu < n, 
  \end{align} 
\end{observation}
\begin{proof}
  Sia $B$ una base tale che
  \begin{gather*}
      M_B(b) = \begin{pmatrix}
          & & 0 \\
          & \pm 1 & \\
          0 & & 
      \end{pmatrix}
  \end{gather*}
  1): Se $\det M_A(b) > 0 $, anche $\det M_B(b) > 0$, infatti,
  il numero di $\nu$ è pari poiché il determinante è positivo. \\
  2): Se $\det M_A(b) < 0$, anche $\det M_B(b) < 0$, infatti 
  il numero di $\nu$ è dispari poiché il determinante è negativo. \\
  3): Se $\det M_A(b) = 0$ anche $\det M_B(b) = 0$, per le proprietà del determinante
  le due matrice devono avere almeno una riga nulla, dunque $p + \nu < n$. 
\end{proof}

\begin{observation}
  Sia $V$ uno spazio vettoriale su $\mathbb{R}$ di dimensione finita 
  e sia $b : V \times V \to \mathbb{R}$ bilineare simmetrica e di segnatura $(p, \nu)$ 
  allora
  \begin{align}
      b > 0 &\Longleftrightarrow (p, \nu) = (n, 0) \\
      b < 0 &\Longleftrightarrow (p, \nu) = (0, n) \\
      b \geq 0 &\Longleftrightarrow \nu = 0 \\
      b \leq 0 &\Longleftrightarrow p = 0
  \end{align}
\end{observation}
\begin{proof}
  Sia $B$ una base di $V$ tale che sulla diagonale ci siano solo elementi
  $\in \{1, -1, 0\}$ provo la prima proposizione (le altre sono uguali): \\
  $\ \Longrightarrow \ $Se $b$ è definita positiva, $b(v_i, v_i) > 0$ e allora
  si può dire che
  \begin{gather*}
      (M_B(b))_{i, i} > 0, \qquad i = 1, \dots, n
  \end{gather*}
  E allora avrà tutti elementi $\in \{1, -1, 0\}$ sulla diagonale principale e quindi
  \begin{gather*}
      p = n \\
      \nu = 0
  \end{gather*}
  $\ \Longleftarrow \ $Supponendo che $p = n$ e $\nu = 0$, allora ha tutti gli
  elementi della diagonale $\in \{1, -1, 0\}$ e $v \in V$. Sia
  $x$ l'$n$-upla delle coordinate di $v$ rispetto a $B$:
  \begin{gather*}
      b(v, v) = \ ^{t}x M_B(b)x = \ ^{t}x I x = \ ^{t}x x = \sum_{i = 1}^{n} x_i^{2}      
  \end{gather*}
  E' maggiore di zero se $v \neq  0$ e allora $b$ è sicuramente definita positivamente. \\
  Le altre si dimostrano in modo analogo. 
\end{proof}

\begin{theorem}[Criterio dei minori principali]
  Sia $V$ spazio vettoriale su $\mathbb{R}$ e sia $b : V \times V \to \mathbb{R}$ bilineare
  simmetrica e sia $B$ una base ordinata di $V$.
  \begin{align}
      b \text{ definita positiva } &\Longleftrightarrow  \det M_B(b)^{1, \dots, i}_{1, \dots, i }  > 0 \\
      b \text{ semidefinita positiva } &\Longleftrightarrow \det\left(M_B(b)^{i_1, \dots, i_n}_{i_1, \dots, i_n} \right) \geq 0
  \end{align}
  Ossia per ogni combinazione possibile di sottomatrice. La rappresentazione
  del criterio è la seguente:
  \begin{gather*}
        \begin{tikzpicture}
              \draw(0, 0) rectangle (0.5, -0.5);
              \draw(0, 0) rectangle (1, -1);
              \draw(0, 0) rectangle (1.5, -1.5);
          \end{tikzpicture}
  \end{gather*}
\end{theorem}

  \begin{example}[Applicazioni dei minori principali]
    $V$ uno spazio vettoriale di $\dim = 3$ su di $\mathbb{R}$ e sia
    $b : V \times V \to \mathbb{R}$ un'applicazione bilineare simmetrica con
    $B$ una base di $V$. Supponendo che
    \begin{gather*}
        M_B(b) = \begin{pmatrix}
            1 & 3 & 2 \\
            3 & 2 & 5 \\
            2 & 5 & 1
        \end{pmatrix}
    \end{gather*} 
    $b$ è definita positivamente se e solo se vale che il determinante di
    tutte le sottomatrici è maggiore stretto di zero e si osserva che 
    non lo è e quindi non è definita positivamente. Non è neanche semidefinita positivamente
    poiché una delle sottomatrici ossia $\begin{pmatrix}
        1 & 3\\
        3 & 2
    \end{pmatrix}$ ha determinante negativo. 
\end{example}

\begin{observation}[La segnatura di una matrice a blocchi è la somma dei blocchi]
  Sia $V$ uno spazio vettoriale su di $\mathbb{R}$ di dimensione finita e sia
  $b : V \times V \to \mathbb{R}$ un'applicazione bilineare simmetrica di 
  segnatura $(p, \nu)$; sia $B$ una base ordinata di $V$ tale che
  \begin{gather*}
      M_B(b) = \begin{pmatrix}
          \begin{tabular}{c  c}
            R & 0 \\
            0 & S
          \end{tabular}
      \end{pmatrix} \qquad \begin{array}{l}
        R \in M(k \times k, \mathbb{R}) \\
        S \in M((n - k) \times (n - k), \mathbb{R})
      \end{array}
  \end{gather*}
  Sia $V' = \left< v_1, \dots, v_k \right>$, $V'' = \left< v_{k + 1}, \dots, v_n \right>$. Posta $(p', \nu')$ la segnatura di $b_{V'\times V'}$ e $(p'', \nu'')$ la segnatura di $ b_{V'' \times V''}$, si ha
\begin{align}
  (p, \nu) = (p', \nu') + (p'', \nu'')    
\end{align}
\end{observation}
\begin{proof}
  Possiamo trovare una base $A'$ di $V'$ tale che 
  $M_{A'}(b_{V' \times V'})$ sia diagonale con elementi sulla diagonale
  appartenenti all'insieme $\{0, 1, -1\}$ e una base di $A''$
  di $V''$ tale che $M_{A''}(b_{V'' \times V''})$ sia diagonale con elementi
  sulla diagonale appartenenti a $\{0, 1, -1\}$. \\
  Unendo le due basi otteniamo una base $A$ di $V$ tale che $M_A(b)$ sia diagonale con
  elementi sulla diagonale $\{0, 1, -1\}$. Essendo che i blocchi sono
  associati alle basi $A'$ e $A''$ allora la matrice associata
  alla base $A$ è la somma della segnatura delle due matrici.
\end{proof}

\begin{theorem}[]
  Sia $V$ uno spazio vettoriale su $\mathbb{R}$ di dimensione finita. Sia
  $b : V \times V \to \mathbb{R}$ un'applicazione bilineare simmetrica di segnatura $(p, \nu)$. allora
  \begin{align}
      p = \max \left\{\dim(U) : \begin{array}{l}
        U \text{ sottospazio vettoriale
        di V tale che} \\
        b_{U \times U} \text{definita positivamente}
      \end{array}\right\}
  \end{align}
\end{theorem}
\begin{proof}
  Negli appunti della Rubei non c'è, forse non è da fare?? Sennò libro pag 188. 
\end{proof}

\begin{definition}[Ortogonalmente simili]
  Sia $A, B \in M(n \times n, \mathbb{R})$. Si dicono allora
  \textbf{ortogonalmente simili} se $\exists Q \in M(n \times n, \mathbb{R})$
  ortogonale tale che 
  \begin{align}
      A = Q^{-1} BQ
  \end{align} 
\end{definition}

\begin{lemma}
  Sia $Q \in M(n \times n, \mathbb{R})$, allora si dice che 
  \begin{align}
      Q \text{ ortogonale} \quad \Longleftrightarrow  \quad (Q^{(i)}, Q^{(j)}) = \delta_{i, j} \quad \forall i, j \in \{1, \dots, n\}
  \end{align}
\end{lemma} 
\begin{proof}
  Dato che $Q$ è ortogonale allora so che
  \begin{gather*}
      ^{t}QQ = I \Longleftrightarrow (^{t}QQ)_{i, j} = (I)_{i, j} \\
      \Longleftrightarrow (^{t}Q )_{(i)} \cdot  Q^{(j)} = \delta_{i, j} \\
      \Longleftrightarrow (Q^{(i)}, Q^{(j)}  ) = \delta_{i, j}  
  \end{gather*}
\end{proof}

\begin{observation}[Trovare la matrice $Q$ ortogonale]
    Sia $A \in M(n \times n, \mathbb{R}) $ simmetrica, come si fa a trovare una matrice
$Q \in M (n \times n, \mathbb{R})$ ortogonale tale che la matrice $^{t}QAQ$ sia diagonale? 
Dobbiamo, per ciascun autovalore di $A$, cercare una base ortonormale per il prodotto scalare del relativo autospazio e formo
$Q$ affiancando tali basi. 

\end{observation}

\begin{example}
  \begin{gather*}
      A = \begin{pmatrix}
          4 & 4 \\
          6 & 2
      \end{pmatrix}
  \end{gather*}
  ha come autovalori $\lambda_1 = 8$ e $\lambda_2 = 2$.  
  Dunque i due autospazi sono i seguenti:
  \begin{gather*}
      \ker \begin{pmatrix}
          -4 & 4 \\
          6 & -6
      \end{pmatrix} = \left< \begin{pmatrix} 1 \\
      1 \end{pmatrix} \right>  \\
      \ker \begin{pmatrix}
          6 & 4 \\
          6 & 4
      \end{pmatrix} = \left< \begin{pmatrix} -2 \\
      3 \end{pmatrix}  \right> 
  \end{gather*}
  La matrice $Q$ che usa le basi ortonormalizzate dei due $\ker$ è la seguente
  \begin{gather*}
      Q = \begin{pmatrix}
          \frac{1}{\sqrt{2} } & \frac{-2}{\sqrt{13} } \\
          \frac{1}{\sqrt{2} } & \frac{3}{\sqrt{13} }
      \end{pmatrix}
  \end{gather*}
  Ogni vettore per la base di ogni $\ker$ è diviso per la propria norma. 
\end{example}

\section{Teorema spettrale e calcolo della segnatura}
\begin{lemma}[Gli autovalori di una matrice simmetrica sono reali]
  Siano $n \in \mathbb{N} - \{0\}$ , $A \in M(n \times n, \mathbb{R})$ simmetrica e sia
  $\lambda \in \mathbb{C}$ un autovalore per $A$. Allora $\lambda \in \mathbb{R}$.
\end{lemma}
\begin{proof}
  Sia $\lambda \in \mathbb{C} - \{0\}$ un autovalore di $A$, quindi esiste un vettore
  $v \in \mathbb{C}^{n} - \{0\}$ tale che $A v = \lambda v$ per
  definizione, allora considero il prodotto $^{t} \overline{v}Av$:
  \begin{gather*}
      ^{t}\overline{v}Av = \ ^{t}\overline{v}(Av) = \ ^{t}\overline{v}(\lambda v) = \lambda \ ^{t}\overline{v}v    
  \end{gather*}
  Il primo membro è anche uguale, dato che $A$ è simmetrica reale, alla seguente 
  \begin{gather*}
      (^{t}\overline{v}A)v = (^{t}\overline{v} \ ^{t}\overline{A})v = (^{t}(\overline{A}\overline{v}) )v
  \end{gather*}
  Queste due uguaglianze valgono perché è una matrice simmetrica reale
  e per la proposizione sulla matrice trasposta. Posso allora continuare a
  svolgere e ottenere
  \begin{gather*}
      (^{t}\overline{Av}v) = \ ^{t}\overline{\lambda v}v = \ ^{t}(\overline{\lambda} \overline{v})v  
  \end{gather*}
  La prima vale perché vale, per due valori $R$ ed $S$,  $\overline{RS} = \overline{R}\cdot \overline{S}$, mentre la seconda vale perché abbiamo imposto che
  $Av = \lambda v$, ottenendo la seguente: $\overline{\lambda} \ ^{t}\overline{v}v$.  \\
  Possiamo imporre l'uguaglianza tra le due conclusioni e ottenere:
  \begin{gather*}
      \lambda \ ^{t}\overline{v}v = \overline{\lambda} \ ^{t}\overline{v}v   \\
      (\lambda - \overline{\lambda}) \ ^{t}\overline{v}v = 0 
  \end{gather*} 
  Ora, dato che il vettore $v$ è definito come
  \begin{gather*}
      v = \begin{pmatrix} x_1 + iy_1 \\
      \vdots \\
      x_n + i y_n \end{pmatrix}, \begin{array}{l}
        x_1, \dots, x_n \in \mathbb{R} \\
        y_1, \dots, y_n \in \mathbb{R}
      \end{array} 
  \end{gather*}
  Da questo sappiamo che
  \begin{gather*}
      \begin{pmatrix}
          x_1 - i y_1 & \dots & x_n - iy_n
      \end{pmatrix} \begin{pmatrix} x_1 + i y_1 \\
      \vdots \\
      x_n + i y_n \end{pmatrix} = x_1^{2}  + y_1^{2}  + \dots + x_n^{2} + y_n^{2} > 0  
  \end{gather*}
  E quindi so che $^{t}\overline{v}v \neq 0$ e allora so che $(\lambda - \overline{\lambda}) ^{t}\overline{v}v = 0$
  e quindi so che $\lambda - \overline{\lambda} = 0$ e allora si ottiene la tesi. 
\end{proof}


\begin{theorem}[Teorema spettrale]
  Sia $A \in M(n \times n, \mathbb{R})$.
  \begin{align}
      A \text{ simmetrica} \Longleftrightarrow A \text{ ortogonalmente simile ad una matrice diagonale}
  \end{align}
  In particolare se $A$ è una matrice simmetrica reale, allora è anche
  diagonalizzabile attraverso una matrice ortogonale. 
\end{theorem}
\begin{proof}
  $\Longleftarrow$ supponendo che $\exists Q \in M(n \times n, \mathbb{R})$ tale che
  \begin{gather*}
      ^{t}Q Q = I, \qquad \ ^{t}QAQ = D   
  \end{gather*}
  Allora, sapendo che $^{t}QAQ =D$ e che $Q$ è una matrice ortogonale allora posso dire che
  \begin{gather*}
      Q^{-1} A Q = D \ \Longrightarrow \  A = QDQ^{-1} = QD \ ^{t}Q 
  \end{gather*}   
  Allora
  \begin{gather*}
      ^{t}A = \ ^{t}(QD \ ^{t}Q) = \ ^{tt}Q \ ^{t}D \ ^{t}Q = QD \ ^{t}Q = A      
  \end{gather*}
  $\ \Longrightarrow \  $Supponendo che $A$ sia simmetrica, allora per induzione su di $n$ 
  ottengo che
  \begin{gather*}
      n = 1, \qquad A \quad 1 \times 1 \ \text{diagonale} \qquad A = \ ^{t}IAI \\
      n - 1 \ \Longrightarrow \   n \qquad A \in M(n \times n, \mathbb{R}) \ \text{simmetrica}
  \end{gather*}
  Si vuole dimostrare che $A$ è ortogonalmente simile ad una matrice diagonale.
  Sia allora $\lambda \in \mathbb{R}$ la radice di $P_A$ allora 
  $\exists v_1 \in \mathbb{R}^{n} - \{0 \} $ tale che $Av_1 = \lambda v_1$. Posso allora completare
  $v_1$ ad una base di $\mathbb{R}^{n}$. \\
  Con il procedimento di Gram-Schmidt 1 possiamo trasformare questa base in una base
  ortonormale per il prodotto scalare standard; il primo è moltiplicato per uno scalare non nullo; quindi continua ad essere un
  autovettore di autovalore $\lambda$; richiamo la nuova base di vettori e considero la matrice
  \begin{gather*}
      P = \begin{pmatrix}
        v_1 & \dots & v_n \\
        &&
      \end{pmatrix}
  \end{gather*} 
  Che è ovviamente ortogonale. Allora posso considerare $^{t}PAP$ e dimostro che è simmetrica:
  \begin{gather*}
    ^{t} (^{t}PAP ) = \ ^{t}P \ ^{t}A \ ^{t}{^{t}P} = \ ^{t}PAP      
  \end{gather*}  
  Posso anche dimostrare che $^{t}PAP \cdot  e_1 = (\lambda, 0, \dots, 0)$, ossia il vettore colonna con solo un elemento non nullo:
  \begin{gather*}
      (^{t}PAP ) e_1 = (^{t}PA)(Pe_1) = (^{t}PA )v_1 = \ ^{t}P(Av_1) = \\
      \ ^{t}P(\lambda v_1) = P^{-1} (\lambda v_1) = \lambda P ^{-1}  v_1 = \lambda e_1    
  \end{gather*}
  ed è dimostrata. Per ipotesi induttiva allora esiste una matrice 
  $R \in M((n - 1) \times (n - 1), \mathbb{R})$ ortogonale tale che 
  \begin{gather*}
      ^{t}RA'R = D' \qquad \text{diagonale} 
  \end{gather*}
  Infatti la matrice
  \begin{gather*}
      ^{t}PAP = \begin{pmatrix}
          \lambda & \dots & 0 \\
          \vdots & A' & \dots \\
          0 & \dots & \dots
      \end{pmatrix} 
  \end{gather*}
  Dove $A'$ è una matrice simmetrica $\in M((n - 1) \times (n - 1), \mathbb{R})$ derivata
  dall'ipotesi induttiva. Si può dunque considerare il seguente prodotto per cui
  \begin{gather*}
      ^{t} \begin{pmatrix}
          1 & \dots & 0 \\
          \vdots & R &  \\
          0 & &  
      \end{pmatrix} \cdot  \ ^{t}PAP \cdot       \begin{pmatrix}
          1 & \dots & 0 \\
          \vdots & R &  \\
          0 & &  
      \end{pmatrix} =  \\
      =    \begin{pmatrix}
          1 & \dots & 0 \\
          \vdots & ^{t} R &  \\
          0 & &  
      \end{pmatrix} \cdot 
      \begin{pmatrix}
          \lambda & \dots & 0 \\
          \vdots & A' & \\
          0 & & 
      \end{pmatrix} \cdot       \begin{pmatrix}
          1 & \dots & 0 \\
          \vdots & R &  \\
          0 & &  
      \end{pmatrix} = \\
      = \begin{pmatrix}
          \lambda & \dots & 0 \\
          \vdots & ^{t}RA'R & \\
          0 & & 
      \end{pmatrix} = \begin{pmatrix}
          \lambda & \dots & 0 \\
          \vdots & D' & \\
          0 & & 
      \end{pmatrix}   
  \end{gather*}
  Questo perché la matrice $R$ è formata dagli autovettori della matrice
  $A'$ e dunque la matrice $D'$ è formata dagli autovalori della matrice $A'$.
  Allora, per il passo induttivo, so che la matrice ottenuta dal prodotto è la matrice che 
  ha sulla diagonale $\lambda$ e gli autovalori di $D$ e dunque, per la definizione di matrice
  ortogonalmente simile e, dato che $P$ è formata dagli autovettori della matrice $A$, $A$ è una
  matrice simmetrica. Posso definire la matrice formata da 1 e $^{t}R$ come $^{t}Q$ e 
  la matrice formata da 1 e $R$ come $Q$ e allora ho dimostrato che
  $^{t}QAQ = D $ è diagonale. Osservo inoltre che la matrice formata da 1 e $R$ è ortogonale infatti:
  \begin{gather*}
      ^{t}\begin{pmatrix}
          1 & \dots & 0 \\
          \vdots & R & \\
          0 && 
      \end{pmatrix} \cdot  \begin{pmatrix}
          1 & \dots & 0 \\
          \vdots & R & \\
          &&
      \end{pmatrix} = \begin{pmatrix}
          1 & 0 \\
          0 & I_{n - 1 }
      \end{pmatrix} = I
  \end{gather*} 
  Inoltre $P$ è ortogonale, il prodotto tra le matrici è ortogonale e allora
  anche $Q$ è ortogonale. 
\end{proof}

\begin{theorem}[Regola dei segni di Cartesio]
  Sia $p(x) = a_n x^{n} + \dots + a_k x^{k}$ un polinomio
  a coefficienti reali con $a_n \neq 0$ e $a_k \neq 0$.
  Supponiamo che $p$ abbia $n$ radici reali, allora 
  \begin{enumerate}
      \item 0 è radice di $p$ se e solo se $k \geq 1$ e la sua molteplicità è $k$;
      \item il numero delle radici positive di $p$ è il numero di variazioni di segno nella successione 
      dei coefficienti di $p$ non nulli.
  \end{enumerate}
\end{theorem}

\begin{theorem}[Metodo degli autovalori per determinare la segnatura di una matrice]
  Sia $V$ uno spazio vettoriale su $\mathbb{R}$ di dimensione finita $n$ e sia
  $b : V \times V \to \mathbb{R}$ una forma bilineare simmetrica. Sia $B$ una base
  ordinata di $V$. Allora
  \begin{align}
      p &= \text{numero di autovalori positivi di } M_B(b) \\
      \nu &= \text{numero di autovalori negativi di } M_B(b)
  \end{align}
\end{theorem}
\begin{proof}
  Si considera $M_B(b)$ una matrice simmetrica reale. Quindi per il teorema
  spettrale esiste $Q \in M(n \times n, \mathbb{R})$ ortogonale tale che
  \begin{gather*}
      ^{t}QM_B(b)Q  = D
  \end{gather*}
  Dove $D$ è diagonale e $^{t}Q = Q^{-1}$ poiché è ortogonale. Sulla diagonale di $D$,
  per il teorema spettrale, ci sono gli avutovalori della matrice $M_B(b)$. 
  Sia $B'$ una base ordinata di $V$ tale che
  \begin{gather*}
      M_{B, B'}(I) = Q
  \end{gather*}
  E che il primo elemento di $B'$ sia combinazione lineare di elementi 
  della base $B$ con coefficienti i coefficienti di $Q^{(i)}$, allora
  \begin{gather*}
      ^{t}M_{B, B'}(I)M_B(b)M_{B, B'}(I) = D = M_{B'}(b)
  \end{gather*}
  Allora il numero di elementi della diagonale maggiori di zero sulla matrice
  $D$ sono il numero di elementi maggiori di zero per la matrice $M_{B'}(b)$, il che
  vuol dire, dato che questi sono autovalori, che il teorema è dimostrato. 
\end{proof}

\begin{example}[Calcolo della segnatura con vari metodi]
  Sia $V$ uno spazio vettoriale di $\dim = 3$ su $\mathbb{R}$, e sia
  $b : V \times V \to \mathbb{R}$ una forma bilineare simmetrica e sia $B$ una
  base di $V$ tale che
  \begin{gather*}
      M_{B}(b) = \begin{pmatrix}
          2 & 1 & 0 \\
          1 & 0 & -3 \\
          0 & -3 & 0
      \end{pmatrix}
  \end{gather*}
  \begin{enumerate}
      \item Ragionamento: 
      \begin{gather*}
          \det M_{B}(b) = -18 < 0 \Rightarrow  rk M_{B}(b) = 3 \Rightarrow p + v = 3 
      \end{gather*}
      Allora so che $\nu$ è dispari e dunque ho due possibilità:
      \begin{gather*}
          (p, \nu) = (2, 1) \text{ o }p (0, 3)
      \end{gather*}
      Se fosse $(0, 3)$ allora la segnatura sarebbe definita negativa ma non lo
      è in quanto $b(v_1, v_1) = 2$ e allora l'unica possibilità è $(2, 1)$.
      \item Metodo babilonese (solo per matrici simmetriche):
      Facciamo operazioni di riga (e le stesse per le colonne) per ottenere una matrice diagonale da quella
      di partenza 
      \begin{gather*}
          \begin{pmatrix}
              2 & 0 & 0 \\
              0 & -\frac{1}{2} & 0 \\
              0 & 0 & 18
          \end{pmatrix}
      \end{gather*}
      In pratica ad ogni operazione di riga (come prima riga meno la terza,
      deve corrispondere un'analoga per le colonne: dunque prima colonna meno la terza).
      Allora la segnatura non è altro che la somma dei termini
      positivi e negativi sulla diagonale.

      \item Autovettori positivi e negativi:
      Calcolo il polinomio caratteristico della matrice di partenza e
      allora so che $p$ è il numero di autovalori positivi e $\nu$ quelli negativi. 

  \end{enumerate}
\end{example}

\begin{example}[Utilizzo dei teoremi di cambiamento di base per facilitare il calcolo della segnatura]
  Sia $V$ uno spazio vettoriale su $\mathbb{R}$ di dimensione $4$ e sia
  $B = \{v_1, v_2, v_3, v_4\}$ una base ordinata di $V$ e sia
  $b : V \times V \to \mathbb{R}$ la forma bilineare tale che
  \begin{gather*}
      M_B(b) = \begin{pmatrix}
          2 & 0 & 0 & 1 \\
          0 & 3 & 4 & 0 \\
          0 & 4 & 3 & 0 \\
          1 & 0 & 0 & 2
      \end{pmatrix}
  \end{gather*}
  i) Calcolare la segnatura di $b$.\\
  Per trovare la segnatura ci sono già tre metodi dell'esercizio precedente,
  ma qui si analizzerà un altro metodo che è molto utile negli esami:
  la segnatura della matrice può essere vista come la segnatura delle matrici
  \begin{gather*}
      \begin{pmatrix}
          2 & 1 \\
          1 & 2
      \end{pmatrix} + \begin{pmatrix}
          3 & 4 \\
          4 & 3
      \end{pmatrix} 
  \end{gather*}
  La prima matrice con il metodo dei minori principali
  ha segnatura $(2, 0)$ poiché ha due matrici con $\det > 0$. L'altra
  matrice invece ha segnatura $(1, 1)$ poiché ha determinante minore di zero e
  ha negatività, per definizione, dispari. La segnatura è allora $(3, 1)$. 
  Perché questo è vero? Perché posso modificare la matrice di partenza attraverso
  le regole di cambio base e ottenere una matrice diagonale a blocchi. In questo
  caso la base $B'$ per cui vale questo ragionamento è $B' = \{v_1, v_4, v_2, v_3\}$,
  in questo modo, date le segnature $b(v_i, v_j)$ della matrice associata alla
  base di partenza, posso determinare la nuova matrice associata alla base $B'$ che è
  formata dagli stessi vettori ed è, per questo, uguale a quella
  di partenza. 
  \begin{gather*}
      M_{B'}(b) = \begin{pmatrix}
          b(v_1, v_1) & b(v_1, v_4) & b(v_1, v_2) & b(v_1, v_3) \\
          b(v_4, v_1) & b(v_4, v_4) & b(v_4, v_2) & b(v_4, v_3) \\
          b(v_2, v_1) & b(v_2, v_4) & b(v_2, v_2) & b(v_2, v_3) \\
          b(v_3, v_1) & b(v_3, v_4) & b(v_3, v_2) & b(v_3, v_3)
      \end{pmatrix} = \begin{pmatrix}
          2 & 1 & 0 & 0 \\
          1 & 2 & 0 & 0 \\
          0 & 0 & 3 & 4 \\
          0 & 0 & 4 & 3
      \end{pmatrix}
  \end{gather*}
  Allora con la formula di cambio base è
  \begin{gather*}
      M_{B'}(b) = \ ^{t}M_{B, B'}(I)M_{B}(b)M_{BB'}(I) 
  \end{gather*}
\end{example}

\begin{definition}[Forme quadratiche]
  Sia $V$ uno spazio vettoriale su di un campo $\mathbb{K}$. Sia
  $b : V \times V \to \mathbb{K}$ una forma bilineare simmetrica. 
  Si dice \textbf{forma quadratica} associata a $b$ la funzione 
  $q : V \to \mathbb{K}$ così definita 
  \begin{align}
      q(v) = b(v, v)
  \end{align}
  Se $V$ ha dimensione finita e $B$ è una base ordinata di $V$ 
  \begin{align}
      q(v) = \ ^{t}x M_B(b) x = \sum_{i, j} x_i x_j (M_B(b))_{i, j}    
  \end{align}
  Se $M_B(b)$ è diagonale, il polinomio diventa una 
  combinazione dei quadrati dei coefficienti di $x$ 
  \begin{align}
      q(v) = \sum_{i} x_i^{2} (M_B(b))_{i, i}  
  \end{align} 
\end{definition}

\begin{lemma}
  Se $\mathbb{K}$ è tale che $1_K + 1_K \neq 0_K$ allora se conosciamo una forma
  quadratica $q$ associata alla forma bilineare simmetrica $b$, conosciamo $ b$.
\end{lemma}
\begin{proof}
  
  infatti si considera $v, w \in V, b(v + w, v + w)$:
  \begin{gather*}
      q(v + w) = b(v +w, v + w) = b (v, v) + b(v, w) + b(w, v) + b(w, w)  = \\
      q(v) + q(w) + (1_k + 1_K)b(v, w) \Rightarrow \\
      q(v + w) - q(v) - q(w ) = (1_K + 1_K) \cdot  b(v, w) \\
      \Rightarrow b(v, w) = (1_K + 1_K)^{-1} (q(v + w) - q(v) - q(w))
  \end{gather*}
\end{proof}

\section{Formula di Cauchy-Schwartz e disuguaglianza triangolare}
\begin{definition}[Formula di Cauchy-Schwartz]
  Sia $V$ uno spazio vettoriale su $\mathbb{R}$ e sia $b : V \times V \to \mathbb{R}$ una forma
  bilineare simmetrica definita positiva. Allora, per qualsiasi $v, w \in V$:
  \begin{align}
      |b(v, w)| \leq |v|_b|w|_b, 
  \end{align}
  dove $|v|_b$ è la norma di $v$ rispetto a $b$ cioè $|v|_b = \sqrt{b(v, v)}$. Se
  vale l'uguaglianza allora uno fra $v$ e $w$ è multiplo dell'altro. 
\end{definition}

\begin{theorem}[Disuguaglianza triangolare]
  Sia $V$ uno spazio vettoriale su $\mathbb{R}$ e sia $b : V \times V \to \mathbb{R}$ una forma
  bilineare simmetrica definita positiva. Allora $\forall v, w \in V$ si ha
  \begin{align}
      |v + w|_b \leq |v|_b + |w|_b
  \end{align}
\end{theorem}

\section{Seno e coseno su $\mathbb{C}$}
\begin{definition}[Esponenziali di numeri complessi]
  \begin{align}
      e^{i\theta} = \cos\theta + i\sin\theta, \qquad \forall \theta \in \mathbb{R} 
  \end{align}
  Si definiscono seno e coseno nei complessi come:
  \begin{align}
      \cos\theta = \frac{e^{i\theta} + e^{-i\theta} }{2}
  \end{align}
  \begin{align}
      \sin\theta = \frac{e^{i\theta} - e^{-i\theta} }{2i}
  \end{align}
\end{definition}

\begin{definition}[Seno e coseno di un numero complesso]
  Sia $z \in \mathbb{C}$, si definiscono seno e coseno di un numero complesso: 
  \begin{align}
      \cos z &= \frac{e^{iz} + e^{-iz} }{2} \\
      \sin z &= \frac{e^{iz} - e^{-iz}  }{2i}
  \end{align}
\end{definition}

\begin{lemma}[Proprietà di seno e coseno complessi]
  Si elencano alcune delle proprietà:
  \begin{enumerate}
    \item \begin{gather*}
        \cos^{2}z + \sin^{2}z = 1  
    \end{gather*}
    \item \begin{gather*}
        \cos(z + w) = \cos(z) \cos(w) - \sin (z) \sin(w) 
    \end{gather*}
    \item \begin{gather*}
        \sin(z + w) = \sin(z) \cos(w) + \sin(w) \cos(z)
    \end{gather*}
  \end{enumerate}
\end{lemma}
\begin{proof}
  Si dimostrano semplicemente applicando le definizioni di seno e coseno 
  nei numeri complessi. 
\end{proof}

\begin{definition}[Seno e coseno iperbolico dei numeri complessi]
  Sia $z \in \mathbb{C}$ si definisce
  \begin{align}
      \cosh(z) &= \frac{e^{z} +e^{-z}  }{2} \ \Longrightarrow \ \cos(z) = \cosh(iz) \\
      \sinh(z) &= \frac{e^{z} - e^{-z} }{2} \ \Longrightarrow \ \sin(z) = \frac{1}{i} \sinh(iz)
  \end{align}
\end{definition}

\begin{lemma}[Seno e coseno di numeri complessi]
  Siano $a, b \in \mathbb{R}$, allora 
  \begin{align}
      \cos(a \pm bi) &= \cos(a) \cosh(b) \mp i \sin(a) \sinh(b) \\
      \sin(a \pm bi) &= \sin(a) \cosh(b) \mp i \cos(a) \sinh(b)
  \end{align}
\end{lemma}
\begin{proof}
   Si dimostrano con le formule di addizione e sottrazione
  del seno e del coseno e tramite la definizione di seno e coseno 
  iperbolici. 
\end{proof}

\section{Esponenziali di matrici}
\begin{definition}[Esponenziali di matrici]
  Sia $A \in M(n \times n, \mathbb{C})$, dove $n \in \mathbb{N} - \{0\}$. Allora
  se $q(x) = a_rx^{r} + \dots + a_1 x + a_0$, definiamo
  \begin{align}
      e^{A} = \sum_{k = 0}^{\infty } \frac{A^{k} }{k!}  
  \end{align} 
  Dove $A^{0} = I_n$, e dunque
  \begin{align}
      e^{A} = I_n + A + \frac{A^{2} }{2!} + \dots 
  \end{align}  
  ossia secondo lo sviluppo di Taylor. 
\end{definition}

\begin{theorem}[L'esponenziale della somma è il prodotto degli esponenziali]
  La serie dell'esponenziale della matrice converge $\forall A \in M(n \times n, \mathbb{C})$. 
  Se $A, B \in M(n \times n, \mathbb{C})$ tale che $AB = BA$, allora
  \begin{align}
      e^{A + B} = e^{A} \cdot  e^{B}   
  \end{align}
\end{theorem}

\begin{proof}
  La prima non si dimostra, ma la seconda deriva dalla formula dell'esponenziale
  di matrici applicata alla somma tra A e B:
  \begin{gather*}
      (A + B)^{k} = \sum_{i = 0}^{k} \begin{pmatrix} k \\
      i \end{pmatrix} A^{i} B^{k - i}  , \qquad e^{A+B} = \sum_{k = 0}^{\infty } \frac{1}{k!}\sum_{i = 0}^{k}\begin{pmatrix} k \\
      i \end{pmatrix} A^{i} B^{k - i}    
  \end{gather*}
  Ossia 
  \begin{gather*}
      e^{A + B} = \sum_{k = 0}^{\infty }  \frac{1}{k!} \sum_{i = 0}^{k}  \begin{pmatrix} k\\
      i \end{pmatrix}A^{i}B^{k - i} = \sum_{k = 0}^{\infty }  \sum_{i = 0}^{k} \frac{1}{k!} \frac{k!}{i!(k - i)!}A^{i}B^{k - i}  = \\
      \sum_{k = 0}^{\infty } \sum_{i = 0}^{k}\frac{A^{i} B^{k -i}  }{i! (k - i)!}     
  \end{gather*}
  Chiamato allora $j = k - i$, si ottiene la tesi poiché
  \begin{gather*}
      e^{A} \cdot  e^{B} =   \sum_{i = 0}^{\infty } \frac{A^{i} }{i!} \sum_{j = 0}^{\infty } \frac{B^{j} }{j!}  
  \end{gather*}
\end{proof}

\begin{lemma}[l'esponenziale di una matrice ha un inversa]
  Sia $A \in M(n \times n, \mathbb{C})$, allora $e^{A}$ è invertibile 
  e la sua inversa è proprio $e^{-A}$ 
\end{lemma}
\begin{proof}
  \begin{gather*}
      A(-A) = -A^{2} = (-A)A \\
      e^{A} \cdot  e^{-A} = e^{A - A} = e^{0} = I + 0 + \frac{0^{2} }{2} = I.     
  \end{gather*}
\end{proof}

\begin{lemma}[L'esponenziale di una matrice è l'esponenziale dei singoli elementi]
  Sia $D$ una matrice diagonale definita sui complessi, 
  allora
  \begin{align}
      e^{D} = \begin{pmatrix}
          e^{d_1} & \dots & 0 \\
          \vdots & \dots & 0 \\
          0 & \dots & e^{d_n}  
      \end{pmatrix} 
  \end{align}
\end{lemma}
\begin{proof}
  Osservo innanzitutto che
  \begin{gather*}
      D^{k} = \begin{pmatrix}
          d_1^{k} & \dots & 0 \\
          \vdots & \dots & 0 \\
          0 & \dots & d_n^{k}  
      \end{pmatrix} 
  \end{gather*}
  Dato che $e^{D } = I + D + \frac{D^{2} }{2} + \dots$ si ha la tesi.  
\end{proof}

\begin{lemma}[Proprietà di matrici elevate e potenze]
  Se $A, B \in M(n \times n, \mathbb{C}) $ e $C \in GL(n, \mathbb{C})$ e $A = C^{-1} BC$, allora
  \begin{align}
      A^{k} = C^{-1} B^{k}C  
  \end{align}
  Inoltre
  \begin{align}
      q(A) = C^{-1} q(B) C
  \end{align}
  e anche
  \begin{align}
      e^{A} = C^{-1} e^{B}C  
  \end{align}
\end{lemma}
\begin{proof}
  Si ha che
  \begin{gather*}
      A^{k} = A \cdot  ... \cdot  A = (C^{-1} BC) \cdot ... \cdot  (C^{-1} BC)  = C^{-1} B^{k} C. 
  \end{gather*}  
  Inoltre, se $q(x) = a_rx^{r} + \dots + a_0$ e se $q(A) = a_rA^{r} + \dots + a_0 I$:
  \begin{gather*}
      q(A) = a_r C^{-1} B^{r} C + \dots + a_0 C^{-1} I C
  \end{gather*} 
  Raccogliendo $C^{-1} $ a sinistra e $C$ a destra si ottiene la tesi per la seconda eguaglianza. \\
  Infine si ha che
  \begin{gather*}
      e^{A} = \sum_{k = 0}^{\infty }\frac{A^{k} }{k!} = \sum_{k = 0}^{\infty } \frac{C^{-1} B^{k} C}{k!} =  C^{-1} \left(\sum_{k = 0}^{\infty } \frac{B^{k} }{k!}\right)C = C^{-1} e^{B}C    
  \end{gather*}
\end{proof}

\begin{definition}[Esponenziali di matrici diagonalizzabili]
  Sia $A \in M(n \times n, \mathbb{C})$ diagonalizzabile, allora
  $\exists C \in GL(n, \mathbb{C})$ tale che $C^{-1} AC$ è simile ad una matrice
  diagonale, allora, detta $M = C^{-1} $, si ha che
  \begin{gather*}
      MAM^{-1} = D \ \Longrightarrow \  A = M^{-1} DM   
  \end{gather*}
  ossia
  \begin{align}
      e^{A} = M^{-1} e^{D}M
  \end{align} 
\end{definition}

\begin{observation}[Esponenziale di matrici triangolari]
  Se $T$ è triangolare superiore allora
  \begin{align}
      e^{T} = \begin{pmatrix}
          e^{t_1} & \dots & 0 \\
          \vdots & \dots & \vdots \\
          0 & \dots & e^{t_n}  
      \end{pmatrix} 
  \end{align}
\end{observation}
\begin{proof}
  Infatti si ha, secondo lo sviluppo di taylor
  \begin{gather*}
      T^{k} = \begin{pmatrix}
          t_1^{k} & & \\
          & \dots& \\
          & & t_n^{k}  
      \end{pmatrix} \ \Longrightarrow \  e^{T} = \begin{pmatrix}
          1 & & 0 \\
          &\dots& \\
          0 & & 1
      \end{pmatrix} + \begin{pmatrix}
          t_1 && 0\\
          &\dots& \\
          0 && t_n
      \end{pmatrix} + \frac{1}{2}\begin{pmatrix}
          t_1^{2} & & 0 \\
          &\dots& \\
          0  & & t_n^{2}  
      \end{pmatrix} + \dots
  \end{gather*}
  Allora, per la definizione di matrice esponenziale, è dimostrata.
\end{proof}

\begin{definition}[Matrice nilpotente]
  Una matrice $A$ si dice \textbf{nilpotente} se e solo se il suo
  determinante è nullo. Una matrice nilpotente è una matrice che, dopo un
  certo numero $n \in \mathbb{N}$ di elevazioni, si annulla completamente.
\end{definition}

\chapter{Matrici Hermitiane, normali, unitarie}
\section{Definizione}
\begin{definition}[Forma hermitiana]
  Sia $V$ uno spazio vettoriale su $\mathbb{C}$. Una forma hermitiana è un'applicazione
  del tipo
  \begin{align}
      h : V \times V \to \mathbb{C}
  \end{align}
  tale che ha le seguenti proprietà
  \begin{enumerate}
      \item $h(v, w ) = \overline{h(w, v)} \quad \forall v, w \in V $: dove $h(v, w )$ è il coniugato simmetrico di $\overline{h(w, v )}$.
      \item $h(\lambda v + \mu u, w) = \lambda h(v, w) + \mu h(u, w) \quad \forall v, u, w \in V \quad \forall \lambda, \mu \in \mathbb{C}$: $h$ è $\mathbb{C}$-lineare nella prima variabile;
      \item $h(w, \lambda v + \mu u) = \overline{\lambda} h(w, v) + \overline{\mu} h(w, u) $: $h$ è $\mathbb{C}$-lineare nella seconda variabile.
  \end{enumerate}
  La linearità della seconda variabile è dimostrabile attraverso le prime due proprietà:
\begin{gather*}
    h(w, \lambda v + \mu u) = \overline{h(\lambda v + \mu u, w)}  = \overline{\lambda h(v, w) + \mu h(u, w)} = \\
    \overline{\lambda} \overline{h(v, w)} + \overline{\mu} \overline{h(u, w)} = \overline{\lambda} h(w, u) + \overline{\mu} h(w, u)  
\end{gather*}
La terza uguaglianza vale perché il coniugato di una somma è la somma dei coniugati
ed il coniugato di un prodotto è il prodotto dei coniugati.
Si osserva che se $h$ è una forma hermitiana allora $h(v, v) \in \mathbb{R}$, infatti,
per la prima proposizione si ha che $h(v, v) = \overline{h(v, v)}$.
Data una matrice a coefficienti complessi $H$ si definisce $\overline{H}$ la matrice 
con lo stesso formato di $H$ che si ottiene da $H$ coniugando ogni coefficiente.   
\end{definition}
\begin{definition}[Matrice stella]
  Siano $m, n \in \mathbb{N} - \{0\}$ e sia $H \in M(m \times n, \mathbb{C})$. Si definisce allora
  $H^{\star}$ la matrice $^{t}\overline{H}$, ossia la trasposta della coniugata di H.  
\end{definition}

\begin{lemma}[Proprietà dell'operatore stella]
  Siano $m, n, r \in \mathbb{N} - \{0\}$. 
  \begin{enumerate}
      \item $(A + B)^{\star} = A^{\star} + B^{\star} \qquad \forall A, B \in M(n \times n, \mathbb{C})$
      \item $(AB)^{\star} =  B^{\star} A^{\star}\qquad \forall A \in M(m \times r, \mathbb{C}), \forall B \in M(r \times n, \mathbb{C})$
      \item $(\lambda A)^{\star} = \overline{\lambda} A^{\star}\qquad \forall A \in M(m \times n, \mathbb{C}), \forall \lambda \in \mathbb{C}$
      \item $(A^{\star} )^{\star} = A\qquad \forall A \in M(m \times n, \mathbb{C})$
  \end{enumerate}
\end{lemma}

\begin{definition}[Matrici hermitiane, unitarie e normali]
  Sia $n \in \mathbb{N} - \{0\}$ e sia $H \in M(n \times n, \mathbb{C})$. \\
  La matrice $H$ si dice \textbf{hermitiana} se $H = H^{\star}$. \\
  La matrice $H$ si dice \textbf{unitaria} se $HH^{\star} = H^{\star}H = I$. \\
  La matrice $H$ si dice \textbf{normale} se $HH^{\star} = H^{\star}H$.   
\end{definition}

\begin{observation}
Le matrici Hermitiane e quelle unitarie sono entrambe normali 
ma può non essere uguale il contrario. Inoltre, se $H$ è una matrice quadrata
a coefficienti reali, allora è hermitiana se e solo se è simmetrica ed è unitaria
se e solo se è ortogonale. Le matrici hermitiane sono dunque la generalizzazione
delle matrici simmetriche reali nel caso complesso e le matrici unitarie sono la 
generalizzazione delle matrici ortogonali al caso complesso. Inoltre, le matrici hermitiane
hanno, necessariamente, solo elementi nei reali sulla diagonale. 
\begin{gather*}
    H_{i, i} = (H_{i, i})^{\star} = (^{t}\overline{H})_{i, i} = \overline{H}_{i, i}  
\end{gather*}
\end{observation}

\begin{definition}[Matrice anti hermitiana]
  Sia $A$ una matrice a coefficienti complessi quadrata, allora essa prende il
  nome di \textbf{anti-hermitiana} se
  \begin{align}
      A^{\star} = -A 
  \end{align}
  Si ottiene moltiplicando la matrice hermitiana per $\pm i$: $A = \pm i H$.
\end{definition}

\begin{definition}[Forma hermitiana associata ad una matrcie hermitiana]
  Sia $h_A : \mathbb{C}^{n} \times \mathbb{C}^{n} \to \mathbb{C}$ associata alla matrice $A \in M(n \times n, \mathbb{C})$ una forma hermitiana. 
  \begin{align}
      h_A(v, w) = \ ^{t}v A \overline{w} 
  \end{align}
  Dove valgono le proprietà della forma hermitiana.
\end{definition}

\begin{definition}[$H$ associata ad una forma hermitiana su di una base $B$]
  Dato una spazio vettoriale $V$ su di un campo $\mathbb{C}$ e una base $B = \left< v_1, \dots, v_n \right>$
  una base dello spazio $V$. Si considera la forma hermitiana associata a questo spazio 
  vettoriale $h : V \times V \to \mathbb{C}$. Si definisce allora $H$ una matrice tale che
  \begin{align}
      H_{i, j} = h(v_i, v_j) 
  \end{align} 
  Che è hermitiana in quanto
  \begin{gather*}
      H_{j, i} = h(v_j, v_i) = \overline{h(v_i, v_j)} = \overline{H_{i, j}}  
  \end{gather*}
  Dati $v, w \in V$ e $x \in \mathbb{C}^{n}$ vettore delle coordinate di $v$ rispetto alla
  base $B$ e $y \in \mathbb{C}^{n}$ vettore dalle coordinate di $w$ rispetto alla base $B$ vale che
  \begin{align}
      h(v, w) = \ ^{t}x H\overline{y} 
  \end{align}
  Questo perché
  \begin{gather*}
      h(v, w) = h\left(\sum_{i = 1}^{n}x_iv_i, \sum_{j = 1}^{n}y_jv_j  \right) = \sum_{i, j = 1}^{n}x_i\overline{y_j}h(v_i, v_j) = \sum_{i, j = 1}^{n}x_i h(v_i, v_j)\overline{y_j} = \ ^{t}xH\overline{y}     
  \end{gather*}
\end{definition}

\begin{definition}
  Si definisce il \textbf{prodotto Hermitiano standard}, o \textbf{forma hermitiana standard}, la seguente
  forma hermitiana $h : \mathbb{C}^{n} \times \mathbb{C}^{n} \to \mathbb{C}$ 
  Tale che
  \begin{align}
      (x, y) \to \  ^{t}x \overline{y} 
  \end{align}
\end{definition}

\begin{observation}[Una matrice Hermitiana ha autovalori nei reali]
  Sia $n \in \mathbb{N} -\{0\}$ e sia $H \in M(n \times n, \mathbb{C})$ hermitiana. Se
  $\lambda \in \mathbb{C}$ un autovalore di $H$, allora $\lambda \in \mathbb{R}$.  
\end{observation}
\begin{proof}
  Si dimostra in modo analogo a quanto dimostrato per le matrici simmetriche, ossia considerando
  due autovalori nei complessi e, dalla definizione di matrice hermitiana, osservare che
  tutti gli elementi sulla diagonale sono $\in \mathbb{R}$:
  \begin{gather*}
      ^{t}\overline{v}Hv \qquad v \in \ker(f_H - \lambda I) 
  \end{gather*}
  Dunque
  \begin{gather*}
      ^{t}\overline{v}(Hv) = ^{t}\overline{v}\lambda v = \lambda ^{t}\overline{v}v   
  \end{gather*}
  Il che deve essere equivalente alla stella:
  \begin{gather*}
      ^{t}\overline{v}H^{\star}v = ^{t}\overline{v}^{t}\overline{H}v = \overline{\lambda}^{t}\overline{v}v     
  \end{gather*}
  Allora
  \begin{gather*}
      ^{t}\overline{v}v(\lambda - \overline{\lambda} ) = 0 \ \Longrightarrow \ \lambda = \overline{\lambda}   
  \end{gather*}

\end{proof}


\begin{lemma}[Ortogonalità degli autovettori tra autovalori distinti]
  Nel caso di matrici reali simmetriche, abbiamo dimostrato che gli autovettori
  relativi ad autovalori distinti sono non solo linearmente indipendenti ma anche
  ortogonali per il prodotto scalare standard. Analogamente si dimostra che gli autovettori
  di matrici hermitiane relativi ad autovalori distinti sono ortogonali per il prodotto
  hermitiano standard; infatti se $H$ è una matrice hermitiana e $v, w$ sono autovettori
  per $H$ di autovalori rispettivamente $\lambda, \mu$ con $\lambda \neq  \mu$:
  \begin{gather*}
      \mu(^{t}v \bar{w} ) = \bar{\mu} (^{t}v\bar{w} ) = \ ^{t}v(\bar{\mu}\bar{w} ) = \ ^{t}v(\overline{\mu w}) = \ ^{t}(\overline{Hw}) = \\
      \  ^{t} v(\bar{H}\bar{w} ) = \ ^{t}v(^{t}H \bar{w} ) = (^{t}v^{t}H)\bar{w} = \ ^{t}(Hv) \bar{w} = \ ^{t}(\lambda v) \bar{w} = \lambda (^{t}v\bar{w} )         
  \end{gather*}
  Si deduce allora che
  \begin{gather*}
      \mu (^{t}v\bar{w} ) = \lambda (^{t}v\bar{w} )
  \end{gather*}
  dato che $\lambda \neq  \mu$, deve necessariamente risultare $^{t}v\bar{w} = 0$, ossia $v, w$ sono ortogonali
  per il prodotto hermitiano standard.   
\end{lemma}

\section{Il teorema spettrale complesso}
\begin{definition}[Matrici unitariamente simili]
  Due matrici $A, B$ si dicono \textbf{unitariamente simili} se esiste una matrice
  $U$ unitaria tale che $A = U^{-1} BU$. 
\end{definition}

\begin{lemma}[Relazioni tra matrici unitariamente simili]
  Per le matrici unitariamente simili esistono tre proprietà, con $A \in M(n \times n, \mathbb{C})$ e 
  $B \in M( n \times n, \mathbb{C})$:
  \begin{enumerate}
      \item Se $A = U^{-1} BU$ la prima è normale $\Longleftrightarrow $ la seconda è normale;
      \item Se $A = U^{-1} BU$ la prima è hermitiana $\Longleftrightarrow $ la seconda è hermitiana;
      \item Se $A = U^{-1} BU$ la prima è unitaria $\Longleftrightarrow $ la seconda è unitaria.
  \end{enumerate}
\end{lemma}
\begin{proof}
  Siano $A, B \in M(n \times n, \mathbb{C})$, per qualche $n \in \mathbb{N} - \{0\}$, due matrici
  unitariamente simili, quindi esiste una $U$ unitaria tale che
  $B = U^{-1} AU$ e quindi si sa che $B$ è normale se e solo se
  \begin{gather*}
      (U^{-1} AU)(U^{-1} AU)^{\star} = (U^{-1} AU)^{\star}(U^{-1} AU) \\
      U^{-1} AUU^{\star}A^{\star}(U^{-1} )^{\star} = U^{\star}A^{\star}(U^{-1} )^{\star}U^{-1} AU \\
      U^{\star}AA^{\star}U = U^{\star}A^{\star}AU \\
      AA^{\star} = A^{\star}A                
  \end{gather*}
  Quindi se e solo se $A$ è normale. \\
  La 2 e la 3 si dimostrano analogamente. 
\end{proof}


\begin{theorem}[Teorema spettrale complesso]
  Sia $n \in \mathbb{N} - \{0\}$ e sia $A \in M(n \times n, \mathbb{C})$. 
  \begin{enumerate}
      \item $A$ è hermitiana $\Longleftrightarrow $ è unitariamente simile ad una matrice diagonale reale;
      \item $A$ è unitaria $\Longleftrightarrow $ è unitariamente simile ad una matrice diagonale con coefficienti sulla diagonale di norma 1.
      \item $A$ è normale $\Longleftrightarrow $ è unitariamente simile ad una matrice diagonale;
  \end{enumerate}
\end{theorem}

\begin{proof}
  La dimsotrazione fa utilizzo della proposizione precedente per raggiungere la tesi. \\
  (solo in questo verso) $\Longleftarrow$ Data una matrice Diagonale $D$, valgono allora le seguenti condizioni
  \begin{enumerate}
      \item $D$ è hermitiana $\Longleftrightarrow$ $D_{i, i} \in \mathbb{R}$ $\Longleftrightarrow$ D è hermitiana $\Longleftrightarrow$ $D = D^{\star}$. 
      \begin{gather*}
          U^{-1}AU = D \ \Longrightarrow \ A = UDU^{-1} \ \Longrightarrow \ A^{\star} = (UDU^{-1})^{\star} = (U^{-1})^{\star}D^{\star}U^{\star} = UDU^{\star} = A
      \end{gather*}
      $(U^{-1})^{\star}$ è esattamente $U$ poiché è unitaria:
      \begin{gather*}
          UU^{\star} = U^{\star}U = I \ \Longrightarrow \ U = I(U^{-1})^{\star} \ \Longrightarrow \ U = (U^{-1})^{\star}, \ U^{\star} = U^{-1}
      \end{gather*}
      Dunque per il lemma precedente $A$ è hermitiana poiché è unitariamente simile a $D$ hermitiana. 
      \item $D$ diagonale con $\left| D_{i, i} \right| = 1 \Longleftrightarrow D$ unitaria $\Longleftrightarrow DD^{\star} = D^{\star}D = I$. 
      \begin{gather*}
          U^{-1}AU = D \ \Longrightarrow \ A = UDU^{-1} \ \Longrightarrow \ A^{\star} = (UDU^{-1})^{\star} = (U^{-1})^{\star}DU^{\star} = UD^{\star}U^{-1}
      \end{gather*} 
      Vale che $AA^{\star} = UDU^{-1}UD^{\star}U^{-1} = UDD^{\star}U^{-1} = U^{-1}U = I$ è unitaria. 
      \item $D$ diagonale $\Longleftrightarrow D$ è normale $\Longleftrightarrow$ $DD^{\star} = DD^{\star}$:
      \begin{gather*}
          U^{-1}AU = D \ \Longrightarrow \ A = UDU^{-1} \ \Longrightarrow \ A^{\star} = UD^{\star}U^{-1}
      \end{gather*}
      Vale che
      \begin{gather*}
          AA^{\star} = UDU^{-1}UD^{\star}U^{-1} = UDID^{\star}U^{-1}= UDD^\star U^{-1} 
      \end{gather*}
      Inoltre vale che
      \begin{gather*}
          A^{\star}A = UD^{\star}U^{-1}UDU^{-1} = UD^{\star}IDU^{-1} = UD^{\star}DU^{-1} = UDD^{\star}U^{-1} = AA^{\star}
      \end{gather*} 
  \end{enumerate} 
\end{proof}

\section{Simultanea diagonalizzabilità}
\begin{observation}
  Se $A$ e $B$ sono due matrici quadrate che commutano, allora 
  \begin{align}
      \forall \ \lambda \in \mathbb{K} \quad B(\ker(A - \lambda I)) \subset \ker(A - \lambda I)
  \end{align}
\end{observation}
\begin{proof}
  Sia $v \in \ker(A - \lambda I)$. Voglio allora dimostrare che 
  $Bv \in \ker(A - \lambda I)$. Ovviamente si ha che per appartenere
  al sottospazio formato dagli autovettori
  \begin{gather*}
      (A - \lambda I)(Bv) = 0 \ \Longrightarrow \  ABv - \lambda Bv 
  \end{gather*}
  Dato che le matrici commutano, sarà uguale alla seguente espressione:
  \begin{gather*}
       BAv - \lambda Bv = B(A - \lambda I)v = B0 = 0
  \end{gather*} 
\end{proof}

\begin{theorem}[Teorema di simultanea diagonalizzabilità]
  Siano $\mathbb{K}$ un campo e $n \in \mathbb{N} -  \{0\}$. Date allora 
  $A, B \in M(n, \mathbb{K})$, esse sono simultaneamente diagonalizzabili se
  e solo se sono diagonalizzabili e commutano. 
\end{theorem}
\begin{proof}
  $ \Longrightarrow $ Per ipotesi $\exists C \in GL(n, \mathbb{K})$ tale che
  $C^{-1} AC = D$ è diagonale e che $C^{-1} BC = D'$ diagonale. 
  Allora
  \begin{gather*}
      A = CDC^{-1} \qquad B = CD'C^{-1} 
  \end{gather*}
  Allora si verifica che commutino:
  \begin{gather*}
      AB = (CDC^{-1})(CD'C^{-1} ) = CDD'C^{-1}  = (CD'C^{-1})(CDC^{-1} ) = BA
  \end{gather*}
  $\Longleftarrow$ Supponendo adesso che $A$ e $B$ siano
  diagonalizzabili e commutino, si vuole dimostrare che 
  sono simultaneamente diagonalizzabili. Sfruttando l'osservazione dimostrata prima
  si può ottenere, dato lo spettro di $A = \{\lambda_1, \dots, \lambda_k\}$ 
  e posta $P$ una base di $\mathbb{K}^{n}$ formata da autovettori di $A$ in modo tale che che si
  ottiene la seguente 
  \begin{gather*}
      M_{P, P}(f_A) = \begin{pmatrix}
          \lambda_1 I_{n_1} & 0 & . & . & . & 0 \\
          0 & \lambda_2 I_{n_2} & 0 & . & . & 0 \\
          . & 0 & . & . & . & . \\
          . & . & . & . & . & . \\
          . & . & . & . & . & 0 \\
          0 & 0 & . & . & 0 & \lambda_k I_{n_k}
      \end{pmatrix}
  \end{gather*}
  Dato che la matrice $B$ deve commutare ed essere diagonalizzabile, allora
  la matrice $M_{P, P}(f_B)$, per l'osservazione precedente, è una matrice
  diagonale a blocchi con blocchi diagonali
  \begin{gather*}
      M_{P, P}(f_B) = \begin{pmatrix}
          R_1 & \dots & \dots  & 0 \\
          0 & R_2 & 0 & \vdots \\
          \vdots & 0 & . & 0 \\
          0 & \dots & \dots & R_k
      \end{pmatrix}
  \end{gather*}
  $M_{P, P}(f_B)$ è una matrice diagonale
  a blocchi con blocchi diagonali di formato $n_1 \times n_1, \dots, n_k \times n_k$, dunque si ha che
  $M_{P, P}(f_B)$ è simile a $B$. Infatti, per la formula di cambiamento di base,
  \begin{gather*}
      M_{P, P}(f_B) = M_{P, \epsilon}(I_{\mathbb{K}^{n} })^{-1} M_{\epsilon, \epsilon}(f_B) M_{P, \epsilon}(I_{\mathbb{K}^{n} })
  \end{gather*}
  Dato che la base associata alla canonica di $f_B$ è proprio la matrice $B$
  \begin{gather*}
      M_{P, P}(f_B) = M_{P, \epsilon}(I_{\mathbb{K}^{n} })^{-1} B  M_{P, \epsilon}(I_{\mathbb{K}^{n} })
  \end{gather*}
  Essendo $B$ diagonalizzabile per ipotesi, allora anche $M_{P, P}(f_B)$ è diagonalizzabile.
  È possibile dunque trovare una matrice $S \in GL(n, \mathbb{K})$ diagonale a blocchi con blocchi
  diagonali di formato $n_1 \times n_1, \dots, n_k \times n_k$. Le matrici $S_i$ che
  compongono la matrice $S$ sono del tipo
  \begin{gather*}
      S_i \in GL(n_i, \mathbb{K}) \quad i = \{1, \dots, k\} \ \Longrightarrow \ S^{-1} M_{P, P}(f_B)S = D'
  \end{gather*}
  Dunque si ottiene $D'$ come la matrice diagonale risultante. Sfruttando la formula
  cambiamento di base:
  \begin{gather*}
      S^{-1} M_{\epsilon, P}(I_{\mathbb{K}^{n} })^{-1} B  M_{\epsilon, P}(I_{\mathbb{K}^{n} })S = D'
  \end{gather*}
  Allora si ottiene che la matrice che diagonalizza $B$ è proprio
  \begin{gather*}
      M_{\epsilon, P}(I_{\mathbb{K}^{n} })S
  \end{gather*}
  Si osserva che diagonalizza anche A: infatti 
  essendo $M_{P, P}(f_A)$ una matrice diagonale a blocchi
  \begin{gather*}
      S^{-1} M_{\epsilon, P}(I_{\mathbb{K}^{n} })^{-1} A M_{\epsilon, P}(I_{\mathbb{K}^{n} })S = \\
      S^{-1} M_{\epsilon, P}(I_{\mathbb{K}^{n} })^{-1} M_{\epsilon, \epsilon}(f_A) M_{\epsilon, P}(I_{\mathbb{K}^{n} })S = S^{-1} M_{P, P}(f_A)S
  \end{gather*}
  Ossia la matrice a blocchi diagonale che cercavamo.
\end{proof}

\begin{lemma}[Osservazione post-dimostrazione]
  Se una delle due matrici è non degenere, allora la 
  procedura per diagonalizzare simultaneamente le matrici
  si semplifica notevolmente in quanto basta trovare una base
  di $P$ formata da autovettori per quella non degenere ed essa
  sarà una base formata da autovettori anche per l'altra: questo perché
  $M_{P, P}(f_B)$ sarà una matrice a blocchi diagonali con blocchi diagonali
  che hanno righe e colonne uguali alle dimensioni
  degli autospazi di $A$ (ossia 1) e allora sarà diagonale.
\end{lemma}

\begin{example}
  Poste 
  \begin{gather*}
      A = \begin{pmatrix}
          2 & 0 & 0 & 0 \\
          5 & 2 & -5 & 0 \\
          5 & 0 & -3 & 0 \\
          15 & 5 & -5 & -3
      \end{pmatrix} \qquad B = \begin{pmatrix}
          1 & 0 & 0 & 0 \\
          0 & 1 & 0 & 4 \\
          -12 & -4 & 5 & 4 \\
          0 & 0 & 0 & 5
      \end{pmatrix}
  \end{gather*}
  Trovare una matrice $C$ tale per cui si possa diagonalizzare
  le matrici e verificare che siano simultaneamente diagonalizzabili.
  Lo spettro di $A$ è $\{2, -3\}$ e di $B$ è $\{1, 5\}$ tutte molteplicità
  algebrica e geometrica uguale a $2$ e quindi sono diagonalizzabili.
  Si verifica anche che 
  \begin{gather*}
      AB = \begin{pmatrix}
          2 & 0 & 0 & 0 \\
          65 & 22 & -25 -12 \\
          41 & 12 & -15 & -12 \\
          75 & 25 & -25 & -15
      \end{pmatrix} = BA
  \end{gather*}   
  Dato che sono simultaneamente diagonalizzabili allora possiamo
  trovare una matrice $C$ che diagonalizza entrambe. Cerchiamo una base $P$ di $\mathbb{R}^{4}$
  formata da autovettori di $A$ 
  \begin{gather*}
      \ker(A - 2I) = \left< \begin{pmatrix} -1 \\
      2\\
      -1\\
      0 \end{pmatrix}, \begin{pmatrix} 0 \\
      1\\
      0\\
      1 \end{pmatrix} \right> \\
      \ker(A + 3I) = \left< \begin{pmatrix} 0 \\
      0\\
      0\\
      1 \end{pmatrix}, \begin{pmatrix} 0 \\
      1\\
      1\\
      0 \end{pmatrix}  \right> 
  \end{gather*}
  La matrice $C$ allora che mi permette simultaneamente di
  diagonalizzare entrambe le matrici si ottiene con la composizione
  dei due autospazi e quindi la base della matrice è
  \begin{gather*}
      P = \left\{ \begin{pmatrix} -1 \\
      2\\
      -1\\
      0 \end{pmatrix}, \begin{pmatrix} 0 \\
      1\\
      0\\
      1 \end{pmatrix}, \begin{pmatrix} 0 \\
      0\\
      0\\
      1 \end{pmatrix}, \begin{pmatrix} 0 \\
      1\\
      1\\
      0 \end{pmatrix}  \right\} 
  \end{gather*}
  Quindi la matrice che parte dalla canonica e che 
  arriva alla base $P$ che diagonalizza a blocchi le matrici è
  \begin{gather*}
      K = M_{P, \epsilon}(I) = \begin{pmatrix}
          -1 & 0 & 0 & 0 \\
          2 & 1 & 0 & 0 \\
          -1 & 0 & 0 & 1 \\
          0 & 1 & 1 & 0
      \end{pmatrix}
  \end{gather*}
  Si ottiene che $K^{-1}BK$
  \begin{gather*}
      \begin{pmatrix}
          1 & 0 & 0 & 0 \\
          0 & 5 & 0 & 0 \\
          0 & 0 & 5 & 0 \\
          0 & 0 & 4 & 1
      \end{pmatrix}
  \end{gather*}
  E $K^{-1} AK$
  \begin{gather*}
      \begin{pmatrix}
          2 & 0 & 0 & 0 \\
          0 & 2 & 0 & 0 \\
          0 & 0 & -3 & 0 \\
          0 & 0 & 0 & -3
      \end{pmatrix}
  \end{gather*}
  Dobbiamo trovare ora la matrice $S$ che diagonalizza entrambe le
  matrici: dato che solo 
  \begin{gather*}
      \begin{pmatrix}
          5 & 0 \\
          4 & 1
      \end{pmatrix} \subset B
  \end{gather*}
  è a blocchi, la matrice per diagonalizzarla 
  è quella tale per cui
  $S_2^{-1}\begin{pmatrix}
          5 & 0 \\
          4 & 1
      \end{pmatrix}S_2$ è diagonale. Allora si vede che $S_2 = \begin{pmatrix}
          1 & 0 \\
          1 & 1
      \end{pmatrix}$ è la matrice cercata; allora possiamo dire che
      \begin{gather*}
          S = \begin{pmatrix}
              1 & 0 & 0 & 0 \\
              0 & 1 & 0 & 0 \\
              0 & 0 & 1 & 0 \\
              0 & 0 & 1 & 1
          \end{pmatrix}
      \end{gather*}
      Allora la matrice che cerchiamo dall'inizio che diagonalizza
      sia la matrice $A$ che la matrice $B$ è proprio
      \begin{gather*}
          C = KS = \begin{pmatrix}
              -1 & 0 & 0 & 0 \\
              2 & 1 & 1 & 1 \\
              -1 & 0 & 1 & 1 \\
              0 & 1 & 1 & 0
          \end{pmatrix}
      \end{gather*}
\end{example}












\appendix
\chapter{Esercizi da esami}
\section{Primo parziale 2024-2025}
\subsection{Esercizio 1}
$\forall a \in \mathbb{R}$ e sia $F_a : \mathbb{R}[x]_{\leq 3} \to \mathbb{R}[x]_{\leq 3}$ la funzione
lineare definita da $F_a(f) = af - f'$, dove $f' = \frac{df}{dx}$.  \\
i) Trovare la matrice $M_{B, B}(F_a)$ dove $B = \{1, x, x^{2}, x^{3}  \}$. \\
La matrice associata alla base $B$ rispetto alla funzione lineare $F_a$ è proprio
\begin{gather*}
    F_a(1) = a\cdot 1 - 0 \\
    F_a(x) = ax - 1 \\
    F_a(x^{2} ) = ax^{2} - 2x \\
    F_a(x^{3} ) = ax^{3} - 3x^{2}  
\end{gather*}
Allora posso esprimere ogni singolo membro come le coordinate rispetto alla base $B$
e dunque ottenere la matrice
\begin{gather*}
    M_{B, B}(F_a) = \begin{pmatrix}
        a &-1 & 0 & 0 \\
        0& a & -2 & 0 \\
        0 & 0 & a & -3 \\
        0 & 0 & 0 & a
    \end{pmatrix}
\end{gather*}
ii) Trovare i valori di $a \in \mathbb{R}$ tali che $F_a$ è suriettiva . \\
Per far sì che $F_a$ sia suriettiva dobbiamo semplicemente porre che il Kernel
della funzione contenga solo $0$, e questo si ha solo per $a \neq 0$. Allora,
dato che $F$ è iniettiva, allora è anche suriettiva. Se si avesse
$a = 0$, allora si avrebbe che non è iniettiva e quindi non suriettiva. \\
iii) Trovare (se esiste) $f \in \mathbb{R}[x]_{\leq 3}$ tale che $F_1(f) = x$. \\
Intanto sappiamo che
\begin{gather*}
    F_1 = \begin{pmatrix}
        1 & -1 & 0 & 0 \\
        0 & 1 & -2 & 0 \\
        0 & 0 & 1 & -3 \\
        0 & 0 & 0 & 1
    \end{pmatrix}\begin{pmatrix} a \\
    bx \\
    cx^{2} \\
    dx^{3}   \end{pmatrix} = \begin{pmatrix} 0 \\
    x \\
    0\\
    0 \end{pmatrix}  
\end{gather*}
E quindi il vettore cercato è $(1, x, 0, 0)$ e quindi $x + 1$. \\
iv) Posto $A = \left< 1, 1 - x^{2}  \right>$, $B = \left< 1, x, x(1 - x^{2} ) \right>$, entrambi sottospazi di 
$\mathbb{R}[x]_{\leq 3}$ e calcolare una base  di $A \cap B$ e $A + B$. \\
Ricordando le definizioni, posso calcolare una base della intersezione
dei due spazi data da
\begin{gather*}
    A \cap  B = \left< 1\right>, A + B = \left<1, x, - x^{2} x^{3}   \right> 
\end{gather*}
Ossia $A + B = \mathbb{R}_{\leq 3}$.


\section{Secondo parziale 2024-2025}
\subsection{Es 6}
Sia $H = \begin{pmatrix}
    1 & 1 - i \\
    1 + i & 3
\end{pmatrix} \in M(2 \times 2, \mathbb{C})$. \\
i) Sia $h : \mathbb{C}^{2} \times \mathbb{C}^{2} \to \mathbb{C}$ la
forma hermitiana la cui matrice associata nella base ordinata $\{e_1, e_2\}$ 
è $H$. Calcolare $h(v, v), v = \begin{pmatrix} 1 \\
-2i \end{pmatrix}$. \\
Data la definizione di matrice associata nella base canonica per le forme bilineari si sa che
\begin{gather*}
    h(e_1, e_1) = 1 \quad h(e_1, e_2) = 1 - i \quad h(e_2, e_1) = 1 + i \quad h(e_2, e_2) = 3
\end{gather*}   
Allora, posso calcolare che
\begin{gather*}
    h(v, v) = h(e_1 - 2i e_2, e_1 -2i e_2) = 17
\end{gather*}
ii) Esprimere $f_H^{\vee}(e_1^{\vee} + 3e_2^{\vee} )$ come combinazione lineare di $e_1^{\vee}$ e $e_2^{\vee}$ 
ossia dove $f_H^{\vee} : (\mathbb{C}^{2} )^{\vee} \to (\mathbb{C}^{2})^{\vee}$ è
l'applicazione lineare duale (anche detta aggiunta) di $f_H : \mathbb{C}^{2} \to \mathbb{C}^{2}$.   \\
Posso chiamare $g = f_H^{\vee}(e_1^{\vee} + 3e_2^{\vee}  )$ e allora ottenere che
l'applicazione lineare la posso esprimere come combinazione
lineare dei vettori duali (in quanto so che per definizione
$e_1 e_1^{\vee} = 1$ per esempio) e dunque
\begin{gather*}
    g = g(e_1)e_1^{\vee} + g(e_2)e_2^{\vee}   
\end{gather*}
Posso allora ottenere
\begin{gather*}
    g(e_1) = f_H^{\vee}(e_1^{\vee} + 3e_2^{\vee}  )(e_1) = (e_1^{\vee} + 3e_2^{\vee}  )f_H(e_1)  = \\ 
    (e_1^{\vee} + 3e_2^{\vee}  )(e_1 + (1 + i)e_2) = 4 + 3i
\end{gather*}
E l'altra secondo lo stesso ragionamento
\begin{gather*}
    g(e_2) = 10 - i
\end{gather*}
alternativamente potevo ottenere la matrice associata
alla canonica duale come la trasposta della matrice della matrice $H$.
\begin{gather*}
    \begin{pmatrix}
        1 & 1 + i \\
        1 - i & 3
    \end{pmatrix} \begin{pmatrix} 1\\
    3 \end{pmatrix} = \begin{pmatrix} 4 + 3i \\
    10 - i \end{pmatrix} 
\end{gather*}
In entrambi i casi si arriva alla conclusione
\begin{gather*}
    f_H^{\vee} (e_1^{\vee} + 3e_2^{\vee}  ) = (4 + 3i)e_1^{\vee} + (10  - i)e_2^{\vee}   
\end{gather*}

\section{Totale 13/6/2025}
\subsection{Es 1}
Data la retta $r$ in $\mathbb{R}^{3}$ con equazioni cartesiane
\begin{gather*}
    \left\{\begin{array}{l}
      x - y + z = 1 \\
      x - 2x = 0
    \end{array}\right.
\end{gather*} 
i) Si trovino le equazioni parametriche della retta $r$. \\
Ponendo $t = z$ posso ottenere il seguente sistema:
\begin{gather*}
    \left\{\begin{array}{l}
      z = t \\
      x = 2t \\
      y = 3t - 1
    \end{array}\right.
\end{gather*}
Ottenendo così la sua espressione parametrica con
vettore direttore $\begin{pmatrix} 2 \\
3\\
1 \end{pmatrix}$.\\

ii)  Si calcoli la distanza $r$ dall'origine. \\
Per trovare la distanza dall'origine della retta posso direttamente 
trovare il piano che interseca la retta e l'origine, ossia il piano
$\Pi : 2x + 3y + z = 0$, (infatti la retta ed un piano perpendicolare
hanno lo stesso vettore direttore). 
Possiamo allora intersecare al retta al piano ed ottenere che
\begin{gather*}
    2(2t) + 3(3t - 1) + t = 0 \\
    t = \frac{3}{14} \ \Rightarrow \ \Pi \cap r = \begin{pmatrix} \frac{6}{14} \\
    -\frac{5}{14} \\
    \frac{3}{14}\end{pmatrix} 
\end{gather*}
Per cui la distanza è esattamente la norma, ossia$\frac{\sqrt{70} }{14}$. \\
iii) Si trovino i piani contenenti $r$ e perpendicolari 
rispettivamente a ciascuno dei tre piani coordinati $x = 0, y = 0, z = 0$. \\
I piani contenenti $r$ formano il fascio di piani come combinazione
lineare
\begin{gather*}
    \lambda(x - y + z - 1) + \mu(x - 2z) = 0
\end{gather*}
E quindi ha vettore normale il vettore direttore della retta 
ossia
\begin{gather*}
    (\lambda + \mu, -\lambda, \lambda - 2\mu)
\end{gather*}
Posso allora dire che questo fascio di piani è perpendicolare
a $x = 0$ e e solo se $\lambda + \mu = 0$, perpendicolare a $y = 0$ 
se e solo se $\lambda = 0$ e perpendicolare a $z = 0$ se e solo se 
$\lambda -2\mu = 0$. 
I piani allora sono
\begin{gather*}
    -y + 3z -1 = 0 \\
    x - 2z = 0 \\
    3x - 2y -2 = 0
\end{gather*}


\subsection{Es 2}
\begin{gather*}
    A_{s, t} = \begin{pmatrix}
        3 & t & 0 & 0 \\
        0 & 3 & 0 & 0 \\
        0 & 0 & 2 & s \\
        0 & 0 & s & 2
    \end{pmatrix}
\end{gather*}
i) Trovate tutte le coppie $(s, t) \in \mathbb{C}^{2}$ tali che $A_{s, t}$ sia diagonalizzabile. \\
La matrice è diagonalizzabile se e solo se i blocchi
\begin{gather*}
    \begin{pmatrix}
        3 & t \\
        0 & 3
    \end{pmatrix}, \begin{pmatrix}
        2 & s \\
        s & 2
    \end{pmatrix} 
\end{gather*}
sono diagonalizzabili. Il primo blocco ha come autovalori
$3$ con $m_a = 2$ e la sua molteplicità algebrica è data da
\begin{gather*}
    \ker \begin{pmatrix}
        0 & t \\
        0 & 0 
    \end{pmatrix}, m_g = \left\{\begin{array}{l}
      1 \ t \neq 0 \\
      2 \ t = 0 
    \end{array}\right.
\end{gather*}
E quindi è diagonalizzabile se e solo se $ t = 0$. L'altro
blocco invece ha come autovalori le radici di
\begin{gather*}
    P_A(\lambda) \begin{pmatrix}
        2- \lambda & s  \\
        s & 2 - \lambda
    \end{pmatrix} = (2 - \lambda)^{2} - s^{2} = \lambda^{2} - 4\lambda + (4 - s^{2} ) = 0   
\end{gather*}
E quindi gli autovalori sono $\lambda_1 = 2 + s$ e $\lambda_2 = 2 - s$.
Osserviamo allora che le molteplicità geometriche sono
\begin{gather*}
    m_g(2 + s) = \ker \begin{pmatrix}
        -s & s \\
        s & -s
    \end{pmatrix} \ \Rightarrow \ = 1 \Leftrightarrow  s \neq 0 \\
    m_g(2 - s) = \ker \begin{pmatrix}
        s & s \\
        s & s
    \end{pmatrix} \ \Rightarrow \ = 1 \Leftrightarrow s \neq 0
\end{gather*}
Tuttavia, per $s = 0$ è già diagonale, allora in conclusione possiamo dire che la matrice $A$ è diagonalizzabile
se e solo se $ t = 0, \forall s \in \mathbb{C}$. \\ 
ii) Per le coppie $(s, t)$ trovare una base di autovettori per $A_{s, t}$. \\
Sapendo che gli autospazi sono definiti da
\begin{gather*}
   \left< \begin{pmatrix} 1 \\
    0 \end{pmatrix}, \begin{pmatrix} 0 \\
    1 \end{pmatrix}   \right>, \left< \begin{pmatrix} 1 \\
    -1 \end{pmatrix}, \begin{pmatrix} 1 \\
    1 \end{pmatrix} \right>  
\end{gather*}
Allora
\begin{gather*}
    \begin{pmatrix}
        1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        0 & 0 & 1 & 1 \\
        0 & 0 & -1 & 1
    \end{pmatrix}
\end{gather*}
E' la matrice base di autovettori per $A_{s, t}$ con $ t = 0 $. \\
iii) Sia $Col(A_{s, t})$ il sottospazio di $\mathbb{C}^{4}$ generato dalle colonne, trovare per
quali $(s, t)\in \mathbb{C}^{2}$ vale che $\begin{pmatrix} 1 \\
1\\
1\\
1 \end{pmatrix} \in Col(A_{s, t})$. \\
Possiamo semplicemente notare che il rango della matrice, per far sì che
quel vettore appartenga al sottospazio in $\mathbb{C}^{4}$ sia di $4$ e quindi
la matrice a blocchi avrà rango quattro se e solo se $s \neq \pm 2$. e pertanto
per questo valore il sottospazio delle colonne coincide con $\mathbb{C}^{4}$ e allora
il vettore dato ne appartiene.  \\
iv) Calcolare le potenze $A_{s, t}^{m}$ per ogni $m \in \mathbb{N}$. \\
Dalla definizione di matrice invertibile, posso semplicemente calcolare
la matrice $C$ affiancando i vettori della base degli autovettori e ottenendo allora
\begin{gather*}
    C^{-1}  = \begin{pmatrix}
        1 & 1 \\
        -1 & 1
    \end{pmatrix}, C  = \frac{1}{\sqrt{2} }\begin{pmatrix}
        1 & 1 \\
        -1 & 1
    \end{pmatrix}
\end{gather*}
E allora la matrice diagonale sarà
\begin{gather*}
    C^{-1} \begin{pmatrix}
        2 & s \\
        s & 2
    \end{pmatrix}C = \begin{pmatrix}
        2 - s & 0 \\
        0 & 2 + s
    \end{pmatrix}
\end{gather*}
Posso ottenere la potenza $m$-esima come
\begin{gather*}
    \begin{pmatrix}
        2 & s \\
        s & 2
    \end{pmatrix}^{m} = C\begin{pmatrix}
        (2 - s)^{m} & 0 \\
        0 & (2 + s)^{m}  
    \end{pmatrix}C^{-1}  
\end{gather*}

\subsection{Es 4}
Sia $V$ uno spazio vettoriale su di un campo $K$ e sia
$\{v_1, \dots,  v_5\}$ una sua base. Allora:
\begin{enumerate}
    \item esiste una ed una sola applicazione lineare $f : V \to V$ tale che 
    $\ker(f) = \left< v_1, v_2, v_3 \right>$ e $Im(f) = \left< v_1, v_2 \right>$.  
    \item $\{f \in \hom(V, V) : \ker(f) \ \text{contiene} \ \left< v_1, v_2, v_3, v_4 \right>\ \text{e} \ Im(f) \subset \left< v_4 \right>  \}$
    è sottospazio vettoriale di $\hom(V, V)$ di dimensione $1$. 
    \item esistono infinite applicazioni lineari $f : V \to V$ tali che $\ker(f) = \left< v_1, v_2, v_3 \right>, f(v_4) = v_5, f(v_5) = v_4$ 
    \item Nessuna delle risposte è corretta 
\end{enumerate}
\begin{enumerate}
    \item Vera Perché $f \in S$, se e solo se
    \begin{gather*}
        M_{B, B}(f) = \begin{pmatrix}
            0 & 0 & 0 & 0 & 0 \\
             0 & 0 & 0 & 0 & 0 \\
             0 & 0 & 0 & 0 & 0 \\
             0 & 0 & 0 & 0 & \cdot  \\
             0 & 0 & 0 & 0 & 0
        \end{pmatrix}
    \end{gather*}
    quindi se $h : \hom(V, V) \to M(5 \times 5, \mathbb{K})$ è l'isomorfismo $f \to M_{B, B}(f)$
    di dimensione $1$ e anche $S$ è sottospazio vettoriale di $\hom(V, V)$ di dimensione 1. 
    \item Falsa perché due applicazioni lineari
    \begin{gather*}
        f_1 : V \to V, \qquad f_2 : V \to V
    \end{gather*}
    soddisfano entrambe le condizioni e sono diverse infatti
    \begin{gather*}
        f_1(v_4) = v_1, f_1(v_5) = v_2 \qquad f_2(v_4) = v_2, f_2(v_5) = v_1
    \end{gather*}
    \item Falsa perché ne esiste una sola di applicazioni
    lineari per il teorema che dice che se $\{v_1, \dots, v_n\}$ è base di $V$
    e $\{w_1, \dots, w_n\}$ base di $W$, allora esiste una ed una sola applicazione
    lineare tale che $f(v_i) = w_i$ per tutti gli indici e allora è falsa.
    \item Falsa.
\end{enumerate}

\subsection{Es 6}
Sia $t \in \mathbb{R}$ e sia $A_t = \begin{pmatrix}
    0 & t + 1 & 0 \\
    t + 1 & 0 &  t \\
    0 &  t & 0
\end{pmatrix}$. \\
i) Calcolare la segnatura al variare di $t$ di $b_{A_t}$. \\
Possiamo intanto cercare di svolgere la matrice e cercare, con le
operazioni di Gauss di riga e di colonna, di trovare una matrice più semplice per calcolare la segnatura:
\begin{gather*}
    \begin{pmatrix}
    0 & t + 1 & 0 \\
    t + 1 & 0 &  t \\
    0 &  t & 0
  \end{pmatrix} \ \Rightarrow \ \begin{pmatrix}
      0 & t + 1 & 0 \\
      1 & 0 & t \\
      0 & t & 0
  \end{pmatrix} \ \Rightarrow  \ \begin{pmatrix}
      0 & 1 & 0 \\
      1 & 0 & 0 \\
      0 & 0 & 0
  \end{pmatrix} 
\end{gather*}
Allora la segnatura della matrice diventa 
\begin{gather*}
    segn = segn \begin{pmatrix}
        0 & 1 \\
        1 & 0
    \end{pmatrix} + segn (0) = (1, 1) + (0, 0)
\end{gather*}
ii) Trovare $M_B(b_{A_t})$ dove $B$ è la base ordinata $\{e_2, e_1, e_1 + e_3\}$. \\
Dalla teoria sappiamo che la matrice $A_t$ è ottenuta tramite la forma bilineare
associata alla base canonica $\epsilon$. Allora posso esprimere gli elementi della nuova
matrice associata alla nuova base $B$ come forme bilineari del tipo $b(e_i, e_j)$, dove 
la forma bilineare di questo tipo individua proprio l'elemento $i, j$esimo della matrice di partenza,
allora, rispetto alla nuova base si ottiene le seguenti:
\begin{gather*}
    M_{B_{1,1}} = b(e_2, e_2) = 0 \\
    M_{B_{1,2}} = b(e_2, e_1) = t + 1 \\
    M_{B_{1,3}} = b(e_2, e_1 + e_3) = b(e_2, e_1) + b(e_2, e_3) = 2t + 1 \\
    M_{B_{2,1}} = b(e_1, e_2) = t + 1 \\
    M_{B_{2,2}} = b(e_1, e_1) = 0 \\
    M_{B_{2,3}} = b(e_1, e_1 + e_3) = 0 \\
    M_{B_{3,1}} = b(e_1 + e_3, e_2) = 2t + 1 \\
    M_{B_{3,2}} = b(e_1 + e_3, e_1) = 0 \\
    M_{B_{3,3}} = b(e_1 + e_3, e_1 + e_3) = 0 \\
    \begin{pmatrix}
        0 & t + 1 & 2t + 1 \\
        t + 1 & 0 & 0 \\
        2t + 1 & 0 & 0
    \end{pmatrix}
\end{gather*}


\section{10/6/2024}
\subsection{Es 2}
Per $a \in \mathbb{C}$ si consideri $f_a : \mathbb{C}^{2} \to \mathbb{C}^{2}$ definita come:
\begin{gather*}
    f_a\begin{pmatrix} x\\
    y \end{pmatrix} = \begin{pmatrix} (a^{2} -1 )x^{2} + y\\
    x + (a + 1)y^{2}  \end{pmatrix}  
\end{gather*}  
Trovare i valori di $a$ per i quali si ha che $f_a$ è una applicazione lineare e calcolarne l'immagine. \\
Per capire se è una applicazione lineare devono valere due condizioni:
\begin{gather*}
    f_a(\lambda v) = \lambda f(v) \\
    f(v_1 + v_2) = f(v_1) + f(v_2)
\end{gather*}
Allora posso dire, numericamente:
\begin{gather*}
    f_a\left(2\begin{pmatrix} 0\\
    1 \end{pmatrix} \right) = 2f_a\begin{pmatrix} 0\\
    1 \end{pmatrix} \ \Rightarrow \ \begin{pmatrix} 2\\
    4(a + 1) \end{pmatrix} = 2 \begin{pmatrix} 1\\
    a + 1 \end{pmatrix}  
\end{gather*}
Per essere uguali deve risultare allora $a = -1$.  Risulta
allora che $f_{-1}$ è
\begin{gather*}
    f_{-1} \begin{pmatrix} x\\
    y \end{pmatrix} = \begin{pmatrix} y \\
    x \end{pmatrix}  
\end{gather*}
E' allora una applicazione lineare se e solo se $a = -1$. In tal caso
si ha che
\begin{gather*}
    f_{-1} = \begin{pmatrix}
        0 & 1 \\
        1 & 0
    \end{pmatrix} \ \Rightarrow \ Im = \left< \begin{pmatrix} 1\\
    0 \end{pmatrix}, \begin{pmatrix} 0\\
    1 \end{pmatrix}   \right> = \mathbb{C}^{2}  
\end{gather*}

\subsection{Es 4}
Trovare, se esiste, $A \in M(3 \times 3, \mathbb{R})$ tale che
\begin{gather*}
    A_{1, 2} = A_{2, 1} = A_{1, 3} = 0 \\
    2 \ \text{autovalore di } A \text{ con } m_a = 2 \\
    \begin{pmatrix} 3\\
    1\\
    0 \end{pmatrix} \text{ autovettore } A \text{ con } \lambda = 0 
\end{gather*}
Cerco allora una matrice. Intanto
\begin{gather*}
  A\begin{pmatrix} 3\\
  1\\
  0 \end{pmatrix} = 0 \ \Rightarrow \ 3A^{(1)} + A^{(2)} = 0 \Rightarrow A^{(2)} = -3A^{(1)}      
\end{gather*}
La matrice allora sarà del tipo
\begin{gather*}
    \begin{pmatrix}
        0 & 0 & 0 \\
        0 & 0 & a \\
        b & -3b & c
    \end{pmatrix}
\end{gather*}
Ora impongo le condizioni dello spettro e dunque
$spettro A = spettro (0) \bigcup \begin{pmatrix}
    0 & x \\
    -3b & y
\end{pmatrix}$. Quindi $2$ deve ora essere autovalore della matrice
che stiamo trovando e dunque 
\begin{gather*}
    -\lambda(c - \lambda) +3ba = 0
\end{gather*}
Dato che $\lambda = 2$, allora dalla teoria si sa che
\begin{gather*}
    tr(A) = 2 + 2 = 4, \det(A) = 2 \cdot 2 = 4 \\
    c = 4, 3ba = 4.
\end{gather*}
Quindi si ottiengono da queste tre condizioni
\begin{gather*}
    c = 4, b = \frac{1}{3}, a = 4
\end{gather*}
E allora la matrice trovata è
\begin{gather*}
   A= \begin{pmatrix}
     0 & 0 & 0 \\
     0 & 0 & 4 \\
     \frac{1}{3} & -1 & 4   
    \end{pmatrix}
\end{gather*}

\subsection{Es 8}
Sia $V$ uno spazio vettoriale su di $\mathbb{R}$. Dato $\omega \in V, \phi \in V^{\vee}$ e denoto con $\omega \phi$
l'applicazione lineare da $V$ a $V$ tale che
\begin{gather*}
    v \to \phi(v)\omega
\end{gather*} 
Siano $B = \{v_1, v_2, v_3, v_4\}$ una base ordinata di $V$ e $a \in \mathbb{R}$. Sia $f_a : V \to V$ l'applicazione
lineare così definita:
\begin{gather*}
    f_a = 3v_1v_1^{\vee} + (a v_1 + 7v_2)v_2^{\vee} + (av_3 + (2a - 2)v_4)v_3^{\vee} + (v_3 + 3v_4)v_4^{\vee}    
\end{gather*}
i) Dire per quali valori di $a$ l'applicazione lineare $f_a$ è diagonalizzabile. \\
Data la definizione di valutazione $v_i v_j^{\vee} = 1$ con $i = j$ e quindi
\begin{gather*}
    f_a(v_1) = 3v_1 \\
    f_a(v_2) = av_1 + 7v_2 \\
    f_a(v_3) = av_3 + (2a - 2)v_4 \\
    f_a(v_4) = v_3 + 3v_4
\end{gather*} 
Allora la matrice associata alla base$B$ dell'applicazione lineare è
proprio
\begin{gather*}
    M_{B, B}(f_a) = \begin{pmatrix}
        3 & a & 0 & 0 \\
        0 & 7 & 0 & 0 \\
        0 & 0 & a & 1 \\
        0 & 0 & 2a-2 & 3
    \end{pmatrix}
\end{gather*}
Quindi la matrice è diagonalizzabile se e solo se le
matrici a blocchi sulla diagonale principale sono entrambe
diagonalizzabili.
La prima matrice $\begin{pmatrix}
    3 & a \\
    0 & 7
\end{pmatrix}$ ha come spettro $\{3, 7\}$ poiché è traingolare superiore
e inoltre è diagonalizzabile poiché sono autovalori distinti. L'altro
blocco 
\begin{gather*}
    \begin{pmatrix}
        a & 1 \\
        2a - 2 & 3
    \end{pmatrix} \ \Rightarrow \ tr = a + 3 \quad \det = a + 2 
\end{gather*}
E gli autovalori, data la teoria, sono $a + 2$ e $1$. Allora, dato che
sono distinti, allora se $a + 2 \neq 1$, la matrice è diagonalizzabile. 
Se fosse dunque $a = -1$ allora $f_a$ non è diagonalizzabile. \\
ii) Per $a = 1$ trovare autovalori e le molteplicità, e gli autospazi. \\
\begin{gather*}
    f_1 = \begin{pmatrix}
        3 & 1 & 0 & 0 \\
        0 & 7 & 0 & 0 \\
        0 & 0 & 1 & 1 \\
        0 & 0 & 0 & 3
    \end{pmatrix}
\end{gather*}
Allora posso trovare gli autovalori delle matrici a blocchi oppure vedere che,
essendo entrambi i blocchi diagonali, allora gli autovalori sono
\begin{gather*}
    spettro(f_1) = \{1, 3, 7\} \\
    m_a(1) = 1 \Rightarrow\  m_g(1) = 1 \\
    m_a(3) = 2 \ \Rightarrow \ m_g(3) = 2 \\
    m_a(7) = 1 \ \Rightarrow \ m_g(7) = 1
\end{gather*}
Per trovare gli autospazi basta calcolare il kernel per ogni autovalore
con la seguente $\ker(A - \lambda I)$. 
\begin{gather*}
    \ker(A - I) = \left< \begin{pmatrix} 0\\
    0\\
    1\\
    0 \end{pmatrix}  \right> = \left< v_3 \right>  \\
    \ker(A - 3I) = \left< \begin{pmatrix} 1\\
    0\\
    0\\
    0 \end{pmatrix}, \begin{pmatrix} 0\\
    0\\
    1\\
    0 \end{pmatrix}   \right> = \left< v_1 + 4 v_2 \right> \\
    \ker(A - 7I) = \left< \begin{pmatrix} 1\\
    4\\
    0\\
    0 \end{pmatrix}  \right>   = \left< v_1, v_3 + 2v_4 \right> 
\end{gather*}
iii) Trovare, se esiste, una mappa lineare $g : V \to V$ non nulla e tale che 
$g \circ f_{-2}$ e $f_{-2} \circ g$ siano le mappe nulle. \\
\begin{gather*}
    M_{B, B}(f_{-2}) = \begin{pmatrix}
        3 & -2 & 0 & 0 \\
        0 & 7 & 0 & 0 \\
        0 & 0 & -2 & 1 \\
        0 & 0 & -6 & 3
    \end{pmatrix} \qquad \begin{array}{l}
      Im = \left< e_1, e_2, e_3 +3e_4 \right> \\
      \ker = \left< e_3 + 2e_4 \right>  
    \end{array}
\end{gather*}
Dato che $g \circ f_{-2} = 0 = f_{-2} \circ g$  allora deve risultare che
$Im(f_{-2}) \subset  \ker g$ e $Im(g) \subset  \ker(f_{-2})$ quindi 
cerco una matrice $4 \times 4$  associata alla mappa $g$ e quindi, so che 
$g(e_1) = g(e_2) = g(e_3 + 4e_4) = 0$ e allora $A^{(1)}$ e $A^{(2)}$ sono uguali a $0$
e so che $A^{(3)} + 3A^{(4)} = 0$ e allora
\begin{gather*}
    g = \begin{pmatrix}
        0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 \\
        0 & 0 & -3x & 1x \\
        0 & 0 & -6x & 2x
    \end{pmatrix}, x \in \mathbb{R} - \{0\}
\end{gather*}   

\section{Gennaio 2026}
\subsection{Es 1}
Sia $v = (1, 1, 1) \in \mathbb{R}^3$. 
\begin{enumerate}
    \item Trovare una base ortonormale del piano $\left< v \right>^{\perp}$.
    \item Si trovi, se esiste, $u \in \left< v \right>^{\perp}$ tale che l’angolo tra $u$ e $e_1$ sia $\frac{\pi}{4}$.
    \item Si trovi $\min\{\overbrace{ue_1} |u \in \left< v \right>^\perp \}$, dove  $\overbrace{u e_1}$ è l’angolo tra $u$ e $e_1$ (il risultato può essere espresso utilizzando la funzione arcoseno).
\end{enumerate} 
1. Una base di $\left< v \right>^{\perp}$ è sicuramente la seguente
\begin{gather*}
    \left< v \right>^{\perp} = \left< \begin{pmatrix} 1\\
    -1\\
    0 \end{pmatrix}, \begin{pmatrix} 1\\
    0\\
    -1 \end{pmatrix}   \right>  
\end{gather*} 
Adesso si ricava una base ortonormale attraverso il procedimento di Gram Schmidt 1:

\end{document}